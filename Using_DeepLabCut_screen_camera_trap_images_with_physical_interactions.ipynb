{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1XR845241n7aeFw7YIDJzrScTYdH4Sj3f",
      "authorship_tag": "ABX9TyPJG6Aw7IqogOZkrNNna+Y8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vchiang001/DeepLabCut/blob/master/Using_DeepLabCut_screen_camera_trap_images_with_physical_interactions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using DeepLabCut to filter out MegaDetector analysed camera trap images that show physically interacting animals**\n",
        "\n",
        "![alt text](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1616492373700-PGOAC72IOB6AUE47VTJX/ke17ZwdGBToddI8pDm48kB8JrdUaZR-OSkKLqWQPp_YUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnBqyW03PFN2MN6T6ry5cmXqqA9xITfsbVGDrg_goIDasRCalqV8R3606BuxERAtDaQ/modelzoo.png?format=1000w)\n",
        "\n",
        "The understanding of sexual diversity including the sociosexual interactions between organisms requires consideration of the natural milieu of these behaviours , as well as the evolutionary perspectives by comparing across species. Camera trap data is pivotal for this purpose to look at these behaviours in wildlife animals, and to understand their context (e.g. where and when). For more details, you can refer to this presentation during the 2022 DeepLabCut AI Residency Program at EPFL: https://zenodo.org/record/7040375#.YyJ9a3bMK39 \n",
        "\n",
        "Our notebook runs on camera trap images that have been analysed using MegaDetector. More details on MegaDetector here: https://github.com/microsoft/CameraTraps/blob/main/megadetector.md \n",
        "\n",
        "Our notebook helps you isolate camera trap images that exhibit physical interactions. This is helpful because scientists invest a huge amount of time reviewing camera trap images, and a huge amount of that time is spent reviewing images they aren’t interested in (e.g. empty images, noise). For those interested in sexual behaviour, this can filter out a substantial amount of images, to visually inspect those images with physical interaction, whether sexual behaviour could be occurring. \n",
        "\n",
        "For this work, we will use DeepLabCut from the model zoo. More details here: http://modelzoo.deeplabcut.org Please cite the relevant papers for the models you use, and consider giving back by helping to label more data. More details here: https://contrib.deeplabcut.org/ \n",
        "\n",
        "- **What you need:**camera trap images, and the JSON output file from MegaDetector analysis (ensure your camera trap images retain the same name in the JSON output file)\n",
        "\n",
        "- **What to do:** (1) in the top right corner, click \"CONNECT\". Then, just hit run (play icon) on each cell below and follow the instructions!\n"
      ],
      "metadata": {
        "id": "VgkZTHEws_xT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Let's get going: install DeepLabCut into COLAB:**\n",
        "*Also, be sure you are connected to a GPU: go to menu, click Runtime > Change Runtime Type > select \"GPU\"*"
      ],
      "metadata": {
        "id": "ex64JvfLzfY5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eT4gL2Gzz_gX"
      },
      "outputs": [],
      "source": [
        "#click the play icon (this will take a few minutes to install all the dependences!)\n",
        "!pip install deeplabcut-live\n",
        "!pip install deeplabcut"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)**"
      ],
      "metadata": {
        "id": "ckkmk3vC1GhH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now let's set the backend & import the DeepLabCut package:"
      ],
      "metadata": {
        "id": "Gc8Y3gWj3mVT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# stifle tensorflow warnings, like we get it already.\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "import deeplabcut"
      ],
      "metadata": {
        "id": "TNIMV3bW3lvd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Link your Google Drive (containing camera trap images & JSON file):\n",
        "\n",
        "### First, place your camera trap image folder & JSON file into you google drive! "
      ],
      "metadata": {
        "id": "KzWAx4Xaz5En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "VRJwtmcmz8ij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Frnj1RVDyEqs"
      },
      "source": [
        "YOU WILL NEED TO EDIT THE FILE PATH & JSON PATH TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "Typically, this will be: /content/drive/My Drive/CameraTrapImages\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup your project variables:\n",
        "# PLEASE EDIT THESE:\n",
        "file_path = '/content/drive/My Drive/CameraTrapImages/'\n",
        "json_path = '/content/drive/My Drive/CameraTrapImages/megadetector.json'"
      ],
      "metadata": {
        "id": "lRluQKje1_dQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Pre-filter camera trap images\n",
        "If you have thousands of camera trap images, you may want to pre-filter them depending on your needs. For our purposes, we want only camera trap images that have 2 or more animals, with MegaDetector confidence above a certain threshold. Running this cell will screen through all the images in your MegaDetector JSON output file, and COPY the images fulfilling our condition into a separate folder. *It will not delete any of your existing images only COPY to a new folder.\n"
      ],
      "metadata": {
        "id": "el2_E7cPiDmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Please edit these:\n",
        "mega_thresh = 0.6 #you can set it at a number between 0.0 – 1.0 (the higher the more confident)\n",
        "filter_folder = \"mre2anim\" #what name you want to call this folder, this will be a new folder in your file_path"
      ],
      "metadata": {
        "id": "CrehjlRaiFBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import shutil\n",
        "import os\n",
        "\n",
        "# access your Megadetector JSON file\n",
        "with open(json_path, 'r') as f:\n",
        "    detection_results = json.load(f)\n",
        "\n",
        "#creates a new folder for your filtered images\n",
        "mre2anim = os.path.join(file_path, filter_folder)\n",
        "os.mkdir(mre2anim) \n",
        "\n",
        "#if there are more than 2 animals that are above confidence thresholder\n",
        "#the image will be copied to the folder defined above.\n",
        "for img_data in detection_results['images']: \n",
        "    if len(img_data['detections']) >= 2: \n",
        "        anim_detect = 0 \n",
        "        print(img_data['file'])\n",
        "        for detect in img_data['detections']: \n",
        "            if detect['category'] == '1' and detect['conf'] >= mega_thresh:\n",
        "                anim_detect += 1\n",
        "        if anim_detect >= 2: \n",
        "            file = img_data['file'] \n",
        "            shutil.copy(file_path+'/'+file, mre2anim)  "
      ],
      "metadata": {
        "id": "47qvRJxmjD-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Select your model from the dropdown menu and set the model for analysis"
      ],
      "metadata": {
        "id": "9KF7WluQ2oYl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "model_options = deeplabcut.create_project.modelzoo.Modeloptions\n",
        "model_selection = widgets.Dropdown(\n",
        "    options=model_options,\n",
        "    value=model_options[0],\n",
        "    description=\"Choose a DLC ModelZoo model!\",\n",
        "    disabled=False\n",
        ")\n",
        "display(model_selection)"
      ],
      "metadata": {
        "id": "e8HNJyDI2yJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model2use = model_selection.value\n",
        "from deeplabcut.utils import auxfun_models\n",
        "auxfun_models.download_model(modelname = model2use, target_dir = file_path)"
      ],
      "metadata": {
        "id": "nPQAil2o4Sez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOU WILL NEED TO EDIT THE MODEL PATH & POSE CONFIG PATH TO BE SET TO YOUR GOOGLE DRIVE LINK!\n",
        "\n",
        "This is currently set in your \"file_path\", so typically, this will be: /content/drive/My Drive/CameraTrapImages"
      ],
      "metadata": {
        "id": "R9chzoNb7Ktt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup where your model is\n",
        "# PLEASE EDIT THESE:\n",
        "model_path = '/content/drive/My Drive/CameraTrapImages'\n",
        "pose_cfg_path = model_path + '/pose_cfg.yaml' #copy the path to the pose_cfg.yaml "
      ],
      "metadata": {
        "id": "skIBu-tO6fF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function crops the bounding box from the JSON output"
      ],
      "metadata": {
        "id": "IDD5MFCh7xES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import PIL\n",
        "from PIL import Image, ImageFile, ImageFont, ImageDraw\n",
        "import numpy as np\n",
        "from tensorflow.python.eager.context import global_seed\n",
        "def crop_detections(detect, img, imageWidth, imageHeight):\n",
        "    x1, y1,w_box, h_box = detect[\"bbox\"]\n",
        "    ymin,xmin,ymax, xmax = y1, x1, y1 + h_box, x1 + w_box \n",
        "    area = (xmin * imageWidth, \n",
        "            ymin * imageHeight, \n",
        "            xmax * imageWidth,\n",
        "            ymax * imageHeight)\n",
        "    cropping = img.crop(area)\n",
        "    list_np_crop = np.asarray(cropping)\n",
        "    return list_np_crop "
      ],
      "metadata": {
        "id": "o4sGQwMT7xim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function runs the DeepLabCut model on individual MegaDetector bounding box"
      ],
      "metadata": {
        "id": "nt7qkVYV7-Bk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from dlclive import DLCLive, Processor\n",
        "\n",
        "def predict_dlc(model_path, list_np_crop):\n",
        "    #list_np_crop = np.array(list_np_crop)\n",
        "    dlc_proc = Processor()\n",
        "    dlc_live = DLCLive(model_path, processor=dlc_proc)\n",
        "    dlc_live.init_inference(list_np_crop)\n",
        "    keypts_xyp = dlc_live.get_pose(list_np_crop) # third column is llk!\n",
        "    return keypts_xyp\n"
      ],
      "metadata": {
        "id": "MppIUjRS8Fpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function converts the DeepLabCut poses normalised to the original image"
      ],
      "metadata": {
        "id": "u4LFkf9K8OGw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yaml\n",
        "from ruamel.yaml import YAML\n",
        "import pandas as pd \n",
        "def pose_orig_img(keypts_xyp, detect, pose_cfg_path):\n",
        "    x1, y1, w_box, h_box = detect[\"bbox\"] \n",
        "    keypts_xyp_orig_img = []\n",
        "    for x, y, p in keypts_xyp:\n",
        "        c = [x+x1, y+y1] \n",
        "        keypts_xyp_orig_img.append(c)\n",
        "    with open(pose_cfg_path, \"r\") as stream:\n",
        "        pose_cfg_dict = yaml.safe_load(stream)\n",
        "    bodypart_label = pose_cfg_dict['all_joints_names']\n",
        "    keypts_xyp_orig_img_df = pd.DataFrame (keypts_xyp_orig_img, columns = ['x', 'y'])\n",
        "    keypts_xyp_orig_img_df['bodypart'] = bodypart_label\n",
        "    keypts_xyp_orig_img_df = keypts_xyp_orig_img_df.set_index('bodypart')\n",
        "    return keypts_xyp_orig_img_df"
      ],
      "metadata": {
        "id": "McIpgCmP8Z23"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function obtains the reference length for each image (as each image differ in size) "
      ],
      "metadata": {
        "id": "gBvefqPu8fv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Please edit these:\n",
        "mega_thresh = 0.6 #for your megadetector bounding box, you can set it at a number between 0.0 – 1.0 (the higher the more confident)\n",
        "dlc_thresh = 0.001 #for your dlc poses, you can set it at a number between 0.0 – 1.0 (the higher the more confident)"
      ],
      "metadata": {
        "id": "kXXdPIxVHePT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "from statistics import mean\n",
        "\n",
        "def nose2eye(allposes_names):\n",
        "    global ref\n",
        "    nose2eye_dist = [] \n",
        "    for entity in allposes_names: \n",
        "        eye_list = [] \n",
        "        df_body = globals()[entity]\n",
        "        df_body_filt = df_body.loc[df_body['p'] > dlc_thresh] \n",
        "        for body in df_body_filt.index: \n",
        "            if body in eye_names: \n",
        "                eye_list.append(list(df_body_filt.loc[body])) \n",
        "        if len(eye_list) != 0: \n",
        "            eye_list = eye_list[0]\n",
        "            eye_list = eye_list[:2]\n",
        "            nose_list = [] \n",
        "            for body in df_body_filt.index:\n",
        "                if body in nose_names:\n",
        "                    nose_list.append(list(df_body_filt.loc[body]))\n",
        "            if len(nose_list) != 0:\n",
        "                nose_list = nose_list[0] \n",
        "                nose_list = nose_list[:2]\n",
        "                dist = distance.euclidean(eye_list,nose_list)\n",
        "                nose2eye_dist.append(dist)\n",
        "    if len(nose2eye_dist) != 0:\n",
        "        ref = min(nose2eye_dist)\n",
        "    else:\n",
        "        ref = 'no' \n",
        "    return ref "
      ],
      "metadata": {
        "id": "0F9byD5m8miE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## This function checks if there is physical interaction between individual MegaDetector boxes in each image"
      ],
      "metadata": {
        "id": "qFHgAVNT9DdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "def phys_int(allposes_names, ref):\n",
        "    int_matrx = [] \n",
        "    for entity in allposes_names:\n",
        "        df_body = globals()[entity]\n",
        "        df_body_filt = df_body.loc[df_body['p'] > dlc_thresh]\n",
        "        df_body_filt = df_body_filt.drop(columns=\"p\")\n",
        "        xy = df_body_filt.values.tolist() \n",
        "        for entity in allposes_names: \n",
        "            df_body2 = globals()[entity]\n",
        "            df_body_filt2 = df_body2.loc[df_body2['p'] > dlc_thresh]\n",
        "            df_body_filt2 = df_body_filt2.drop(columns=\"p\")\n",
        "            ab = df_body_filt2.values.tolist() \n",
        "            for i in xy: \n",
        "                for b in ab: \n",
        "                    dist = distance.euclidean(i, b)\n",
        "                    if dist != 0: \n",
        "                        int_matrx.append(dist) \n",
        "    if any(i < ref for i in int_matrx): \n",
        "        social = \"physically interacting\"\n",
        "    else:\n",
        "        social = 'not physically interacting'\n",
        "    return social "
      ],
      "metadata": {
        "id": "XJgD7B3t9Lzm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## After loading all the functions, we are ready to run! \n",
        "\n",
        "This creates a separate folder, and all camera trap images that exhibit physical interactions will be copied into the folder. \\\n",
        "Currently this is set to only the MegaDetector bounding boxes with confidence above 0.6, but this can be adjusted. \\\n",
        "Note that if you have thousands of images, it could take hours to run this (~10h for 7000 images). Therefore there is a textfile that keeps track which files and how many have been processed out of the total. There is an option below if your COLAB crashes, and you need to pick up from where it crashed. \n"
      ],
      "metadata": {
        "id": "jXfQqX2s9Zri"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Please edit this:\n",
        "camtrap4dlc = '/content/drive/My Drive/CameraTrapImages/mre2anim' #if you didn't do the optional filter step above, you need to change this "
      ],
      "metadata": {
        "id": "ohyiKdYUlFT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# open json file\n",
        "import json \n",
        "with open(json_path, 'r') as f:\n",
        "    detection_results = json.load(f)\n",
        "\n",
        "# this is used to temporary name bounding boxes\n",
        "import random \n",
        "import string  \n",
        "\n",
        "# creates a new folder to copy camera trap images with physical interactions\n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "phys_int_path = os.path.join(file_path, \"phys_int_camtrap\")\n",
        "os.mkdir(phys_int_path) \n",
        "\n",
        "#finds the total number of images to process & writes to a new text file to monitor progress\n",
        "num_img = len(os.listdir(camtrap4dlc))\n",
        "txt_path = os.path.join(file_path, 'progress.txt')\n",
        "prog_txt = open(txt_path, 'a')\n",
        "prog_txt.write('total number of images is' + str(num_img))\n",
        "prog_txt.close()\n",
        "\n",
        "#bodypart names in various models to use for reference length \n",
        "eye_names = ['L_Eye', 'R_Eye', 'r_eye', 'l_eye', 'forehead', 'left_eye', 'right_eye']  \n",
        "nose_names = ['Nose', 'nose', 'chin', 'upper_jaw', 'mouth_end_right ', 'lower_jaw', 'mouth_end_left']     \n",
        "\n",
        "\n",
        "# this runs through all images in the JSON file, including monitoring the progress\n",
        "# the loop goes through in order of the Megadetector JSON file\n",
        "prog = 0 \n",
        "prog_txt = open(txt_path, 'a') \n",
        "for img_data in detection_results[\"images\"]:\n",
        "    img_path =  os.path.join(file_path, img_data['file'])\n",
        "    if os.path.exists(img_path) == True: \n",
        "      img = Image.open(img_path)\n",
        "      imageWidth = img.size[0] \n",
        "      imageHeight = img.size[1]\n",
        "      prog += 1 \n",
        "      prog_txt.write(img_path + '\\n') \n",
        "      prog_txt.write(str(prog) + 'out of' + str(num_img) + 'has been processed' + '\\n')       \n",
        "      if len(img_data[\"detections\"]) >= 2: \n",
        "        print(img_data['file'])\n",
        "        allposes_names = []  \n",
        "        for detect in img_data[\"detections\"]: \n",
        "            if detect['conf'] >= mega_thresh: \n",
        "                list_np_crop = crop_detections(detect, img, imageWidth, imageHeight) \n",
        "                keypts_xyp = predict_dlc(model_path, list_np_crop) \n",
        "                keypts_xyp_orig_img_df = pose_orig_img(keypts_xyp, detect, pose_cfg_path)\n",
        "                letters = string.ascii_letters \n",
        "                name =  ''.join(random.choice(letters) for i in range(10)) \n",
        "                allposes_names.append(name) \n",
        "                locals()[name] = keypts_xyp_orig_img_df \n",
        "        ref =  nose2eye(allposes_names)\n",
        "        if ref == 'no':\n",
        "            print('no ref')\n",
        "        else: \n",
        "            print(ref)\n",
        "            social = phys_int(allposes_names, ref)\n",
        "            file = img_data['file'] \n",
        "            if social == \"physically interacting\":\n",
        "                shutil.copy(file_path+'/'+file, phys_int_path) \n",
        "prog_txt.close() "
      ],
      "metadata": {
        "id": "5uySVIDA9nne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## (OPTIONAL) Resuming if your COLAB crashes\n",
        "Since the COLAB may need to run for a long time if you have thousands of camera trap data to process through. You can run this cell, to find out the last image that was processed, and find the index in the original JSON file. "
      ],
      "metadata": {
        "id": "x4ogB__6nKxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# find the last img processed \n",
        "with open(txt_path, 'r') as f:\n",
        "    last_proc_img = f.readlines()[-2]\n",
        "\n",
        "#get the basename, without the /n \n",
        "print(last_proc_img)\n",
        "last_proc_img_base = os.path.basename(last_proc_img)\n",
        "last_proc_img_base_wo_n = last_proc_img_base[:-1]\n",
        "print(last_proc_img_base_wo_n)\n",
        "\n",
        "# finds the progress through the images \n",
        "with open(txt_path, 'r') as f:\n",
        "    last_prog_line = f.readlines()[-1]\n",
        "\n",
        "last_prog = int(last_prog_line[0])\n",
        "print(last_prog)\n",
        "\n",
        "#access the megadetector JSON file \n",
        "import json \n",
        "with open(json_path, 'r') as f:\n",
        "    detection_results = json.load(f)\n",
        "\n",
        "#find the index of a file \n",
        "files = detection_results['images']\n",
        "for file in files:\n",
        "    if file['file'] == last_proc_img_base_wo_n:\n",
        "        last_proc_img_index = int(files.index(file)) \n",
        "\n",
        "print(last_proc_img_index) "
      ],
      "metadata": {
        "id": "UiZKfx-tocTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After you ran the above cells, and finds the index, you can run the cell below. It's similar to the the one above, but it changes to start from the image with the index number found above. "
      ],
      "metadata": {
        "id": "baGFHma8smF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json \n",
        "import random \n",
        "import string  \n",
        "import glob\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "with open(json_path, 'r') as f:\n",
        "    detection_results = json.load(f)\n",
        "phys_int_path = os.path.join(file_path, \"phys_int_camtrap\")\n",
        "num_img = len(os.listdir(camtrap4dlc))\n",
        "txt_path = os.path.join(file_path, 'progress.txt')\n",
        "prog = last_prog \n",
        "\n",
        "eye_names = ['L_Eye', 'R_Eye', 'r_eye', 'l_eye', 'forehead', 'left_eye', 'right_eye']  \n",
        "nose_names = ['Nose', 'nose', 'chin', 'upper_jaw', 'mouth_end_right ', 'lower_jaw', 'mouth_end_left']    \n",
        "\n",
        "prog_txt = open(txt_path, 'a') \n",
        "for img_data in detection_results[\"images\"][last_proc_img_index:]: \n",
        "    img_path =  os.path.join(file_path, img_data['file'])\n",
        "    if os.path.exists(img_path) == True: \n",
        "      img = Image.open(img_path)\n",
        "      imageWidth = img.size[0] \n",
        "      imageHeight = img.size[1]\n",
        "      prog += 1 \n",
        "      prog_txt.write(img_path + '\\n') \n",
        "      prog_txt.write(str(prog) + 'out of' + str(num_img) + 'has been processed' + '\\n')       \n",
        "      if len(img_data[\"detections\"]) >= 2: \n",
        "        print(img_data['file'])\n",
        "        allposes_names = []  \n",
        "        for detect in img_data[\"detections\"]: \n",
        "            if detect['conf'] >= mega_thresh: \n",
        "                list_np_crop = crop_detections(detect, img, imageWidth, imageHeight) \n",
        "                keypts_xyp = predict_dlc(model_path, list_np_crop) \n",
        "                keypts_xyp_orig_img_df = pose_orig_img(keypts_xyp, detect, pose_cfg_path)\n",
        "                letters = string.ascii_letters \n",
        "                name =  ''.join(random.choice(letters) for i in range(10)) \n",
        "                allposes_names.append(name) \n",
        "                locals()[name] = keypts_xyp_orig_img_df \n",
        "        ref =  nose2eye(allposes_names)\n",
        "        if ref == 'no':\n",
        "            print('no ref')\n",
        "        else: \n",
        "            print(ref)\n",
        "            social = phys_int(allposes_names, ref)\n",
        "            file = img_data['file'] \n",
        "            if social == \"physically interacting\":\n",
        "                shutil.copy(file_path+'/'+file, phys_int_path) \n",
        "prog_txt.close() "
      ],
      "metadata": {
        "id": "9gW_DMzLsj3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Happy DeepLabCutting! Welcome to the Zoo :)"
      ],
      "metadata": {
        "id": "hDoXLVav-6Ts"
      }
    }
  ]
}