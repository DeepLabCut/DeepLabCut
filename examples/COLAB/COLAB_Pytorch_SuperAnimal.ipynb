{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MMathisLab/DeepLabCut/blob/main/COLAB_Pytorch_SuperAnimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5SSZpZUu0Z4S"
   },
   "source": [
    "# DeepLabCut Model Zoo: SuperAnimal models\n",
    "\n",
    "![alt text](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1616492373700-PGOAC72IOB6AUE47VTJX/ke17ZwdGBToddI8pDm48kB8JrdUaZR-OSkKLqWQPp_YUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnBqyW03PFN2MN6T6ry5cmXqqA9xITfsbVGDrg_goIDasRCalqV8R3606BuxERAtDaQ/modelzoo.png?format=1000w)\n",
    "\n",
    "# ðŸ¦„ SuperAnimal in DeepLabCut PyTorch! ðŸ”¥\n",
    "\n",
    "This notebook demos how to use our SuperAnimal models within DLC3. Please read more in [Ye et al. Nature Communications 2024](https://www.nature.com/articles/s41467-024-48792-2) about the available SuperAnimal models, and follow along below!\n",
    "\n",
    "### **Let's get going: install DeepLabCut into COLAB:**\n",
    "\n",
    "*Also, be sure you are connected to a GPU: go to menu, click Runtime > Change Runtime Type > select \"GPU\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AjET5cJE5UYM",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "0a9bc286-e21e-4fe2-cd1a-fb570f35e719"
   },
   "outputs": [],
   "source": [
    "!pip install deeplabcut==3.0.0rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5h0vq6E50Z4W"
   },
   "source": [
    "### PLEASE, click \"restart runtime\" from the output above before proceeding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvnlIvQm0Z4X",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5fff9d1f-621f-4147-9a54-39d4c01db0b3"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import deeplabcut\n",
    "from deeplabcut.pose_estimation_pytorch.apis.analyze_images import (\n",
    "    superanimal_analyze_images,\n",
    ")\n",
    "from deeplabcut.core.weight_init import WeightInitialization\n",
    "from deeplabcut.core.engine import Engine\n",
    "from deeplabcut.modelzoo.utils import (\n",
    "    create_conversion_table,\n",
    "    read_conversion_table_from_csv,\n",
    ")\n",
    "from deeplabcut.modelzoo.video_inference import video_inference_superanimal\n",
    "from deeplabcut.utils.pseudo_label import keypoint_matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UeXjmtu40Z4X"
   },
   "source": [
    "## Zero-shot Image Inference & Video Inference\n",
    "SuperAnimal models are foundation animal pose models. They can be used for zero-shot predictions without further training on the data.\n",
    "In this section, we show how to use SuperAnimal models to predict pose from images (given an image folder) and output the predicted images (with pose) into another dest folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FvFzntDMxPoL"
   },
   "source": [
    "## Zero-shot image inference\n",
    "\n",
    "- If you have a single Image you want to test, upload it here!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NbDsZQfsxPoL"
   },
   "source": [
    "#### Upload the images you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "c4yfTj7r0Z4Y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5cd0b4e7-2d47-4432-ec1e-8f66c0362c7b"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "for filepath, content in uploaded.items():\n",
    "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
    "image_path = os.path.abspath(filepath)\n",
    "image_name = os.path.splitext(image_path)[0]\n",
    "\n",
    "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
    "# manually upload your video via the Files menu to the left\n",
    "# and define `video_path` yourself with right click > copy path on the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jashzdjb0Z4Y"
   },
   "source": [
    "### Select a SuperAnimal name and corresponding model architecture\n",
    "\n",
    "Check Our Docs on [SuperAnimals](https://github.com/DeepLabCut/DeepLabCut/blob/main/docs/ModelZoo.md) to learn more!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uH9LXig90Z4Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @markdown ---\n",
    "# @markdown SuperAnimal Configurations\n",
    "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
    "model_name = \"hrnetw32\" #@param [\"hrnetw32\"]\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown What is the maximum number of animals you expect to have in an image\n",
    "max_individuals = 3  # @param {type:\"slider\", min:1, max:30, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OmJtVmHq0Z4Y",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Note you need to enter max_individuals correctly to get the correct number of predictions in the image.\n",
    "superanimal_analyze_images(\n",
    "    superanimal_name,\n",
    "    model_name,\n",
    "    image_path,\n",
    "    max_individuals,\n",
    "    out_folder=\"/content/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6VEjHu-00Z4Y"
   },
   "source": [
    "### Zero-shot Video Inference\n",
    "- Without Video adaptation (faster, but not self-supervised fine-tuned on your data!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGoAhxZOxPoM"
   },
   "source": [
    "#### Upload a video you want to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92
    },
    "id": "PK3efA0I0Z4Y",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "e334e7ae-6904-4853-dade-500cee55e7f5"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "for filepath, content in uploaded.items():\n",
    "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
    "video_path = os.path.abspath(filepath)\n",
    "video_name = os.path.splitext(video_path)[0]\n",
    "\n",
    "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
    "# manually upload your video via the Files menu to the left\n",
    "# and define `video_path` yourself with right click > copy path on the video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JoA-RATSICj_"
   },
   "source": [
    "#### Choose the superanimal and the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiRAP9XD0Z4Z",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# @markdown ---\n",
    "# @markdown SuperAnimal Configurations\n",
    "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
    "model_name = \"hrnetw32\" #@param [\"hrnetw32\"]\n",
    "\n",
    "# @markdown ---\n",
    "# @markdown What is the maximum number of animals you expect to have in an image\n",
    "max_individuals = 3  # @param {type:\"slider\", min:1, max:30, step:1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv3v0QgSJNOg"
   },
   "source": [
    "### Zero-shot Video Inference without video adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "poqynL0UJTBp",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "118cab24-69e6-4ce9-c546-ee9e306dbba2"
   },
   "outputs": [],
   "source": [
    "video_inference_superanimal(\n",
    "    videos=video_path,\n",
    "    superanimal_name=f\"{superanimal_name}_{model_name}\",\n",
    "    video_adapt=False,\n",
    "    max_individuals=max_individuals,\n",
    "    dest_folder=\"/content/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z8Z5GSti0Z4Z"
   },
   "source": [
    "### Zero-shot Video Inference with video adaptation (unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "fec6601a01a44935a06d432c1f94349b",
      "867c5b27246c40b3823c60a014329427",
      "4aba38bf1a9b4be092fffb7581f7cd0b",
      "69a82035258c40b5beed0d9673f8b3b6",
      "c16a46a621ab497183ec0b136749eaeb",
      "3b1eaec29f014a97a8ee14e62b34db83",
      "013e4fb17d234b18934f11593557eec4",
      "787f617ecbf74db6b020a066acb12840",
      "7cd0520e8c26480297b86824e92d77ec",
      "c5d2d04a28d743799dfa706e00303f08",
      "579cc60d128445b8ae10ee036c036bb5"
     ]
    },
    "id": "5mhOmtzw0Z4Z",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "a87f668a-c735-4372-c5ac-15fea95f5a99"
   },
   "outputs": [],
   "source": [
    "video_inference_superanimal(\n",
    "    videos=[video_path],\n",
    "    superanimal_name=f\"{superanimal_name}_{model_name}\",\n",
    "    video_adapt=True,\n",
    "    max_individuals=max_individuals,\n",
    "    pseudo_threshold=0.1,\n",
    "    bbox_threshold=0.9,\n",
    "    detector_epochs=1,\n",
    "    pose_epochs=1,\n",
    "    dest_folder=\"/content/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "br3pwGf40Z4a"
   },
   "source": [
    "## Training with SuperAnimal\n",
    "In this section, we compare different ways to train the model, using and without using SuperAnimal.\n",
    "You can compare the evaluation results and get a sense of each baseline.\n",
    "We have following baselines:\n",
    "\n",
    "- ImageNet transfer learning (training without superanimal)\n",
    "- SuperAnimal transfer learning (baseline 1)\n",
    "- SuperAnimal naive fine-tuning (baseline 2)\n",
    "- SuperAnimal memory-replay fine-tuning (baseline3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2wxevEn0Z4a"
   },
   "source": [
    "#### Uploading your DLC project into Drive. Note you have to zip your DLC project and select the zipped file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 129
    },
    "id": "visacW8i0Z4a",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "db60ca6e-b38d-44c7-bc69-a327b40c8d4a"
   },
   "outputs": [],
   "source": [
    "uploaded = files.upload()\n",
    "for filename in uploaded.keys():\n",
    "    zip_file_path = os.path.join(\"/content\", filename)\n",
    "    with zipfile.ZipFile(zip_file_path, \"r\") as zip_ref:\n",
    "        zip_ref.extractall(\"/content/dlc_project_folder\")\n",
    "\n",
    "print(\"Contents of the extracted folder:\")\n",
    "extracted_files = os.listdir(\"/content/dlc_project_folder\")\n",
    "for file in extracted_files:\n",
    "    print(f\"- {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5UqfHcnxPoO"
   },
   "source": [
    "#### Change the path to your project in dlc_project_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nY7Sv9pslaMh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "dlc_proj_root = Path(\"/content/dlc_project_folder/daniel3mouse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BPvoL9uZ0Z4a"
   },
   "source": [
    "#### Comparison between different training baselines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eVmpaLdB0Z4a"
   },
   "source": [
    "Definition of data split: the unique combination of training images and testing images.\n",
    "We create a data split named split 0. All baselines will share the data split to make fair comparisons.\n",
    "- split 0 -> shared by all baselines\n",
    "- shuffle 0 (split0) -> imagenet transfer learning\n",
    "- shuffle 1 (split0) -> superanimal transfer learning\n",
    "- shuffle 2 (split0) -> superanimal naive fine-tuning\n",
    "- shuffle 3 (split0) -> superanimal memory-replay fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WofR2jytxPoR"
   },
   "source": [
    "### What is the difference between baselines?\n",
    "\n",
    "**Transfer learning** For canonical task-agnostic transfer learning,\n",
    "the encoder learns universal visual features from ImageNet, and a randomly\n",
    "initialized decoder is used to learn the pose fromthe downstream dataset.\n",
    "\n",
    "**Fine-tuning** For task aware\n",
    "fine-tuning, both encoder and decoder learn task-related visual-pose features\n",
    "in the pre-training datasets, and the decoder is fine-tuned to update pose\n",
    "priors in downstream datasets. Crucially, the network has pose-estimation-specific\n",
    "weights\n",
    "\n",
    "**ImageNet transfer-learning** The encoder was pre-trained from ImageNet. The decoder is trained from scratch in the downstream tasks\n",
    "\n",
    "**SuperAnimal transfer-learning** The encoder was pre-trained first from ImageNet, then in pose datasets we colleceted. Then decoder is trained from scratch in downstream tasks.\n",
    "\n",
    "**SuperAnimal naive fine-tuning** Both the encoder and the decoder were pre-trained in pose datasets we collected. In downstream datsets, we only finetune convolutional channels that correspond to the annotated keypoints in the downstream datasets. This introduces catastrophic forgetting in keypoints that are not annotated in the downstream datasets.\n",
    "\n",
    "**SuperAnimal memory-replay fine-tuning** If we apply fine-tuning with SuperAnimal without further cares, the models will forget about keypoints that are not annotated in the downstream datasets. To mitigate this, we mix the annotations and zero-shot predictions of SuperAnimal models to create a dataset that 'replays' the memory of the SuperAnimal keypoints.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgIsUu6v0Z4a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "imagenet_transfer_learning_shuffle = 0\n",
    "superanimal_transfer_learning_shuffle = 1\n",
    "superanimal_naive_finetune_shuffle = 2\n",
    "superanimal_memory_replay_shuffle = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kuKcxM8F0Z4a",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bb09b5cd-bd25-416b-8b94-aa25a5ffd1a7"
   },
   "outputs": [],
   "source": [
    "config_path = dlc_proj_root / \"config.yaml\"\n",
    "deeplabcut.create_training_dataset(\n",
    "    config_path,\n",
    "    Shuffles = [imagenet_transfer_learning_shuffle],\n",
    "    net_type=\"top_down_hrnet_w32\",\n",
    "    engine=Engine.PYTORCH,\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6RncQbr0Z4a"
   },
   "source": [
    "### ImageNet transfer learning\n",
    "\n",
    "Historically, the transfer learning using ImageNet weights strategies assumed no â€œanimal pose task priorsâ€ in the pretrained\n",
    "model, a paradigm adopted from previous task-agnostic transfer learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H2z8kM340Z4a",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bd322c11-20e4-4b75-da4d-fdae1b7e5937"
   },
   "outputs": [],
   "source": [
    "# Note we skip the detector training to save time. \n",
    "# The evaluation is by default using ground-truth bounding box.\n",
    "\n",
    "# But to train a model that can be used to inference videos and\n",
    "# images, you have to set detector_epochs > 0.\n",
    "deeplabcut.train_network(\n",
    "    config_path,\n",
    "    detector_epochs=0,\n",
    "    epochs=3,\n",
    "    save_epochs=3,\n",
    "    batch_size=64,\n",
    "    shuffle=imagenet_transfer_learning_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-udMck7nDbG"
   },
   "source": [
    "#### Though the evaluation was also done during training, let's just do it again here to double-check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TDHMdKz4m_16",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "091082c1-0c61-4967-9750-092541dad0ae"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config_path, Shuffles=[imagenet_transfer_learning_shuffle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GIFWU-MxPoR"
   },
   "source": [
    "### Transfer learning with SuperAnimal weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGhAuyqs0Z4a"
   },
   "source": [
    "#### Prepare training shuffle for transfer-learning with SuperAnimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wOSdZQtOp8qa",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "5c76dbca-4706-4f9d-a70d-0d7763cdcda0"
   },
   "outputs": [],
   "source": [
    "weight_init = WeightInitialization(\n",
    "    dataset=f\"{superanimal_name}\",\n",
    "    with_decoder=False,\n",
    ")\n",
    "\n",
    "deeplabcut.create_training_dataset_from_existing_split(\n",
    "    config_path,\n",
    "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
    "    shuffles=[superanimal_transfer_learning_shuffle],\n",
    "    engine=Engine.PYTORCH,\n",
    "    net_type=\"top_down_hrnet_w32\",\n",
    "    weight_init=weight_init,\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qFxlRHixPoR"
   },
   "source": [
    "#### Launch the training for transfer-learning with SuperAnimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W60UgRQWqghn",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "1ebd95f1-830a-4ba0-9f4b-71ac749ef50e"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(\n",
    "    config_path,\n",
    "    detector_epochs=0,\n",
    "    epochs=3,\n",
    "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
    "    shuffle=superanimal_transfer_learning_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzOWKiOixPoR"
   },
   "source": [
    "#### Evaluate the model obtained by transfer-learning with SuperAnimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jpO3aIAIsWbz",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "51ab747e-bf7f-4cf0-bdb5-02774439a08b"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_transfer_learning_shuffle])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Es6RR-_0Z4b"
   },
   "source": [
    "### Fine-tuning with SuperAnimal (without keeping full SuperAnimal keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6oo9oJ8XyZrn"
   },
   "source": [
    "#### Setup the weight init and dataset\n",
    "First we do keypoint matching. This steps make it possible to understand the correspondance between the existing annotations and SuperAnimal annotations. This step produces 3 outputs\n",
    "- The confusion matrix\n",
    "- The conversion table\n",
    "- Pseudo predictions over the whole dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fRm62Ji_xPoS"
   },
   "source": [
    "#### What is keypoint matching?\n",
    "\n",
    "Because SuperAnimal models have their pre-defined keypoints that are potentially different from your annotations, we porposed this algorithm to minimize the gap between the model and the dataset. We use our model to perform zero-shot inference on the whole dataset. This gives pairs of predictions and ground truth for every image. Then, we cast the matching between modelsâ€™ predictions (2D coordinates)\n",
    "and ground truth as bipartitematching using the Euclidean distance as the cost between paired of keypoints. We then solve the matching using the Hungarian algorithm. Thus for every image, we end up getting a matching matrix where 1 counts formatch and 0 counts for non-matching. Because the modelsâ€™ predictions can be noisy from image to image, we average the aforementioned matching matrix across all the images and perform another bipartite matching, resulting in the final keypoint conversion table between the model and the dataset. Note that the quality of thematching will impact the performance\n",
    "of the model, especially for zero-shot. In the case where, e.g., the annotation nose is mistakenly converted to keypoint tail and vice versa, the model will have to unlearn the channel that corresponds to nose and tail (see also case study in Mathis et al.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "vEHeuKSKyjA6",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "f85fa523-910a-444d-914f-4a67730f1bc7"
   },
   "outputs": [],
   "source": [
    "keypoint_matching(\n",
    "    config_path,\n",
    "    superanimal_name,\n",
    "    model_name,\n",
    ")\n",
    "\n",
    "conversion_table_path = dlc_proj_root / \"memory_replay\" / \"conversion_table.csv\"\n",
    "confusion_matrix_path = dlc_proj_root / \"memory_replay\" / \"confusion_matrix.png\"\n",
    "# You can visualize the pseudo predictions, or do pose embedding clustering etc.\n",
    "pseudo_prediction_path = dlc_proj_root / \"memory_replay\" / \"pseudo_predictions.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sA8yyLgs0zoO"
   },
   "source": [
    "#### Display the confusion matrix\n",
    "\n",
    "The x axis lists the keypoints in the existing annotations. The y axis lists the keypoints in SuperAnimal keypoint space. Darker color encodes stronger correspondance between the human annotation and SuperAnimal annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "luDxpD9H0zYZ",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "d6420e08-3e9c-40dc-8a13-92bc0d8c220b"
   },
   "outputs": [],
   "source": [
    "confusion_matrix_image = Image.open(confusion_matrix_path)\n",
    "\n",
    "plt.imshow(confusion_matrix_image)\n",
    "plt.axis('off')  # Hide the axes for better view\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0QWikYmy_Mj"
   },
   "source": [
    "#### Display the conversion table\n",
    "The gt columns represents the keypoint names in the existing dataset. The MasterName represents the correspoinding keypoints in SuperAnimal keypoint space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CeA-NzDMynYV",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "ae27fb36-223c-4aa2-f63f-42adadb95f02"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(conversion_table_path)\n",
    "df = df.dropna()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GkfIo8zTxPoS"
   },
   "source": [
    "#### Prepare the training shuffle and weight initialization for (naive) fine-tuning with SuperAnimal weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xEeM_hrOu6k8",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "4a5f4d5f-d1c5-42f8-f4ed-e8c208a1dc10"
   },
   "outputs": [],
   "source": [
    "table = create_conversion_table(\n",
    "    config=config_path,\n",
    "    super_animal=superanimal_name,\n",
    "    project_to_super_animal=read_conversion_table_from_csv(conversion_table_path),\n",
    ")\n",
    "\n",
    "weight_init = WeightInitialization(\n",
    "    dataset=superanimal_name,\n",
    "    with_decoder=True,\n",
    "    conversion_array=table.to_array()\n",
    ")\n",
    "\n",
    "deeplabcut.create_training_dataset_from_existing_split(\n",
    "    config_path,\n",
    "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
    "    shuffles=[superanimal_naive_finetune_shuffle],\n",
    "    engine=Engine.PYTORCH,\n",
    "    net_type=\"top_down_hrnet_w32\",\n",
    "    weight_init=weight_init,\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gZx6nr-ExPoS"
   },
   "source": [
    "#### Launch the training for (naive) fine-tuning with SuperAnimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c3XAr6uRyXOD",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "03740373-f9cd-4708-d140-0127033bfdc8"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(\n",
    "    config_path,\n",
    "    detector_epochs=0,\n",
    "    epochs=3,\n",
    "    save_epochs=3,\n",
    "    eval_interval=1,\n",
    "    batch_size=64,\n",
    "    shuffle=superanimal_naive_finetune_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXuRshzhxPoS"
   },
   "source": [
    "#### Evaluate the model obtained by (naive) fine-tuning with SuperAnimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXfdKS-H2yqw",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "53b1b8fa-6aa3-4dad-a5be-153e96eb0323"
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(\n",
    "    config_path,\n",
    "    Shuffles=[superanimal_naive_finetune_shuffle],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_nUAMlbZ0Z4b"
   },
   "source": [
    "## Memory-replay fine-tuning with SuperAnimal (keeping full SuperAnimal keypoints)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6HPu6RaxPoS"
   },
   "source": [
    "**Catastrophic forgetting** describes a\n",
    "classic problemin continual learning. Indeed, amodel gradually loses\n",
    "its ability to solve previous tasks after it learns to solve new ones.\n",
    "Fine-tuning a SuperAnimal models falls into the category of continual\n",
    "learning: the downstream dataset defines potentially different\n",
    "keypoints than those learned by the models. Thus, the models might\n",
    "forget the keypoints they learned and only pick up those defined in the\n",
    "target dataset. Here, retraining with the original dataset and the new\n",
    "one, is not a feasible option as datasets cannot be easily shared and\n",
    "more computational resources would be required.\n",
    "To counter that, we treat zero-shot inference of the model as a\n",
    "memory buffer that stores knowledge from the original model. When\n",
    "we fine-tune a SuperAnimal model, we replace the model predicted\n",
    "keypoints with the ground-truth annotations, resulting in hybrid\n",
    "learning of old and new knowledge. The quality of the zero-shot predictions\n",
    "can vary and we use the confidence of prediction (0.7) as a\n",
    "threshold to filter out low-confidence predictions. With the threshold\n",
    "set to 1, memory replay fine-tuning becomes naive-fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSLmjlCIxPoS"
   },
   "source": [
    "#### Prepare training shuffle and weight initialization for memory-replay finetuning with SuperAnimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BKEF76AI0Z4c",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "bf107c7b-6e3c-4ece-f680-067e4d7641f0",
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "weight_init = WeightInitialization(\n",
    "    dataset=superanimal_name,\n",
    "    conversion_array=table.to_array(),\n",
    "    with_decoder=True,\n",
    "    memory_replay=True,\n",
    ")\n",
    "\n",
    "deeplabcut.create_training_dataset_from_existing_split(\n",
    "    config_path,\n",
    "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
    "    shuffles=[superanimal_memory_replay_shuffle],\n",
    "    engine=Engine.PYTORCH,\n",
    "    net_type=\"top_down_hrnet_w32\",\n",
    "    weight_init=weight_init,\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKwJiIyKxPoT"
   },
   "source": [
    "#### Launch the training for memory-replay fine-tuning with SuperAnimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ru8tIFmD2Mkv",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "81a0ec13-6ba7-4089-bed5-f19c6bae0bcb"
   },
   "outputs": [],
   "source": [
    "deeplabcut.train_network(\n",
    "    config_path,\n",
    "    detector_epochs=0,\n",
    "    epochs=3,\n",
    "    save_epochs=3,\n",
    "    eval_interval=1,\n",
    "    batch_size=64,   # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
    "    shuffle=superanimal_naive_finetune_shuffle,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i-2MBRDjxPoT"
   },
   "source": [
    "#### Evaluate the model obtained by memory-replay finetuning with SuperAnimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sfMcK3gq8WxZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_memory_replay_shuffle])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "013e4fb17d234b18934f11593557eec4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3b1eaec29f014a97a8ee14e62b34db83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4aba38bf1a9b4be092fffb7581f7cd0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_787f617ecbf74db6b020a066acb12840",
      "max": 165432914,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7cd0520e8c26480297b86824e92d77ec",
      "value": 165432914
     }
    },
    "579cc60d128445b8ae10ee036c036bb5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "69a82035258c40b5beed0d9673f8b3b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c5d2d04a28d743799dfa706e00303f08",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_579cc60d128445b8ae10ee036c036bb5",
      "value": "â€‡165M/165Mâ€‡[00:03&lt;00:00,â€‡36.1MB/s]"
     }
    },
    "787f617ecbf74db6b020a066acb12840": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7cd0520e8c26480297b86824e92d77ec": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "867c5b27246c40b3823c60a014329427": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3b1eaec29f014a97a8ee14e62b34db83",
      "placeholder": "â€‹",
      "style": "IPY_MODEL_013e4fb17d234b18934f11593557eec4",
      "value": "model.safetensors:â€‡100%"
     }
    },
    "c16a46a621ab497183ec0b136749eaeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c5d2d04a28d743799dfa706e00303f08": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fec6601a01a44935a06d432c1f94349b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_867c5b27246c40b3823c60a014329427",
       "IPY_MODEL_4aba38bf1a9b4be092fffb7581f7cd0b",
       "IPY_MODEL_69a82035258c40b5beed0d9673f8b3b6"
      ],
      "layout": "IPY_MODEL_c16a46a621ab497183ec0b136749eaeb"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
