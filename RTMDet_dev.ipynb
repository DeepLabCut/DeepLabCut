{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6eec817-48c7-40d7-92ab-f20705562a62",
   "metadata": {},
   "source": [
    "# Let's try to figure out how to develop RTMDet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8618fe-8064-4c3c-9840-ff0c19525aa7",
   "metadata": {},
   "source": [
    "## First, let's understand what is the difference between CSPNext and RTMPose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a7e73-0b2e-44a1-a575-3579f3eb168c",
   "metadata": {},
   "source": [
    "### (RTMPose is CSPNext + SimCC HEAD?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8920742e-91fd-4887-b7b4-fc39f69ac626",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# ----\n",
    "\n",
    "# First, let's create a CSPNeXt object\n",
    "from deeplabcut.pose_estimation_pytorch.models.backbones.cspnext import CSPNeXt\n",
    "\n",
    "cspnext = CSPNeXt()\n",
    "print(type(cspnext)) # cspnext is of type deeplabcut.pose_estimation_pytorch.models.backbones.cspnext.CSPNeXt , which is a nn.Module\n",
    "print(f\"Nb parameters: {sum(p.numel() for p in cspnext.parameters())}\")\n",
    "# print(cspnext) # This floods the output\n",
    "y = cspnext(x)\n",
    "print(f\"Shape of output of dummy tensor: {y.shape}\")\n",
    "\n",
    "print(\"-\"* 80)\n",
    "\n",
    "# Next, let's create a RTMPose nn.Module object\n",
    "from deeplabcut.pose_estimation_pytorch.config.utils import get_config_folder_path, replace_default_values\n",
    "from deeplabcut.core.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "\n",
    "net_type = \"rtmpose_m\" # for example\n",
    "nb_bodyparts = 5 # for example\n",
    "\n",
    "configs_dir = get_config_folder_path()\n",
    "architecture = net_type.split(\"_\")[0]\n",
    "cfg_path = cfg_path = configs_dir / architecture / f\"{net_type}.yaml\"\n",
    "model_cfg = read_config_as_dict(cfg_path)\n",
    "model_cfg = replace_default_values(\n",
    "    model_cfg,\n",
    "    num_bodyparts=nb_bodyparts,\n",
    ") # Interesting observation: the yaml file defines values that depend on nb_individuals and/or nb_bodyparts, and are updated with real values once known (when creating the actual pytorch_config.yaml)\n",
    "rtmpose = PoseModel.build(model_cfg[\"model\"]) # here, there might be some optional parameters, todo investigate\n",
    "print(type(rtmpose))\n",
    "print(f\"Nb parameters: {sum(p.numel() for p in rtmpose.parameters())}\")\n",
    "# print(rtmpose) # This floods the output\n",
    "z = rtmpose(x)\n",
    "print(f\"Shape of output of dummy tensor: {z['bodypart']['x'].shape} , {z['bodypart']['y'].shape}\")\n",
    "# Okay so basically, the first level keys are defined by the pytorch_config.yaml (in the heads block), and the second level keys are defined in the RTMCCHead forward method.\n",
    "# The first level keys are certainly added when doing PoseModel.build\n",
    "# When printing rtmpose, there are different submodules (backbone, head). They are very certainly created during PoseModel.build()\n",
    "# The differences in sizes of rtmpose are the same ones as the difference in sizes of cspnext. \n",
    "# The different parameters (deepen_factor, widen_factor, backbone_output_channels) are configured in the rtmpose yaml files.\n",
    "\n",
    "# - state_dict() returns a dictionary containing all the model’s learnable parameters and buffers,\n",
    "# while load_state_dict() restores those values into a model with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3521a-ab8c-44dc-81dd-975cb8c9d864",
   "metadata": {},
   "source": [
    "## Then, let's dive into how the existing detectors work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f5165-9431-4bb8-abcc-298b9524fed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deeplabcut.pose_estimation_pytorch.models.detectors.fasterRCNN import FasterRCNN\n",
    "\n",
    "# Instantiate a pretrained Faster R-CNN with a MobileNetV3 backbone\n",
    "detector = FasterRCNN(\n",
    "    variant=\"fasterrcnn_mobilenet_v3_large_fpn\",  # or \"fasterrcnn_resnet50_fpn\"\n",
    "    pretrained=True,                              # load COCO pretrained weights\n",
    "    box_score_thresh=0.05,                        # filter weak detections\n",
    ")\n",
    "# This throws away the Head and loads an other one, so the Head is not pretrained anymore.\n",
    "\n",
    "# Dummy input batch of 2 RGB images, 3x224x224 each\n",
    "images = [torch.rand(3, 224, 224), torch.rand(3, 224, 224)]\n",
    "\n",
    "# During inference (no targets)\n",
    "detector.eval()\n",
    "with torch.no_grad():\n",
    "    losses, detections = detector(images)\n",
    "\n",
    "print(detections)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfc8ef3-a43a-478b-a87b-ecec9655a1b9",
   "metadata": {},
   "source": [
    "## Finally, let's instantiate the official RTMDet network for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c510989-d6bf-47d7-b2f1-5e8d54623fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this must be performed in the openmmlab environment.\n",
    "from mmdet.utils import register_all_modules\n",
    "register_all_modules()\n",
    "\n",
    "import torch\n",
    "from mmengine import Config\n",
    "from mmdet.registry import MODELS\n",
    "\n",
    "#x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# Load an RTMDet config — you can use any variant: tiny, s, m, l, x\n",
    "rtmdet_cfg = Config.fromfile('/home/max/Work/mmdetection/configs/rtmdet/rtmdet_tiny_8xb32-300e_coco.py')\n",
    "\n",
    "# Build the model\n",
    "rtmdet = MODELS.build(rtmdet_cfg.model)\n",
    "\n",
    "# Put it in evaluation mode (no gradients, etc.)\n",
    "rtmdet.eval()\n",
    "\n",
    "print(type(rtmdet))\n",
    "\n",
    "print(f\"Nb parameters: {sum(p.numel() for p in rtmdet.parameters())}\")\n",
    "\n",
    "#print(rtmdet) # This floods the output\n",
    "print(type(rtmdet.backbone))\n",
    "print(type(rtmdet.neck))\n",
    "print(type(rtmdet.bbox_head))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2646a08e-4aa2-47ea-8088-e59bb5bfb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mmdet.apis import DetInferencer\n",
    "\n",
    "inferencer = DetInferencer(\n",
    "    model=\"/home/max/Work/mmdetection/rtmdet_tiny_8xb32-300e_coco.py\",\n",
    "    weights=\"/home/max/Work/mmdetection/rtmdet_tiny_8xb32-300e_coco_20220902_112414-78e30dcc.pth\",\n",
    "    device=\"cpu\",\n",
    ")\n",
    "\n",
    "inferencer.model.test_cfg.chunked_size = -1\n",
    "\n",
    "inferencer(\n",
    "    inputs=\"/home/max/Work/mmdetection/demo/demo.jpg\",\n",
    "    out_dir=\"/home/max/Work/mmdetection/outputs\",\n",
    "    no_save_pred=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
