Our pose-aware ReID transformer takes features from the pre-trained multi-task CNNs and are used to train a shallow transformer with triplet loss. This model is then used to provide a stitching loss for appearance-based tracking.

Part of our transformer code is inspired by: https://github.com/damo-cv/TransReID
