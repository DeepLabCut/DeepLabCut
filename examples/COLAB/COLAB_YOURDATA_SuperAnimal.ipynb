{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMathisLab/DeepLabCut/blob/main/examples/COLAB/COLAB_YOURDATA_SuperAnimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SSZpZUu0Z4S"
      },
      "source": [
        "# DeepLabCut Model Zoo: SuperAnimal models\n",
        "\n",
        "![alt text](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1616492373700-PGOAC72IOB6AUE47VTJX/ke17ZwdGBToddI8pDm48kB8JrdUaZR-OSkKLqWQPp_YUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnBqyW03PFN2MN6T6ry5cmXqqA9xITfsbVGDrg_goIDasRCalqV8R3606BuxERAtDaQ/modelzoo.png?format=1000w)\n",
        "\n",
        "# ü¶Ñ SuperAnimal in DeepLabCut PyTorch! üî•\n",
        "\n",
        "This notebook demos how to use our SuperAnimal models within DeepLabCut 3.0! Please read more in [Ye et al. Nature Communications 2024](https://www.nature.com/articles/s41467-024-48792-2) about the available SuperAnimal models, and follow along below!\n",
        "\n",
        "### **Let's get going: install the latest version of DeepLabCut into COLAB:**\n",
        "\n",
        "*Also, be sure you are connected to a GPU: go to menu, click Runtime > Change Runtime Type > select \"GPU\"*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AjET5cJE5UYM",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/DeepLabCut/DeepLabCut.git@mwm/humanbody\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0vq6E50Z4W"
      },
      "source": [
        "**PLEASE, click \"restart runtime\" from the output above before proceeding!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvnlIvQm0Z4X",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import deeplabcut\n",
        "import deeplabcut.utils.auxiliaryfunctions as auxiliaryfunctions\n",
        "from deeplabcut.pose_estimation_pytorch.apis import (\n",
        "    superanimal_analyze_images,\n",
        ")\n",
        "from deeplabcut.modelzoo import build_weight_init\n",
        "from deeplabcut.modelzoo.utils import (\n",
        "    create_conversion_table,\n",
        "    read_conversion_table_from_csv,\n",
        ")\n",
        "from deeplabcut.modelzoo.video_inference import video_inference_superanimal\n",
        "from deeplabcut.utils.pseudo_label import keypoint_matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeXjmtu40Z4X"
      },
      "source": [
        "## Zero-shot Image & Video Inference\n",
        "SuperAnimal models are foundation animal pose models. They can be used for zero-shot predictions without further training on the data.\n",
        "In this section, we show how to use SuperAnimal models to predict pose from images (given an image folder) and output the predicted images (with pose) into another destination folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFzntDMxPoL"
      },
      "source": [
        "### Zero-shot image inference\n",
        "\n",
        "If you have a single Image you want to test, upload it here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbDsZQfsxPoL"
      },
      "source": [
        "#### Upload the images you want to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4yfTj7r0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "image_path = os.path.abspath(filepath)\n",
        "image_name = os.path.splitext(image_path)[0]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your video via the Files menu to the left\n",
        "# and define `image_path` yourself with right click > copy path on the image:\n",
        "#\n",
        "# image_path = \"/path/to/my/image.png\"\n",
        "# image_name = os.path.splitext(image_path)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jashzdjb0Z4Y"
      },
      "source": [
        "#### Select a SuperAnimal name and corresponding model architecture\n",
        "\n",
        "Check Our Docs on [SuperAnimals](https://github.com/DeepLabCut/DeepLabCut/blob/main/docs/ModelZoo.md) to learn more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH9LXig90Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\"]\n",
        "detector_name = \"fasterrcnn_resnet50_fpn_v2\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown What is the maximum number of animals you expect to have in an image\n",
        "max_individuals = 3  # @param {type:\"slider\", min:1, max:30, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmJtVmHq0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Note you need to enter max_individuals correctly to get the correct number of predictions in the image.\n",
        "_ = superanimal_analyze_images(\n",
        "    superanimal_name,\n",
        "    model_name,\n",
        "    detector_name,\n",
        "    image_path,\n",
        "    max_individuals,\n",
        "    out_folder=\"/content/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VEjHu-00Z4Y"
      },
      "source": [
        "### Zero-shot Video Inference\n",
        "\n",
        "This can be done with or without video adaptation (faster, but not self-supervised fine-tuned on your data!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGoAhxZOxPoM"
      },
      "source": [
        "#### Upload a video you want to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "PK3efA0I0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "42f0b220-eb2f-470b-b559-1a8d6a23f073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-8cd596b4-b243-413b-b281-ccd7d6897cff\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-8cd596b4-b243-413b-b281-ccd7d6897cff\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving jasper.mov to jasper.mov\n",
            "User uploaded file 'jasper.mov' with length 3471938 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "video_path = os.path.abspath(filepath)\n",
        "video_name = os.path.splitext(video_path)[0]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your video via the Files menu to the left\n",
        "# and define `video_path` yourself with right click > copy path on the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoA-RATSICj_"
      },
      "source": [
        "#### Choose the superanimal and the model name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "OiRAP9XD0Z4Z",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_quadruped\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\", \"superanimal_superbird\", \"superanimal_humanbody\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\", \"rtmpose_x\"]\n",
        "detector_name = \"fasterrcnn_resnet50_fpn_v2\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown What is the maximum number of animals you expect to have in an image\n",
        "max_individuals = 3  # @param {type:\"slider\", min:1, max:30, step:1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv3v0QgSJNOg"
      },
      "source": [
        "#### Zero-shot Video Inference without video adaptation\n",
        "\n",
        "The labeled video (and pose predictions for the video) are saved in `\"/content/\"`, with the labeled video name being `{your_video_name}_superanimal_{superanimal_name}_hrnetw32_labeled.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "poqynL0UJTBp",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "fa91289b-03e2-4ee5-c9e1-19289f17ba08",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "989416e0a07342ea9f7eaf3447547996",
            "76f1609d3cb04a46b67b6f5b5ccb2d2d",
            "c2918207dc8246e4b16705c143014e5b",
            "ba7e0159cdf04133b72c79ee19192b02",
            "95a6e10a4a8b4dd3b67ee9dc59a7e7d9",
            "0cda3eaea9a040b887d4022e9f433605",
            "547c3c81aa6b4caabfb8ee8f9f8d0f26",
            "56ff4cc30dce4e568cf05f5f51673167",
            "771bf74fb7594a2ab45246a9b4c12584",
            "c8394528aef94e10b917163e50e0cbd4",
            "e725fa852e2048c2bbd3521f7d8f9907"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running video inference on /content/jasper.mov with superanimal_quadruped_hrnet_w32\n",
            "Using pytorch for model hrnet_w32\n",
            "Loading.... superanimal_quadruped_fasterrcnn_resnet50_fpn_v2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "(‚Ä¶)_quadruped_fasterrcnn_resnet50_fpn_v2.pt:   0%|          | 0.00/173M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "989416e0a07342ea9f7eaf3447547996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_v2_coco-dd69338a.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 167M/167M [00:00<00:00, 178MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing video /content/jasper.mov\n",
            "Starting to analyze /content/jasper.mov\n",
            "Video metadata: \n",
            "  Overall # of frames:    108\n",
            "  Duration of video [s]:  3.60\n",
            "  fps:                    30.0\n",
            "  resolution:             w=720, h=1280\n",
            "\n",
            "Running detector with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:   0%|          | 0/108 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:   2%|‚ñè         | 2/108 [00:00<00:29,  3.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:   3%|‚ñé         | 3/108 [00:00<00:24,  4.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:   5%|‚ñç         | 5/108 [00:01<00:19,  5.26it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:   6%|‚ñã         | 7/108 [00:01<00:17,  5.63it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:   8%|‚ñä         | 9/108 [00:01<00:16,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  10%|‚ñà         | 11/108 [00:02<00:16,  5.90it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  12%|‚ñà‚ñè        | 13/108 [00:02<00:15,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  14%|‚ñà‚ñç        | 15/108 [00:02<00:15,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  16%|‚ñà‚ñå        | 17/108 [00:03<00:15,  5.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  18%|‚ñà‚ñä        | 19/108 [00:03<00:14,  5.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  19%|‚ñà‚ñâ        | 21/108 [00:03<00:14,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  21%|‚ñà‚ñà‚ñè       | 23/108 [00:04<00:14,  5.84it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  22%|‚ñà‚ñà‚ñè       | 24/108 [00:04<00:14,  5.74it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  24%|‚ñà‚ñà‚ñç       | 26/108 [00:04<00:14,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  25%|‚ñà‚ñà‚ñå       | 27/108 [00:04<00:14,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  27%|‚ñà‚ñà‚ñã       | 29/108 [00:05<00:14,  5.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  28%|‚ñà‚ñà‚ñä       | 30/108 [00:05<00:14,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  29%|‚ñà‚ñà‚ñä       | 31/108 [00:05<00:14,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  31%|‚ñà‚ñà‚ñà       | 33/108 [00:05<00:13,  5.47it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  31%|‚ñà‚ñà‚ñà‚ñè      | 34/108 [00:06<00:13,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  33%|‚ñà‚ñà‚ñà‚ñé      | 36/108 [00:06<00:12,  5.77it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  35%|‚ñà‚ñà‚ñà‚ñå      | 38/108 [00:06<00:11,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/108 [00:07<00:11,  5.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/108 [00:07<00:11,  5.94it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/108 [00:07<00:10,  6.03it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/108 [00:08<00:10,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/108 [00:08<00:10,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  46%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/108 [00:08<00:09,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/108 [00:09<00:09,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/108 [00:09<00:08,  6.01it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/108 [00:09<00:08,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 58/108 [00:10<00:08,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/108 [00:10<00:08,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 62/108 [00:10<00:07,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/108 [00:11<00:07,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 66/108 [00:11<00:07,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/108 [00:11<00:06,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 70/108 [00:12<00:06,  6.00it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/108 [00:12<00:06,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 74/108 [00:12<00:05,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/108 [00:13<00:05,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 78/108 [00:13<00:05,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/108 [00:13<00:04,  5.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 82/108 [00:14<00:04,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/108 [00:14<00:04,  5.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 86/108 [00:14<00:03,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/108 [00:15<00:03,  5.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 90/108 [00:15<00:03,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/108 [00:15<00:02,  5.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 94/108 [00:16<00:02,  5.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 95/108 [00:16<00:02,  5.72it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 97/108 [00:16<00:01,  5.66it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 98/108 [00:16<00:01,  5.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/108 [00:17<00:01,  5.73it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 101/108 [00:17<00:01,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 103/108 [00:17<00:00,  5.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rDetector:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/108 [00:17<00:00,  5.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 106/108 [00:18<00:00,  5.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:18<00:00,  5.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "DEBUG: Device: cuda:0\n",
            "DEBUG: Model threshold: 0.9\n",
            "DEBUG: Processing image 0\n",
            "DEBUG: Image size: (720, 1280)\n",
            "DEBUG: Using transforms preprocessing\n",
            "DEBUG: Batch tensor shape: torch.Size([3, 1280, 720])\n",
            "DEBUG: Raw predictions type: <class 'dict'>\n",
            "DEBUG: Raw predictions keys: dict_keys([])\n",
            "DEBUG: Unexpected output format\n",
            "Running pose prediction with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pose: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 108/108 [00:01<00:00, 76.68it/s]\n",
            "WARNING:root:Only 0 of 108 frames had detections!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No objects were detected in the video. This can happen if:\n1. The video doesn't contain the type of objects the model was trained to detect\n2. The objects are too small, blurry, or occluded\n3. The detector confidence threshold is too high\n4. The video quality is poor\n\nTry:\n- Using a different video with clearer objects\n- Adjusting the detector confidence threshold\n- Checking if the model is appropriate for your use case",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-22-2412961881.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpickle\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFALSE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m _ = video_inference_superanimal(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvideos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvideo_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msuperanimal_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuperanimal_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deeplabcut/modelzoo/video_inference.py\u001b[0m in \u001b[0;36mvideo_inference_superanimal\u001b[0;34m(videos, superanimal_name, model_name, detector_name, scale_list, videotype, dest_folder, cropping, video_adapt, plot_trajectories, batch_size, detector_batch_size, pcutoff, adapt_iterations, pseudo_threshold, bbox_threshold, detector_epochs, pose_epochs, max_individuals, video_adapt_batch_size, device, customized_pose_checkpoint, customized_detector_checkpoint, customized_model_config, plot_bboxes)\u001b[0m\n\u001b[1;32m    617\u001b[0m             \u001b[0mpose_model_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapted_pose_checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m         return _video_inference_superanimal(\n\u001b[0m\u001b[1;32m    620\u001b[0m             \u001b[0mvideos\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m             \u001b[0msuperanimal_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deeplabcut/pose_estimation_pytorch/modelzoo/inference.py\u001b[0m in \u001b[0;36m_video_inference_superanimal\u001b[0;34m(video_paths, superanimal_name, model_cfg, model_snapshot_path, detector_snapshot_path, max_individuals, pcutoff, batch_size, detector_batch_size, cropping, dest_folder, output_suffix, plot_bboxes, bboxes_pcutoff)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Saving results to {dest_folder}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m         df = create_df_from_prediction(\n\u001b[0m\u001b[1;32m    159\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0mdlc_scorer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdlc_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/deeplabcut/pose_estimation_pytorch/apis/videos.py\u001b[0m in \u001b[0;36mcreate_df_from_prediction\u001b[0;34m(predictions, dlc_scorer, multi_animal, model_cfg, output_path, output_prefix, save_as_csv)\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;31m# Check if any predictions were made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;34m\"No objects were detected in the video. This can happen if:\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m             \u001b[0;34m\"1. The video doesn't contain the type of objects the model was trained to detect\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No objects were detected in the video. This can happen if:\n1. The video doesn't contain the type of objects the model was trained to detect\n2. The objects are too small, blurry, or occluded\n3. The detector confidence threshold is too high\n4. The video quality is poor\n\nTry:\n- Using a different video with clearer objects\n- Adjusting the detector confidence threshold\n- Checking if the model is appropriate for your use case"
          ]
        }
      ],
      "source": [
        "from pickle import FALSE\n",
        "_ = video_inference_superanimal(\n",
        "    videos=video_path,\n",
        "    superanimal_name=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    pcutoff=0.4,\n",
        "    video_adapt=False,\n",
        "    max_individuals=max_individuals,\n",
        "    dest_folder=\"/content/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Z5GSti0Z4Z"
      },
      "source": [
        "#### Zero-shot Video Inference with video adaptation (unsupervised)\n",
        "\n",
        "The labeled video (and pose predictions for the video) are saved in `\"/content/\"`, with the labeled video name being `{your_video_name}_superanimal_{superanimal_name}_hrnetw32_labeled_after_adapt.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mhOmtzw0Z4Z",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "_ = video_inference_superanimal(\n",
        "    videos=[video_path],\n",
        "    superanimal_name=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    video_adapt=True,\n",
        "    max_individuals=max_individuals,\n",
        "    pseudo_threshold=0.1,\n",
        "    bbox_threshold=0.9,\n",
        "    detector_epochs=1,\n",
        "    pose_epochs=1,\n",
        "    dest_folder=\"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br3pwGf40Z4a"
      },
      "source": [
        "## Training with SuperAnimal\n",
        "\n",
        "In this section, we compare different ways to train models in DeepLabCut 3.0, with or without using SuperAnimal-pretrained models.\n",
        "You can compare the evaluation results and get a sense of each baseline. We have following baselines:\n",
        "\n",
        "- ImageNet transfer learning (training without superanimal)\n",
        "- SuperAnimal transfer learning (baseline 1)\n",
        "- SuperAnimal naive fine-tuning (baseline 2)\n",
        "- SuperAnimal memory-replay fine-tuning (baseline3)\n",
        "\n",
        "This is done on one of your DeepLabCut projects! If you don't have a DeepLabCut project that you can use SuperAnimal models with, you can always using the example openfield dataset [available in the DeepLabCut repository](https://github.com/DeepLabCut/DeepLabCut/tree/main/examples/openfield-Pranav-2018-10-30) or the Tri-Mouse dataset available on [Zenodo](https://zenodo.org/records/5851157)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPy5VgDDhD6o"
      },
      "source": [
        "### Preparing the DeepLabCut Project\n",
        "\n",
        "First, place your DeepLabCut project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXzBBV8ehDR9"
      },
      "outputs": [],
      "source": [
        "# Now, let's link to your GoogleDrive. Run this cell and follow the\n",
        "# authorization instructions:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QmTftBMo4h6"
      },
      "source": [
        "You will need to edit the project path in the config.yaml file to be set to your Google Drive link!\n",
        "\n",
        "Typically, this will be in the format: `/content/drive/MyDrive/yourProjectFolderName`. You can obtain this path by going to the file navigator in the left pane, finding your DeepLabCut project folder, clicking on the vertical `...` next to the folder name and selecting \"Copy path\".\n",
        "\n",
        "If the `drive` folder is not immediately visible after mounting the drive, refresh the available files!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iFFEYAB7Uum"
      },
      "outputs": [],
      "source": [
        "# TODO: Update the `project_path` to be the path of your DeepLabCut project!\n",
        "project_path = Path(\"/content/drive/MyDrive/my-project-2024-07-17\")\n",
        "config_path = str(project_path / \"config.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZTG3Eo475w0"
      },
      "source": [
        "Then, use the panel below to select the appropriate SuperAnimal model for your project (don't forget to run the cell)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8NtCy1Jo0bu"
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\"]\n",
        "detector_name = \"fasterrcnn_resnet50_fpn_v2\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPvoL9uZ0Z4a"
      },
      "source": [
        "### Comparison between different training baselines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVmpaLdB0Z4a"
      },
      "source": [
        "Definition of data split: the unique combination of training images and testing images.\n",
        "We create a data split named split 0. All baselines will share the data split to make fair comparisons.\n",
        "- split 0 -> shared by all baselines\n",
        "- shuffle 0 (split0) -> imagenet transfer learning\n",
        "- shuffle 1 (split0) -> superanimal transfer learning\n",
        "- shuffle 2 (split0) -> superanimal naive fine-tuning\n",
        "- shuffle 3 (split0) -> superanimal memory-replay fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WofR2jytxPoR"
      },
      "source": [
        "### What is the difference between baselines?\n",
        "\n",
        "**Transfer learning** For canonical task-agnostic transfer learning,\n",
        "the encoder learns universal visual features from a large pre-training dataset, and a randomly\n",
        "initialized decoder is used to learn the pose from the downstream dataset.\n",
        "\n",
        "**Fine-tuning** For task aware\n",
        "fine-tuning, both encoder and decoder learn task-related visual-pose features\n",
        "in the pre-training datasets, and the decoder is fine-tuned to update pose\n",
        "priors in downstream datasets. Crucially, the network has pose-estimation-specific\n",
        "weights\n",
        "\n",
        "**ImageNet transfer-learning** The encoder was pre-trained from ImageNet. The decoder is trained from scratch in the downstream tasks\n",
        "\n",
        "**SuperAnimal transfer-learning** The encoder was pre-trained first from ImageNet, then in pose datasets we colleceted. Then decoder is trained from scratch in downstream tasks.\n",
        "\n",
        "**SuperAnimal naive fine-tuning** Both the encoder and the decoder were pre-trained in pose datasets we collected. In downstream datasets, we only finetune convolutional channels that correspond to the annotated keypoints in the downstream datasets. This introduces catastrophic forgetting in keypoints that are not annotated in the downstream datasets.\n",
        "\n",
        "**SuperAnimal memory-replay fine-tuning** If we apply fine-tuning with SuperAnimal without further cares, the models will forget about keypoints that are not annotated in the downstream datasets. To mitigate this, we mix the annotations and zero-shot predictions of SuperAnimal models to create a dataset that 'replays' the memory of the SuperAnimal keypoints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgIsUu6v0Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "imagenet_transfer_learning_shuffle = 0\n",
        "superanimal_transfer_learning_shuffle = 1\n",
        "superanimal_naive_finetune_shuffle = 2\n",
        "superanimal_memory_replay_shuffle = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuKcxM8F0Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.create_training_dataset(\n",
        "    config_path,\n",
        "    Shuffles=[imagenet_transfer_learning_shuffle],\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6RncQbr0Z4a"
      },
      "source": [
        "### ImageNet transfer learning\n",
        "\n",
        "Historically, the transfer learning using ImageNet weights strategies assumed no ‚Äúanimal pose task priors‚Äù in the pretrained\n",
        "model, a paradigm adopted from previous task-agnostic transfer learning.\n",
        "\n",
        "You can change the number of epochs you want to train for. How long training will take depends on many parameters, including the number of images in your dataset, the resolution of the images, and the number of epochs you train for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2z8kM340Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Note we skip the detector training to save time.\n",
        "# For Top-Down models, the evaluation is by default using ground-truth bounding\n",
        "#  boxes. But to train a model that can be used to inference videos and images,\n",
        "#  you have to set detector_epochs > 0.\n",
        "\n",
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=imagenet_transfer_learning_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-udMck7nDbG"
      },
      "source": [
        "Now let's evaluate the performance of our trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDHMdKz4m_16",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[imagenet_transfer_learning_shuffle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GIFWU-MxPoR"
      },
      "source": [
        "### Transfer learning with SuperAnimal weights\n",
        "\n",
        "First, we prepare training shuffle for transfer-learning with SuperAnimal weights. As we've already create a shuffle with a train/test split that we want to reuse, we use `deeplabcut.create_training_dataset_from_existing_split` to keep the same train/test indices as in the ImageNet transfer learning shuffle.\n",
        "\n",
        "We specify that we want to initialize the model weights with the selected SuperAnimal model, but without keeping the decoding layers (this is called transfer learning)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOSdZQtOp8qa",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=False,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_transfer_learning_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qFxlRHixPoR"
      },
      "source": [
        "Then, we launch the training for transfer-learning with SuperAnimal weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W60UgRQWqghn",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_transfer_learning_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzOWKiOixPoR"
      },
      "source": [
        "Finally, we evaluate the model obtained by transfer-learning with SuperAnimal weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpO3aIAIsWbz",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_transfer_learning_shuffle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Es6RR-_0Z4b"
      },
      "source": [
        "### Fine-tuning with SuperAnimal (without keeping full SuperAnimal keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oo9oJ8XyZrn"
      },
      "source": [
        "#### Setup the weight init and dataset\n",
        "\n",
        "First we do keypoint matching. This steps make it possible to understand the correspondence between the existing annotations and SuperAnimal annotations. This step produces 3 outputs\n",
        "- The confusion matrix\n",
        "- The conversion table\n",
        "- Pseudo predictions over the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRm62Ji_xPoS"
      },
      "source": [
        "#### What is keypoint matching?\n",
        "\n",
        "Because SuperAnimal models have their pre-defined keypoints that are potentially different from your annotations, we proposed this algorithm to minimize the gap between the model and the dataset. We use our model to perform zero-shot inference on the whole dataset. This gives pairs of predictions and ground truth for every image. Then, we cast the matching between models‚Äô predictions (2D coordinates)\n",
        "and ground truth as bipartitematching using the Euclidean distance as the cost between paired of keypoints. We then solve the matching using the Hungarian algorithm. Thus for every image, we end up getting a matching matrix where 1 counts formatch and 0 counts for non-matching. Because the models‚Äô predictions can be noisy from image to image, we average the aforementioned matching matrix across all the images and perform another bipartite matching, resulting in the final keypoint conversion table between the model and the dataset. Note that the quality of thematching will impact the performance\n",
        "of the model, especially for zero-shot. In the case where, e.g., the annotation nose is mistakenly converted to keypoint tail and vice versa, the model will have to unlearn the channel that corresponds to nose and tail (see also case study in Mathis et al.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEHeuKSKyjA6",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "keypoint_matching(\n",
        "    config_path,\n",
        "    superanimal_name,\n",
        "    model_name,\n",
        "    detector_name,\n",
        "    copy_images=True,\n",
        ")\n",
        "\n",
        "conversion_table_path = project_path / \"memory_replay\" / \"conversion_table.csv\"\n",
        "confusion_matrix_path = project_path / \"memory_replay\" / \"confusion_matrix.png\"\n",
        "\n",
        "# You can visualize the pseudo predictions, or do pose embedding clustering etc.\n",
        "pseudo_prediction_path = project_path / \"memory_replay\" / \"pseudo_predictions.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA8yyLgs0zoO"
      },
      "source": [
        "#### Display the confusion matrix\n",
        "\n",
        "The x axis lists the keypoints in the existing annotations. The y axis lists the keypoints in SuperAnimal keypoint space. Darker color encodes stronger correspondence between the human annotation and SuperAnimal annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luDxpD9H0zYZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "confusion_matrix_image = Image.open(confusion_matrix_path)\n",
        "\n",
        "plt.imshow(confusion_matrix_image)\n",
        "plt.axis('off')  # Hide the axes for better view\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0QWikYmy_Mj"
      },
      "source": [
        "#### Display the conversion table\n",
        "The gt columns represents the keypoint names in the existing dataset. The MasterName represents the corresponding keypoints in SuperAnimal keypoint space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeA-NzDMynYV",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(conversion_table_path)\n",
        "df = df.dropna()\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhWakwQ0JKCQ"
      },
      "source": [
        "#### Adding the Conversion Table to your project's `config.yaml` file\n",
        "\n",
        "Once you've run keypoint matching, you can add the conversion table to your project's `config.yaml` file, and edit it if there are some matches you think are wrong. As an example, for a top-view mouse dataset with 4 bodyparts labeled (`'snout', 'leftear', 'rightear', 'tailbase'`), the conversion table mapping project bodyparts to SuperAnimal bodyparts would be added as:\n",
        "\n",
        "```yaml\n",
        "# Conversion tables to fine-tune SuperAnimal weights\n",
        "SuperAnimalConversionTables:\n",
        "  superanimal_topviewmouse:\n",
        "    snout: nose\n",
        "    leftear: left_ear\n",
        "    rightear: right_ear\n",
        "    tailbase: tail_base\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7nypq5IJKCQ"
      },
      "outputs": [],
      "source": [
        "create_conversion_table(\n",
        "    config=config_path,\n",
        "    super_animal=superanimal_name,\n",
        "    project_to_super_animal=read_conversion_table_from_csv(\n",
        "        conversion_table_path\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkfIo8zTxPoS"
      },
      "source": [
        "#### Prepare the training shuffle and weight initialization for (naive) fine-tuning with SuperAnimal weights\n",
        "\n",
        "Then, when you call `build_weight_init` with `with_decoder=True`, the conversion table in your project's `config.yaml` is used to get predictions for the correct bodyparts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeM_hrOu6k8",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=True,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_naive_finetune_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZx6nr-ExPoS"
      },
      "source": [
        "#### Launch the training for (naive) fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3XAr6uRyXOD",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_naive_finetune_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXuRshzhxPoS"
      },
      "source": [
        "#### Evaluate the model obtained by (naive) fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXfdKS-H2yqw",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(\n",
        "    config_path,\n",
        "    Shuffles=[superanimal_naive_finetune_shuffle],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nUAMlbZ0Z4b"
      },
      "source": [
        "### Memory-replay fine-tuning with SuperAnimal (keeping full SuperAnimal keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6HPu6RaxPoS"
      },
      "source": [
        "**Catastrophic forgetting** describes a\n",
        "classic problemin continual learning. Indeed, amodel gradually loses\n",
        "its ability to solve previous tasks after it learns to solve new ones.\n",
        "Fine-tuning a SuperAnimal models falls into the category of continual\n",
        "learning: the downstream dataset defines potentially different\n",
        "keypoints than those learned by the models. Thus, the models might\n",
        "forget the keypoints they learned and only pick up those defined in the\n",
        "target dataset. Here, retraining with the original dataset and the new\n",
        "one, is not a feasible option as datasets cannot be easily shared and\n",
        "more computational resources would be required.\n",
        "To counter that, we treat zero-shot inference of the model as a\n",
        "memory buffer that stores knowledge from the original model. When\n",
        "we fine-tune a SuperAnimal model, we replace the model predicted\n",
        "keypoints with the ground-truth annotations, resulting in hybrid\n",
        "learning of old and new knowledge. The quality of the zero-shot predictions\n",
        "can vary and we use the confidence of prediction (0.7) as a\n",
        "threshold to filter out low-confidence predictions. With the threshold\n",
        "set to 1, memory replay fine-tuning becomes naive-fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSLmjlCIxPoS"
      },
      "source": [
        "#### Prepare training shuffle and weight initialization for memory-replay finetuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKEF76AI0Z4c",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=True,\n",
        "    memory_replay=True,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_memory_replay_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKwJiIyKxPoT"
      },
      "source": [
        "#### Launch the training for memory-replay fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru8tIFmD2Mkv",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_memory_replay_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-2MBRDjxPoT"
      },
      "source": [
        "#### Evaluate the model obtained by memory-replay finetuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfMcK3gq8WxZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_memory_replay_shuffle])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UeXjmtu40Z4X",
        "FvFzntDMxPoL",
        "6VEjHu-00Z4Y"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "989416e0a07342ea9f7eaf3447547996": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76f1609d3cb04a46b67b6f5b5ccb2d2d",
              "IPY_MODEL_c2918207dc8246e4b16705c143014e5b",
              "IPY_MODEL_ba7e0159cdf04133b72c79ee19192b02"
            ],
            "layout": "IPY_MODEL_95a6e10a4a8b4dd3b67ee9dc59a7e7d9"
          }
        },
        "76f1609d3cb04a46b67b6f5b5ccb2d2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0cda3eaea9a040b887d4022e9f433605",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_547c3c81aa6b4caabfb8ee8f9f8d0f26",
            "value": "(‚Ä¶)_quadruped_fasterrcnn_resnet50_fpn_v2.pt:‚Äá100%"
          }
        },
        "c2918207dc8246e4b16705c143014e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56ff4cc30dce4e568cf05f5f51673167",
            "max": 173406754,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_771bf74fb7594a2ab45246a9b4c12584",
            "value": 173406754
          }
        },
        "ba7e0159cdf04133b72c79ee19192b02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8394528aef94e10b917163e50e0cbd4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e725fa852e2048c2bbd3521f7d8f9907",
            "value": "‚Äá173M/173M‚Äá[00:02&lt;00:00,‚Äá83.9MB/s]"
          }
        },
        "95a6e10a4a8b4dd3b67ee9dc59a7e7d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cda3eaea9a040b887d4022e9f433605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "547c3c81aa6b4caabfb8ee8f9f8d0f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56ff4cc30dce4e568cf05f5f51673167": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "771bf74fb7594a2ab45246a9b4c12584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8394528aef94e10b917163e50e0cbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e725fa852e2048c2bbd3521f7d8f9907": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}