{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true,
   "gpuType": "T4"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "<a href=\"https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/main/examples/COLAB/COLAB_HumanPose_with_RTMPose.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# DeepLabCut 3.0+ - Human pose estimation in DeepLabCut 3.0 with RTMPose"
   ],
   "metadata": {
    "id": "t3P1R5BTwud1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Some useful links:\n",
    "\n",
    "- DeepLabCut's GitHub: [github.com/DeepLabCut/DeepLabCut](https://github.com/DeepLabCut/DeepLabCut/tree/main)\n",
    "- DeepLabCut's Documentation: [deeplabcut.github.io/DeepLabCut](https://deeplabcut.github.io/DeepLabCut/README.html)\n",
    "\n",
    "This notebook illustrates how to use the cloud to run pose estimation on humans using a pre-trained [RTMPose](https://arxiv.org/abs/2303.07399) model. **⚠️Note: It uses DeepLabCut's low-level interface, so may be suited for more experienced users.⚠️**\n",
    "\n",
    "RTMPose is a top-down pose estimation model, which means that bounding boxes must be obtained for individuals (which is usually done through an [object detection model](https://en.wikipedia.org/wiki/Object_detection)) before running pose estimation. We obtain bounding boxes using a pre-trained object detector provided by [`torchvision`](https://pytorch.org/vision/main/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
    "\n",
    "## Selecting the Runtime and Installing DeepLabCut\n",
    "\n",
    "**First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\".**\n",
    "\n",
    "Next, we need to install DeepLabCut and its dependencies."
   ],
   "metadata": {
    "id": "tJm8QpTzyAEe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# this will take a couple of minutes to install all the dependencies!\n",
    "!pip install --pre deeplabcut"
   ],
   "metadata": {
    "id": "Aj7Fgm0Xx_fS"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !) You will see this button at the output of the cells above ^.**"
   ],
   "metadata": {
    "id": "twiCWHbgzbwH"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importing Packages and Downloading Model Snapshots"
   ],
   "metadata": {
    "id": "x6DugzWMzGoj"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we'll need to import `deeplabcut`, `huggingface_hub` and other dependencies needed to run the demo."
   ],
   "metadata": {
    "id": "Y7jKbk_mzPJR"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "import huggingface_hub\n",
    "import matplotlib.collections as collections\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.models.detection as detection\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ],
   "metadata": {
    "id": "gbXwpGKXzF98"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can now download the pre-trained RTMPose model weights with which we'll run pose estimation."
   ],
   "metadata": {
    "id": "6KWKmWRxzX5R"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Folder in COLAB where snapshots will be saved\n",
    "model_files = Path(\"hf_files\").resolve()\n",
    "model_files.mkdir(exist_ok=True)\n",
    "\n",
    "# Download the snapshot and model configuration file\n",
    "#   This is generic code to download any snapshot from HuggingFace\n",
    "#   To download DeepLabCut SuperAnimal or Model Zoo models, check\n",
    "#   out dlclibrary!\n",
    "path_model_config = Path(\n",
    "    huggingface_hub.hf_hub_download(\n",
    "        \"DeepLabCut/HumanBody\",\n",
    "        \"rtmpose-x_simcc-body7_pytorch_config.yaml\",\n",
    "        local_dir=model_files,\n",
    "    )\n",
    ")\n",
    "path_snapshot = Path(\n",
    "    huggingface_hub.hf_hub_download(\n",
    "        \"DeepLabCut/HumanBody\",\n",
    "        \"rtmpose-x_simcc-body7.pt\",\n",
    "        local_dir=model_files,\n",
    "    )\n",
    ")"
   ],
   "metadata": {
    "id": "L_V11iCszw3s"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We'll now also define some parameters that we'll later use to plot predictions:\n",
    "\n",
    "- a colormap for the keypoints to plot\n",
    "- a colormap for the limbs of the skeleton\n",
    "- a skeleton for the model\n"
   ],
   "metadata": {
    "id": "eEqukXXy0coy"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cmap_keypoints = plt.get_cmap(\"rainbow\")\n",
    "cmap_skeleton = plt.get_cmap(\"rainbow\")\n",
    "skeleton = [\n",
    "    [16, 14],\n",
    "    [14, 12],\n",
    "    [17, 15],\n",
    "    [15, 13],\n",
    "    [12, 13],\n",
    "    [6, 12],\n",
    "    [7, 13],\n",
    "    [6, 7],\n",
    "    [6, 8],\n",
    "    [7, 9],\n",
    "    [8, 10],\n",
    "    [9, 11],\n",
    "    [2, 3],\n",
    "    [1, 2],\n",
    "    [1, 3],\n",
    "    [2, 4],\n",
    "    [3, 5],\n",
    "    [4, 6],\n",
    "    [5, 7],\n",
    "]"
   ],
   "metadata": {
    "id": "Tam4rfJK0c_b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Inference on Images"
   ],
   "metadata": {
    "id": "cCxkkd-b0EJq"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First, let's upload some images to run inference on. To do so, you can just run the cell below."
   ],
   "metadata": {
    "id": "dotn_xN-05gh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "for filepath, content in uploaded.items():\n",
    "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
    "\n",
    "image_paths = [Path(filepath).resolve() for filepath in uploaded.keys()]\n",
    "\n",
    "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
    "# manually upload your image via the Files menu to the left and define\n",
    "# `image_paths` yourself with right `click` > `copy path` on the image:\n",
    "#\n",
    "# image_paths = [\n",
    "#   Path(\"/path/to/my/image_000.png\"),\n",
    "#   Path(\"/path/to/my/image_001.png\"),\n",
    "# ]\n"
   ],
   "metadata": {
    "id": "mZtikE1H0D34"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nj-HtOBSwtdk"
   },
   "outputs": [],
   "source": [
    "# Define the device on which the models will run\n",
    "device = \"cuda\"  # e.g. cuda, cpu\n",
    "\n",
    "# The maximum number of detections to keep in an image\n",
    "max_detections = 10\n",
    "\n",
    "#############################################\n",
    "# Run a pretrained detector to get bounding boxes\n",
    "\n",
    "# Load the detector from torchvision\n",
    "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=weights, box_score_thresh=0.6,\n",
    ")\n",
    "detector.eval()\n",
    "detector.to(device)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# The context is a list containing the bounding boxes predicted\n",
    "# for each image; it will be given to the RTMPose model alongside\n",
    "# the images.\n",
    "context = []\n",
    "\n",
    "print(\"Running object detection\")\n",
    "with torch.no_grad():\n",
    "    for image_path in tqdm(image_paths):\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        batch = [preprocess(image).to(device)]\n",
    "        predictions = detector(batch)[0]\n",
    "        bboxes = predictions[\"boxes\"].cpu().numpy()\n",
    "        labels = predictions[\"labels\"].cpu().numpy()\n",
    "\n",
    "        # Obtain the bounding boxes predicted for humans\n",
    "        human_bboxes = [\n",
    "            bbox for bbox, label in zip(bboxes, labels) if label == 1\n",
    "        ]\n",
    "\n",
    "        # Convert bounding boxes to xywh format\n",
    "        bboxes = np.zeros((0, 4))\n",
    "        if len(human_bboxes) > 0:\n",
    "            bboxes = np.stack(human_bboxes)\n",
    "        bboxes[:, 2] -= bboxes[:, 0]\n",
    "        bboxes[:, 3] -= bboxes[:, 1]\n",
    "\n",
    "        # Only keep the best N detections\n",
    "        bboxes = bboxes[:max_detections]\n",
    "\n",
    "        context.append({\"bboxes\": bboxes})\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Run inference on the images\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=16,\n",
    "    max_individuals=max_detections,\n",
    ")\n",
    "\n",
    "print(\"Running pose estimation\")\n",
    "predictions = runner.inference(tqdm(zip(image_paths, context)))\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Create a DataFrame with the predictions, and save them to a CSV file.\n",
    "print(\"Saving the predictions to a CSV file\")\n",
    "df = dlc_torch.build_predictions_dataframe(\n",
    "    scorer=\"rtmpose-body7\",\n",
    "    predictions={\n",
    "        img_path: img_predictions\n",
    "        for img_path, img_predictions in zip(image_paths, predictions)\n",
    "    },\n",
    "    parameters=dlc_torch.PoseDatasetParameters(\n",
    "        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "        individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"image_predictions.csv\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can plot the predictions!"
   ],
   "metadata": {
    "id": "pWtdL4U52OBJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#############################################\n",
    "# Unpack and plot predictions\n",
    "plot_skeleton = True\n",
    "plot_pose_markers = True\n",
    "plot_bounding_boxes = True\n",
    "marker_size = 12\n",
    "\n",
    "for image_path, image_predictions in zip(image_paths, predictions):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    pose = image_predictions[\"bodyparts\"]\n",
    "    bboxes = image_predictions[\"bboxes\"]\n",
    "    num_individuals, num_bodyparts = pose.shape[:2]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(image)\n",
    "    ax.set_xlim(0, image.width)\n",
    "    ax.set_ylim(image.height, 0)\n",
    "    ax.axis(\"off\")\n",
    "    for idv_pose in pose:\n",
    "        if plot_skeleton:\n",
    "            bones = []\n",
    "            for bpt_1, bpt_2 in skeleton:\n",
    "                bones.append([idv_pose[bpt_1 - 1, :2], idv_pose[bpt_2 - 1, :2]])\n",
    "\n",
    "            bone_colors = cmap_skeleton\n",
    "            if not isinstance(cmap_skeleton, str):\n",
    "                bone_colors = cmap_skeleton(np.linspace(0, 1, len(skeleton)))\n",
    "\n",
    "            ax.add_collection(\n",
    "                collections.LineCollection(bones, colors=bone_colors)\n",
    "            )\n",
    "\n",
    "        if plot_pose_markers:\n",
    "            ax.scatter(\n",
    "                idv_pose[:, 0],\n",
    "                idv_pose[:, 1],\n",
    "                c=list(range(num_bodyparts)),\n",
    "                cmap=\"rainbow\",\n",
    "                s=marker_size,\n",
    "            )\n",
    "\n",
    "    if plot_bounding_boxes:\n",
    "        for x, y, w, h in bboxes:\n",
    "            ax.plot(\n",
    "                [x, x + w, x + w, x, x],\n",
    "                [y, y, y + h, y + h, y],\n",
    "                c=\"r\",\n",
    "            )\n",
    "\n",
    "    plt.show()"
   ],
   "metadata": {
    "id": "3slKu6Lr2MUh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Running Inference on a Video\n",
    "\n",
    "Running pose inference on a video is very similar! First, upload a video to Google Drive."
   ],
   "metadata": {
    "id": "wO18A_3m5Spk"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "for filepath, content in uploaded.items():\n",
    "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
    "\n",
    "\n",
    "video_path = [Path(filepath).resolve() for filepath in uploaded.keys()][0]\n",
    "\n",
    "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
    "# manually upload your video via the Files menu to the left and define\n",
    "# `video_path` yourself with right `click` > `copy path` on the video:\n",
    "#\n",
    "# video_path = Path(\"/path/to/my/video.mp4\")\n"
   ],
   "metadata": {
    "id": "d9a7gSe15bCa"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Define the device on which the models will run\n",
    "device = \"cuda\"  # e.g. cuda, cpu\n",
    "\n",
    "# The maximum number of individuals to detect in an image\n",
    "max_detections = 30\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Create a video iterator\n",
    "video = dlc_torch.VideoIterator(video_path)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Run a pretrained detector to get bounding boxes\n",
    "\n",
    "# Load the detector from torchvision\n",
    "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
    "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
    "    weights=weights, box_score_thresh=0.6,\n",
    ")\n",
    "detector.eval()\n",
    "detector.to(device)\n",
    "preprocess = weights.transforms()\n",
    "\n",
    "# The context is a list containing the bounding boxes predicted for each frame\n",
    "# in the video.\n",
    "context = []\n",
    "\n",
    "print(\"Running object detection\")\n",
    "with torch.no_grad():\n",
    "    for frame in tqdm(video):\n",
    "        batch = [preprocess(Image.fromarray(frame)).to(device)]\n",
    "        predictions = detector(batch)[0]\n",
    "        bboxes = predictions[\"boxes\"].cpu().numpy()\n",
    "        labels = predictions[\"labels\"].cpu().numpy()\n",
    "\n",
    "        # Obtain the bounding boxes predicted for humans\n",
    "        human_bboxes = [\n",
    "            bbox for bbox, label in zip(bboxes, labels) if label == 1\n",
    "        ]\n",
    "\n",
    "        # Convert bounding boxes to xywh format\n",
    "        bboxes = np.zeros((0, 4))\n",
    "        if len(human_bboxes) > 0:\n",
    "            bboxes = np.stack(human_bboxes)\n",
    "        bboxes[:, 2] -= bboxes[:, 0]\n",
    "        bboxes[:, 3] -= bboxes[:, 1]\n",
    "\n",
    "        # Only keep the top N bounding boxes\n",
    "        bboxes = bboxes[:max_detections]\n",
    "\n",
    "        context.append({\"bboxes\": bboxes})\n",
    "\n",
    "# Set the context for the video\n",
    "video.set_context(context)\n",
    "\n",
    "\n",
    "#############################################\n",
    "# Run inference on the images (in this case a single image)\n",
    "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
    "runner = dlc_torch.get_pose_inference_runner(\n",
    "    pose_cfg,\n",
    "    snapshot_path=path_snapshot,\n",
    "    batch_size=16,\n",
    "    max_individuals=max_detections,\n",
    ")\n",
    "\n",
    "print(\"Running pose estimation\")\n",
    "predictions = runner.inference(tqdm(video))\n",
    "\n",
    "\n",
    "print(\"Saving the predictions to a CSV file\")\n",
    "df = dlc_torch.build_predictions_dataframe(\n",
    "    scorer=\"rtmpose-body7\",\n",
    "    predictions={\n",
    "        idx: img_predictions\n",
    "        for idx, img_predictions in enumerate(predictions)\n",
    "    },\n",
    "    parameters=dlc_torch.PoseDatasetParameters(\n",
    "        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
    "        individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
    "    )\n",
    ")\n",
    "df.to_csv(\"video_predictions.csv\")\n",
    "\n",
    "print(\"Done!\")"
   ],
   "metadata": {
    "id": "I885B01359qu"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, we can plot the predictions on the video! The labeled video output is saved in the `\"video_predictions.mp4\"` file, and can be downloaded to be viewed."
   ],
   "metadata": {
    "id": "altka3NGB_su"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
    "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
    "\n",
    "video_output_path = \"video_predictions.mp4\"\n",
    "\n",
    "clip = VideoProcessorCV(str(video_path), sname=video_output_path, codec=\"mp4v\")\n",
    "CreateVideo(\n",
    "    clip,\n",
    "    df,\n",
    "    pcutoff=0.4,\n",
    "    dotsize=3,\n",
    "    colormap=\"rainbow\",\n",
    "    bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
    "    trailpoints=0,\n",
    "    cropping=False,\n",
    "    x1=0,\n",
    "    x2=clip.w,\n",
    "    y1=0,\n",
    "    y2=clip.h,\n",
    "    bodyparts2connect=skeleton,\n",
    "    skeleton_color=\"k\",\n",
    "    draw_skeleton=True,\n",
    "    displaycropped=False,\n",
    "    color_by=\"bodypart\",\n",
    ")"
   ],
   "metadata": {
    "id": "xRWxH0gO6oPg"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
