{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MMathisLab/DeepLabCut/blob/main/examples/COLAB/COLAB_YOURDATA_SuperAnimal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5SSZpZUu0Z4S"
      },
      "source": [
        "# DeepLabCut Model Zoo: SuperAnimal models\n",
        "\n",
        "![alt text](https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1616492373700-PGOAC72IOB6AUE47VTJX/ke17ZwdGBToddI8pDm48kB8JrdUaZR-OSkKLqWQPp_YUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYwL8IeDg6_3B-BRuF4nNrNcQkVuAT7tdErd0wQFEGFSnBqyW03PFN2MN6T6ry5cmXqqA9xITfsbVGDrg_goIDasRCalqV8R3606BuxERAtDaQ/modelzoo.png?format=1000w)\n",
        "\n",
        "# ğŸ¦„ SuperAnimal in DeepLabCut PyTorch! ğŸ”¥\n",
        "\n",
        "This notebook demos how to use our SuperAnimal models within DeepLabCut 3.0! Please read more in [Ye et al. Nature Communications 2024](https://www.nature.com/articles/s41467-024-48792-2) about the available SuperAnimal models, and follow along below!\n",
        "\n",
        "### **Let's get going: install the latest version of DeepLabCut into COLAB:**\n",
        "\n",
        "*Also, be sure you are connected to a GPU: go to menu, click Runtime > Change Runtime Type > select \"GPU\"*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "AjET5cJE5UYM",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "f8781104-801d-41f2-a512-d246f0e94413"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/DeepLabCut/DeepLabCut.git@mwm/humanbody\n",
            "  Cloning https://github.com/DeepLabCut/DeepLabCut.git (to revision mwm/humanbody) to /tmp/pip-req-build-3dxo55mj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/DeepLabCut/DeepLabCut.git /tmp/pip-req-build-3dxo55mj\n",
            "  Running command git checkout -b mwm/humanbody --track origin/mwm/humanbody\n",
            "  Switched to a new branch 'mwm/humanbody'\n",
            "  Branch 'mwm/humanbody' set up to track remote branch 'mwm/humanbody' from 'origin'.\n",
            "  Resolved https://github.com/DeepLabCut/DeepLabCut.git to commit 3515ba52a194e0183eb433a4d5c82f86b9f129de\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting albumentations<=1.4.3 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading albumentations-1.4.3-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting dlclibrary>=0.0.7 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading dlclibrary-0.0.9-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.8.1)\n",
            "Collecting filterpy>=1.4.4 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading filterpy-1.4.5.zip (177 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m178.0/178.0 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ruamel.yaml>=0.15.0 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading ruamel.yaml-0.18.14-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting imgaug>=0.4.0 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: imageio-ffmpeg in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.6.0)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.60.0)\n",
            "Collecting matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: networkx>=2.6 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (3.5)\n",
            "Collecting numpy<2.0.0,>=1.18.5 (from deeplabcut==3.0.0rc10)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas!=1.5.0,>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (2.2.2)\n",
            "Requirement already satisfied: scikit-image>=0.17 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.25.2)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (1.6.1)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (1.15.3)\n",
            "Requirement already satisfied: statsmodels>=0.11 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.14.4)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (1.0.16)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (4.67.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (2.0.10)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (6.0.2)\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.11/dist-packages (from deeplabcut==3.0.0rc10) (11.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.3->deeplabcut==3.0.0rc10) (4.14.0)\n",
            "Requirement already satisfied: opencv-python-headless>=4.9.0 in /usr/local/lib/python3.11/dist-packages (from albumentations<=1.4.3->deeplabcut==3.0.0rc10) (4.11.0.86)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.11/dist-packages (from dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (0.33.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->deeplabcut==3.0.0rc10) (1.17.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->deeplabcut==3.0.0rc10) (4.11.0.86)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->deeplabcut==3.0.0rc10) (2.37.0)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.11/dist-packages (from imgaug>=0.4.0->deeplabcut==3.0.0rc10) (2.1.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.7.0,!=3.7.1,<3.9,>=3.3->deeplabcut==3.0.0rc10) (2.9.0.post0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->deeplabcut==3.0.0rc10) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut==3.0.0rc10) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas!=1.5.0,>=1.0.1->deeplabcut==3.0.0rc10) (2025.2)\n",
            "Collecting ruamel.yaml.clib>=0.2.7 (from ruamel.yaml>=0.15.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.17->deeplabcut==3.0.0rc10) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image>=0.17->deeplabcut==3.0.0rc10) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->deeplabcut==3.0.0rc10) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0->deeplabcut==3.0.0rc10) (3.6.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.11->deeplabcut==3.0.0rc10) (1.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (3.18.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->deeplabcut==3.0.0rc10)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->deeplabcut==3.0.0rc10) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->deeplabcut==3.0.0rc10) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm->deeplabcut==3.0.0rc10) (0.5.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (1.1.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->deeplabcut==3.0.0rc10) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub->dlclibrary>=0.0.7->deeplabcut==3.0.0rc10) (2025.6.15)\n",
            "Downloading albumentations-1.4.3-py3-none-any.whl (137 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.0/137.0 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dlclibrary-0.0.9-py3-none-any.whl (17 kB)\n",
            "Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading matplotlib-3.8.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m80.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m64.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml-0.18.14-py3-none-any.whl (118 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m118.6/118.6 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m91.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruamel.yaml.clib-0.2.12-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (739 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m739.1/739.1 kB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: deeplabcut, filterpy\n",
            "  Building wheel for deeplabcut (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deeplabcut: filename=deeplabcut-3.0.0rc10-py3-none-any.whl size=2163104 sha256=30e2070ad048beccba8d60f5839d2ac7d60d5ba3f9d7363638de0c2d940f2ac7\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-jf9b4cg1/wheels/55/43/44/f0abc80d56fb5a0786a31b35abe99f095d2e168b5d8785a388\n",
            "  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110460 sha256=77067cca3a75a939cc3fa04a31aacf15e4996d4e17f6bf7c91825b763dfa122f\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/dc/3c/e12983eac132d00f82a20c6cbe7b42ce6e96190ef8fa2d15e1\n",
            "Successfully built deeplabcut filterpy\n",
            "Installing collected packages: ruamel.yaml.clib, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, ruamel.yaml, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, matplotlib, dlclibrary, imgaug, filterpy, albumentations, deeplabcut\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: matplotlib\n",
            "    Found existing installation: matplotlib 3.10.0\n",
            "    Uninstalling matplotlib-3.10.0:\n",
            "      Successfully uninstalled matplotlib-3.10.0\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 2.0.8\n",
            "    Uninstalling albumentations-2.0.8:\n",
            "      Successfully uninstalled albumentations-2.0.8\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed albumentations-1.4.3 deeplabcut-3.0.0rc10 dlclibrary-0.0.9 filterpy-1.4.5 imgaug-0.4.0 matplotlib-3.8.4 numpy-1.26.4 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ruamel.yaml-0.18.14 ruamel.yaml.clib-0.2.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "matplotlib",
                  "mpl_toolkits",
                  "numpy"
                ]
              },
              "id": "11effcc4cf004216880a496c56af8ae7"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install git+https://github.com/DeepLabCut/DeepLabCut.git@mwm/humanbody\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5h0vq6E50Z4W"
      },
      "source": [
        "**PLEASE, click \"restart runtime\" from the output above before proceeding!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvnlIvQm0Z4X",
        "jupyter": {
          "outputs_hidden": true
        },
        "outputId": "1eb92ceb-1606-461b-e2f1-b6ff7671b6b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading DLC 3.0.0rc10...\n",
            "DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import deeplabcut\n",
        "import deeplabcut.utils.auxiliaryfunctions as auxiliaryfunctions\n",
        "from deeplabcut.pose_estimation_pytorch.apis import (\n",
        "    superanimal_analyze_images,\n",
        ")\n",
        "from deeplabcut.modelzoo import build_weight_init\n",
        "from deeplabcut.modelzoo.utils import (\n",
        "    create_conversion_table,\n",
        "    read_conversion_table_from_csv,\n",
        ")\n",
        "from deeplabcut.modelzoo.video_inference import video_inference_superanimal\n",
        "from deeplabcut.utils.pseudo_label import keypoint_matching"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeXjmtu40Z4X"
      },
      "source": [
        "## Zero-shot Image & Video Inference\n",
        "SuperAnimal models are foundation animal pose models. They can be used for zero-shot predictions without further training on the data.\n",
        "In this section, we show how to use SuperAnimal models to predict pose from images (given an image folder) and output the predicted images (with pose) into another destination folder."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvFzntDMxPoL"
      },
      "source": [
        "### Zero-shot image inference\n",
        "\n",
        "If you have a single Image you want to test, upload it here!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbDsZQfsxPoL"
      },
      "source": [
        "#### Upload the images you want to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c4yfTj7r0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "image_path = os.path.abspath(filepath)\n",
        "image_name = os.path.splitext(image_path)[0]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your video via the Files menu to the left\n",
        "# and define `image_path` yourself with right click > copy path on the image:\n",
        "#\n",
        "# image_path = \"/path/to/my/image.png\"\n",
        "# image_name = os.path.splitext(image_path)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jashzdjb0Z4Y"
      },
      "source": [
        "#### Select a SuperAnimal name and corresponding model architecture\n",
        "\n",
        "Check Our Docs on [SuperAnimals](https://github.com/DeepLabCut/DeepLabCut/blob/main/docs/ModelZoo.md) to learn more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH9LXig90Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\"]\n",
        "detector_name = \"fasterrcnn_resnet50_fpn_v2\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown What is the maximum number of animals you expect to have in an image\n",
        "max_individuals = 3  # @param {type:\"slider\", min:1, max:30, step:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OmJtVmHq0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Note you need to enter max_individuals correctly to get the correct number of predictions in the image.\n",
        "_ = superanimal_analyze_images(\n",
        "    superanimal_name,\n",
        "    model_name,\n",
        "    detector_name,\n",
        "    image_path,\n",
        "    max_individuals,\n",
        "    out_folder=\"/content/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VEjHu-00Z4Y"
      },
      "source": [
        "### Zero-shot Video Inference\n",
        "\n",
        "This can be done with or without video adaptation (faster, but not self-supervised fine-tuned on your data!)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGoAhxZOxPoM"
      },
      "source": [
        "#### Upload a video you want to predict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PK3efA0I0Z4Y",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 92
        },
        "outputId": "48b28793-f1f9-4a14-a10b-39cc794695d8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-13fe0222-b73f-446a-b1f8-13e24ad47e90\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-13fe0222-b73f-446a-b1f8-13e24ad47e90\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving jasper-short.mov to jasper-short.mov\n",
            "User uploaded file 'jasper-short.mov' with length 382692 bytes\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "video_path = os.path.abspath(filepath)\n",
        "video_name = os.path.splitext(video_path)[0]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your video via the Files menu to the left\n",
        "# and define `video_path` yourself with right click > copy path on the video."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoA-RATSICj_"
      },
      "source": [
        "#### Choose the superanimal and the model name\n",
        "- for zero-shot inference, we support several superanimal models (not all are supported yet for fine-tuning below)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "OiRAP9XD0Z4Z",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_quadruped\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\", \"superanimal_superbird\", \"superanimal_humanbody\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\", \"rtmpose_x\"]\n",
        "detector_name = \"fasterrcnn_mobilenet_v3_large_fpn\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]\n",
        "\n",
        "# @markdown ---\n",
        "# @markdown What is the maximum number of animals you expect to have in an image\n",
        "max_individuals = 1  # @param {type:\"slider\", min:1, max:30, step:1}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv3v0QgSJNOg"
      },
      "source": [
        "#### Zero-shot Video Inference without video adaptation\n",
        "\n",
        "The labeled video (and pose predictions for the video) are saved in `\"/content/\"`, with the labeled video name being `{your_video_name}_superanimal_{superanimal_name}_hrnetw32_labeled.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "poqynL0UJTBp",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f3a4cf4-f1a6-4927-faad-6fdf29b943f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running video inference on /content/jasper-short.mov with superanimal_quadruped_hrnet_w32\n",
            "Using pytorch for model hrnet_w32\n",
            "DEBUG: update_config called with superanimal_name: NOT_SET\n",
            "DEBUG: update_config returning with superanimal_name: NOT_SET\n",
            "DEBUG: video_inference_superanimal set superanimal_name: superanimal_quadruped\n",
            "DEBUG: update_config called with superanimal_name: superanimal_quadruped\n",
            "DEBUG: update_config returning with superanimal_name: superanimal_quadruped\n",
            "DEBUG: video_inference_superanimal after update_config superanimal_name: superanimal_quadruped\n",
            "DEBUG: Creating detector for superanimal_name: 'superanimal_quadruped'\n",
            "DEBUG: FasterRCNN.__init__ called with variant=fasterrcnn_mobilenet_v3_large_fpn, pretrained=False\n",
            "DEBUG: FasterRCNN.__init__ completed, model type: <class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n",
            "DEBUG: Created custom detector from DETECTORS registry for superanimal_quadruped\n",
            "DEBUG: Custom detector type: <class 'deeplabcut.pose_estimation_pytorch.models.detectors.fasterRCNN.FasterRCNN'>\n",
            "Processing video /content/jasper-short.mov\n",
            "Starting to analyze /content/jasper-short.mov\n",
            "Video metadata: \n",
            "  Overall # of frames:    5\n",
            "  Duration of video [s]:  0.17\n",
            "  fps:                    30.0\n",
            "  resolution:             w=720, h=1280\n",
            "\n",
            "Running detector with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 13.09it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.982368528842926\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9872713685035706\n",
            "DEBUG: Scores above threshold: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 14.58it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.9950140118598938\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9586158990859985\n",
            "DEBUG: Scores above threshold: 1\n",
            "[PATCH] Detector returned 4 predictions for 5 frames. Padding with empty bboxes.\n",
            "Running pose prediction with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pose:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 12.33it/s]\n",
            "WARNING:root:Only 4 of 5 frames had detections!\n",
            "/usr/local/lib/python3.11/dist-packages/deeplabcut/utils/make_labeled_video.py:146: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
            "  Dataframe.groupby(level=\"individuals\", axis=1).size().values // 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/\n",
            "Saving results in /content/jasper-short_superanimal_quadruped_fasterrcnn_mobilenet_v3_large_fpn_hrnet_w32.h5 and /content/jasper-short_superanimal_quadruped_fasterrcnn_mobilenet_v3_large_fpn_hrnet_w32_full.pickle\n",
            "Duration of video [s]: 0.17, recorded with 30.0 fps!\n",
            "Overall # of frames: 5 with cropped frame dimensions: 720 1280\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 35.92it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video with predictions was saved as /content\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "from pickle import FALSE\n",
        "_ = video_inference_superanimal(\n",
        "    videos=video_path,\n",
        "    superanimal_name=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    pcutoff=0.2,\n",
        "    video_adapt=False,\n",
        "    max_individuals=max_individuals,\n",
        "    dest_folder=\"/content/\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Z5GSti0Z4Z"
      },
      "source": [
        "#### Zero-shot Video Inference with video adaptation (unsupervised)\n",
        "\n",
        "The labeled video (and pose predictions for the video) are saved in `\"/content/\"`, with the labeled video name being `{your_video_name}_superanimal_{superanimal_name}_hrnetw32_labeled_after_adapt.mp4`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "5mhOmtzw0Z4Z",
        "jupyter": {
          "outputs_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4a0b24c-b568-4642-add7-f475c4c684ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running video inference on ['/content/jasper-short.mov'] with superanimal_quadruped_hrnet_w32\n",
            "Using pytorch for model hrnet_w32\n",
            "DEBUG: update_config called with superanimal_name: NOT_SET\n",
            "DEBUG: update_config returning with superanimal_name: NOT_SET\n",
            "DEBUG: video_inference_superanimal set superanimal_name: superanimal_quadruped\n",
            "DEBUG: update_config called with superanimal_name: superanimal_quadruped\n",
            "DEBUG: update_config returning with superanimal_name: superanimal_quadruped\n",
            "DEBUG: video_inference_superanimal after update_config superanimal_name: superanimal_quadruped\n",
            "Using /content/jasper-short.mov for video adaptation training\n",
            "DEBUG: Creating detector for superanimal_name: 'superanimal_quadruped'\n",
            "DEBUG: FasterRCNN.__init__ called with variant=fasterrcnn_mobilenet_v3_large_fpn, pretrained=False\n",
            "DEBUG: FasterRCNN.__init__ completed, model type: <class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n",
            "DEBUG: Created custom detector from DETECTORS registry for superanimal_quadruped\n",
            "DEBUG: Custom detector type: <class 'deeplabcut.pose_estimation_pytorch.models.detectors.fasterRCNN.FasterRCNN'>\n",
            "Processing video /content/jasper-short.mov\n",
            "Starting to analyze /content/jasper-short.mov\n",
            "Video metadata: \n",
            "  Overall # of frames:    5\n",
            "  Duration of video [s]:  0.17\n",
            "  fps:                    30.0\n",
            "  resolution:             w=720, h=1280\n",
            "\n",
            "Running detector with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 13.06it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.982368528842926\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9872713685035706\n",
            "DEBUG: Scores above threshold: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 13.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.9950140118598938\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9586158990859985\n",
            "DEBUG: Scores above threshold: 1\n",
            "[PATCH] Detector returned 4 predictions for 5 frames. Padding with empty bboxes.\n",
            "Running pose prediction with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pose:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00,  9.00it/s]\n",
            "WARNING:root:Only 4 of 5 frames had detections!\n",
            "/usr/local/lib/python3.11/dist-packages/deeplabcut/utils/make_labeled_video.py:146: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
            "  Dataframe.groupby(level=\"individuals\", axis=1).size().values // 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/\n",
            "Saving results in /content/jasper-short_superanimal_quadruped_fasterrcnn_mobilenet_v3_large_fpn_hrnet_w32.h5 and /content/jasper-short_superanimal_quadruped_fasterrcnn_mobilenet_v3_large_fpn_hrnet_w32_full.pickle\n",
            "Duration of video [s]: 0.17, recorded with 30.0 fps!\n",
            "Overall # of frames: 5 with cropped frame dimensions: 720 1280\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 25.32it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video with predictions was saved as /content\n",
            "Video frames being extracted to /content/pseudo_jasper-short/images for video adaptation.\n",
            "Constructing pseudo dataset at /content/pseudo_jasper-short\n",
            "Running video adaptation with following parameters:\n",
            "  (pose training) pose_epochs: 1\n",
            "  (pose) save_epochs: 1\n",
            "  detector_epochs: 1\n",
            "  detector_save_epochs: 1\n",
            "  video adaptation batch size: 8\n",
            "\n",
            "DEBUG: FasterRCNN.__init__ called with variant=fasterrcnn_mobilenet_v3_large_fpn, pretrained=False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/deeplabcut/pose_estimation_pytorch/data/transforms.py:51: UserWarning: Be careful! Do not train pose models with horizontal flips if you have symmetric keypoints!\n",
            "  warnings.warn(\n",
            "Data Transforms:\n",
            "  Training:   Compose([\n",
            "  HorizontalFlip(always_apply=False, p=0.5),\n",
            "  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (-40, 40), 'y': (-40, 40)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n",
            "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
            "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
            "  Validation: Compose([\n",
            "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
            "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
            "Using custom collate function: {'type': 'ResizeFromDataSizeCollate', 'min_scale': 0.4, 'max_scale': 1.0, 'min_short_side': 128, 'max_short_side': 1152, 'multiple_of': 32, 'to_square': False}\n",
            "\n",
            "Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.\n",
            "This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.\n",
            "If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. \n",
            "This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).\n",
            "\n",
            "Using 1 images and 1 for testing\n",
            "\n",
            "Starting object detector training...\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: FasterRCNN.__init__ completed, model type: <class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/1 (lr=1e-05), train loss 0.00000\n",
            "Data Transforms:\n",
            "  Training:   Compose([\n",
            "  Affine(always_apply=False, p=0.5, interpolation=1, mask_interpolation=0, cval=0, mode=0, scale={'x': (1.0, 1.0), 'y': (1.0, 1.0)}, translate_percent=None, translate_px={'x': (0, 0), 'y': (0, 0)}, rotate=(-30, 30), fit_output=False, shear={'x': (0.0, 0.0), 'y': (0.0, 0.0)}, cval_mask=0, keep_ratio=True, rotate_method='largest_box'),\n",
            "  GaussNoise(always_apply=False, p=0.5, var_limit=(0, 162.5625), per_channel=True, mean=0),\n",
            "  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),\n",
            "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
            "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
            "  Validation: Compose([\n",
            "  PadIfNeeded(always_apply=False, p=1.0, min_height=None, min_width=None, pad_height_divisor=32, pad_width_divisor=32, position=PositionType.RANDOM, border_mode=4, value=None, mask_value=None),\n",
            "  Normalize(always_apply=False, p=1.0, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0),\n",
            "], p=1.0, bbox_params={'format': 'coco', 'label_fields': ['bbox_labels'], 'min_area': 0.0, 'min_visibility': 0.0, 'min_width': 0.0, 'min_height': 0.0, 'check_each_transform': True}, keypoint_params={'format': 'xy', 'label_fields': ['class_labels'], 'remove_invisible': False, 'angle_in_degrees': True, 'check_each_transform': True}, additional_targets={}, is_check_shapes=True)\n",
            "\n",
            "Note: According to your model configuration, you're training with batch size 1 and/or ``freeze_bn_stats=false``. This is not an optimal setting if you have powerful GPUs.\n",
            "This is good for small batch sizes (e.g., when training on a CPU), where you should keep ``freeze_bn_stats=true``.\n",
            "If you're using a GPU to train, you can obtain faster performance by setting a larger batch size (the biggest power of 2 where you don't geta CUDA out-of-memory error, such as 8, 16, 32 or 64 depending on the model, size of your images, and GPU memory) and ``freeze_bn_stats=false`` for the backbone of your model. \n",
            "This also allows you to increase the learning rate (empirically you can scale the learning rate by sqrt(batch_size) times).\n",
            "\n",
            "Using 1 images and 1 for testing\n",
            "\n",
            "Starting pose model training...\n",
            "--------------------------------------------------\n",
            "Epoch 1/1 (lr=1e-05), train loss 0.00294\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Creating detector for superanimal_name: 'superanimal_quadruped'\n",
            "DEBUG: FasterRCNN.__init__ called with variant=fasterrcnn_mobilenet_v3_large_fpn, pretrained=False\n",
            "DEBUG: FasterRCNN.__init__ completed, model type: <class 'torchvision.models.detection.faster_rcnn.FasterRCNN'>\n",
            "DEBUG: Created custom detector from DETECTORS registry for superanimal_quadruped\n",
            "DEBUG: Custom detector type: <class 'deeplabcut.pose_estimation_pytorch.models.detectors.fasterRCNN.FasterRCNN'>\n",
            "Processing video /content/jasper-short.mov\n",
            "Starting to analyze /content/jasper-short.mov\n",
            "Video metadata: \n",
            "  Overall # of frames:    5\n",
            "  Duration of video [s]:  0.17\n",
            "  fps:                    30.0\n",
            "  resolution:             w=720, h=1280\n",
            "\n",
            "Running detector with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 2/5 [00:00<00:00, 13.04it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.982368528842926\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9872713685035706\n",
            "DEBUG: Scores above threshold: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Detector:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 15.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Max score: 0.9950140118598938\n",
            "DEBUG: Scores above threshold: 1\n",
            "DEBUG: Max score: 0.9586158990859985\n",
            "DEBUG: Scores above threshold: 1\n",
            "[PATCH] Detector returned 4 predictions for 5 frames. Padding with empty bboxes.\n",
            "Running pose prediction with batch size 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Pose:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 4/5 [00:00<00:00, 11.30it/s]\n",
            "Only 4 of 5 frames had detections!\n",
            "/usr/local/lib/python3.11/dist-packages/deeplabcut/utils/make_labeled_video.py:146: FutureWarning: DataFrame.groupby with axis=1 is deprecated. Do `frame.T.groupby(...)` without axis instead.\n",
            "  Dataframe.groupby(level=\"individuals\", axis=1).size().values // 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving results to /content/\n",
            "Saving results in /content/jasper-short_superanimal_quadruped_snapshot-fasterrcnn_mobilenet_v3_large_fpn-001_snapshot-hrnet_w32-001.h5 and /content/jasper-short_superanimal_quadruped_snapshot-fasterrcnn_mobilenet_v3_large_fpn-001_snapshot-hrnet_w32-001_full.pickle\n",
            "Duration of video [s]: 0.17, recorded with 30.0 fps!\n",
            "Overall # of frames: 5 with cropped frame dimensions: 720 1280\n",
            "Generating frames and creating video.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 30.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Video with predictions was saved as /content\n"
          ]
        }
      ],
      "source": [
        "_ = video_inference_superanimal(\n",
        "    videos=[video_path],\n",
        "    superanimal_name=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    video_adapt=True,\n",
        "    max_individuals=max_individuals,\n",
        "    pseudo_threshold=0.1,\n",
        "    bbox_threshold=0.9,\n",
        "    detector_epochs=1,\n",
        "    pose_epochs=1,\n",
        "    dest_folder=\"/content/\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "br3pwGf40Z4a"
      },
      "source": [
        "## Training with SuperAnimal\n",
        "\n",
        "In this section, we compare different ways to train models in DeepLabCut 3.0, with or without using SuperAnimal-pretrained models.\n",
        "You can compare the evaluation results and get a sense of each baseline. We have following baselines:\n",
        "\n",
        "- ImageNet transfer learning (training without superanimal)\n",
        "- SuperAnimal transfer learning (baseline 1)\n",
        "- SuperAnimal naive fine-tuning (baseline 2)\n",
        "- SuperAnimal memory-replay fine-tuning (baseline3)\n",
        "\n",
        "This is done on one of your DeepLabCut projects! If you don't have a DeepLabCut project that you can use SuperAnimal models with, you can always using the example openfield dataset [available in the DeepLabCut repository](https://github.com/DeepLabCut/DeepLabCut/tree/main/examples/openfield-Pranav-2018-10-30) or the Tri-Mouse dataset available on [Zenodo](https://zenodo.org/records/5851157)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPy5VgDDhD6o"
      },
      "source": [
        "### Preparing the DeepLabCut Project\n",
        "\n",
        "First, place your DeepLabCut project folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXzBBV8ehDR9"
      },
      "outputs": [],
      "source": [
        "# Now, let's link to your GoogleDrive. Run this cell and follow the\n",
        "# authorization instructions:\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-QmTftBMo4h6"
      },
      "source": [
        "You will need to edit the project path in the config.yaml file to be set to your Google Drive link!\n",
        "\n",
        "Typically, this will be in the format: `/content/drive/MyDrive/yourProjectFolderName`. You can obtain this path by going to the file navigator in the left pane, finding your DeepLabCut project folder, clicking on the vertical `...` next to the folder name and selecting \"Copy path\".\n",
        "\n",
        "If the `drive` folder is not immediately visible after mounting the drive, refresh the available files!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_iFFEYAB7Uum"
      },
      "outputs": [],
      "source": [
        "# TODO: Update the `project_path` to be the path of your DeepLabCut project!\n",
        "project_path = Path(\"/content/drive/MyDrive/my-project-2024-07-17\")\n",
        "config_path = str(project_path / \"config.yaml\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZTG3Eo475w0"
      },
      "source": [
        "Then, use the panel below to select the appropriate SuperAnimal model for your project (don't forget to run the cell)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8NtCy1Jo0bu"
      },
      "outputs": [],
      "source": [
        "# @markdown ---\n",
        "# @markdown SuperAnimal Configurations\n",
        "superanimal_name = \"superanimal_topviewmouse\" #@param [\"superanimal_topviewmouse\", \"superanimal_quadruped\"]\n",
        "model_name = \"hrnet_w32\" #@param [\"hrnet_w32\", \"resnet_50\"]\n",
        "detector_name = \"fasterrcnn_resnet50_fpn_v2\" #@param [\"fasterrcnn_resnet50_fpn_v2\", \"fasterrcnn_mobilenet_v3_large_fpn\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPvoL9uZ0Z4a"
      },
      "source": [
        "### Comparison between different training baselines\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVmpaLdB0Z4a"
      },
      "source": [
        "Definition of data split: the unique combination of training images and testing images.\n",
        "We create a data split named split 0. All baselines will share the data split to make fair comparisons.\n",
        "- split 0 -> shared by all baselines\n",
        "- shuffle 0 (split0) -> imagenet transfer learning\n",
        "- shuffle 1 (split0) -> superanimal transfer learning\n",
        "- shuffle 2 (split0) -> superanimal naive fine-tuning\n",
        "- shuffle 3 (split0) -> superanimal memory-replay fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WofR2jytxPoR"
      },
      "source": [
        "### What is the difference between baselines?\n",
        "\n",
        "**Transfer learning** For canonical task-agnostic transfer learning,\n",
        "the encoder learns universal visual features from a large pre-training dataset, and a randomly\n",
        "initialized decoder is used to learn the pose from the downstream dataset.\n",
        "\n",
        "**Fine-tuning** For task aware\n",
        "fine-tuning, both encoder and decoder learn task-related visual-pose features\n",
        "in the pre-training datasets, and the decoder is fine-tuned to update pose\n",
        "priors in downstream datasets. Crucially, the network has pose-estimation-specific\n",
        "weights\n",
        "\n",
        "**ImageNet transfer-learning** The encoder was pre-trained from ImageNet. The decoder is trained from scratch in the downstream tasks\n",
        "\n",
        "**SuperAnimal transfer-learning** The encoder was pre-trained first from ImageNet, then in pose datasets we colleceted. Then decoder is trained from scratch in downstream tasks.\n",
        "\n",
        "**SuperAnimal naive fine-tuning** Both the encoder and the decoder were pre-trained in pose datasets we collected. In downstream datasets, we only finetune convolutional channels that correspond to the annotated keypoints in the downstream datasets. This introduces catastrophic forgetting in keypoints that are not annotated in the downstream datasets.\n",
        "\n",
        "**SuperAnimal memory-replay fine-tuning** If we apply fine-tuning with SuperAnimal without further cares, the models will forget about keypoints that are not annotated in the downstream datasets. To mitigate this, we mix the annotations and zero-shot predictions of SuperAnimal models to create a dataset that 'replays' the memory of the SuperAnimal keypoints.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgIsUu6v0Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "imagenet_transfer_learning_shuffle = 0\n",
        "superanimal_transfer_learning_shuffle = 1\n",
        "superanimal_naive_finetune_shuffle = 2\n",
        "superanimal_memory_replay_shuffle = 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuKcxM8F0Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.create_training_dataset(\n",
        "    config_path,\n",
        "    Shuffles=[imagenet_transfer_learning_shuffle],\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6RncQbr0Z4a"
      },
      "source": [
        "### ImageNet transfer learning\n",
        "\n",
        "Historically, the transfer learning using ImageNet weights strategies assumed no â€œanimal pose task priorsâ€ in the pretrained\n",
        "model, a paradigm adopted from previous task-agnostic transfer learning.\n",
        "\n",
        "You can change the number of epochs you want to train for. How long training will take depends on many parameters, including the number of images in your dataset, the resolution of the images, and the number of epochs you train for."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2z8kM340Z4a",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "# Note we skip the detector training to save time.\n",
        "# For Top-Down models, the evaluation is by default using ground-truth bounding\n",
        "#  boxes. But to train a model that can be used to inference videos and images,\n",
        "#  you have to set detector_epochs > 0.\n",
        "\n",
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=imagenet_transfer_learning_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-udMck7nDbG"
      },
      "source": [
        "Now let's evaluate the performance of our trained models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDHMdKz4m_16",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[imagenet_transfer_learning_shuffle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GIFWU-MxPoR"
      },
      "source": [
        "### Transfer learning with SuperAnimal weights\n",
        "\n",
        "First, we prepare training shuffle for transfer-learning with SuperAnimal weights. As we've already create a shuffle with a train/test split that we want to reuse, we use `deeplabcut.create_training_dataset_from_existing_split` to keep the same train/test indices as in the ImageNet transfer learning shuffle.\n",
        "\n",
        "We specify that we want to initialize the model weights with the selected SuperAnimal model, but without keeping the decoding layers (this is called transfer learning)!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOSdZQtOp8qa",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=False,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_transfer_learning_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qFxlRHixPoR"
      },
      "source": [
        "Then, we launch the training for transfer-learning with SuperAnimal weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W60UgRQWqghn",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_transfer_learning_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XzOWKiOixPoR"
      },
      "source": [
        "Finally, we evaluate the model obtained by transfer-learning with SuperAnimal weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpO3aIAIsWbz",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_transfer_learning_shuffle])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Es6RR-_0Z4b"
      },
      "source": [
        "### Fine-tuning with SuperAnimal (without keeping full SuperAnimal keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oo9oJ8XyZrn"
      },
      "source": [
        "#### Setup the weight init and dataset\n",
        "\n",
        "First we do keypoint matching. This steps make it possible to understand the correspondence between the existing annotations and SuperAnimal annotations. This step produces 3 outputs\n",
        "- The confusion matrix\n",
        "- The conversion table\n",
        "- Pseudo predictions over the whole dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRm62Ji_xPoS"
      },
      "source": [
        "#### What is keypoint matching?\n",
        "\n",
        "Because SuperAnimal models have their pre-defined keypoints that are potentially different from your annotations, we proposed this algorithm to minimize the gap between the model and the dataset. We use our model to perform zero-shot inference on the whole dataset. This gives pairs of predictions and ground truth for every image. Then, we cast the matching between modelsâ€™ predictions (2D coordinates)\n",
        "and ground truth as bipartitematching using the Euclidean distance as the cost between paired of keypoints. We then solve the matching using the Hungarian algorithm. Thus for every image, we end up getting a matching matrix where 1 counts formatch and 0 counts for non-matching. Because the modelsâ€™ predictions can be noisy from image to image, we average the aforementioned matching matrix across all the images and perform another bipartite matching, resulting in the final keypoint conversion table between the model and the dataset. Note that the quality of thematching will impact the performance\n",
        "of the model, especially for zero-shot. In the case where, e.g., the annotation nose is mistakenly converted to keypoint tail and vice versa, the model will have to unlearn the channel that corresponds to nose and tail (see also case study in Mathis et al.)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEHeuKSKyjA6",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "keypoint_matching(\n",
        "    config_path,\n",
        "    superanimal_name,\n",
        "    model_name,\n",
        "    detector_name,\n",
        "    copy_images=True,\n",
        ")\n",
        "\n",
        "conversion_table_path = project_path / \"memory_replay\" / \"conversion_table.csv\"\n",
        "confusion_matrix_path = project_path / \"memory_replay\" / \"confusion_matrix.png\"\n",
        "\n",
        "# You can visualize the pseudo predictions, or do pose embedding clustering etc.\n",
        "pseudo_prediction_path = project_path / \"memory_replay\" / \"pseudo_predictions.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sA8yyLgs0zoO"
      },
      "source": [
        "#### Display the confusion matrix\n",
        "\n",
        "The x axis lists the keypoints in the existing annotations. The y axis lists the keypoints in SuperAnimal keypoint space. Darker color encodes stronger correspondence between the human annotation and SuperAnimal annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "luDxpD9H0zYZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "confusion_matrix_image = Image.open(confusion_matrix_path)\n",
        "\n",
        "plt.imshow(confusion_matrix_image)\n",
        "plt.axis('off')  # Hide the axes for better view\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0QWikYmy_Mj"
      },
      "source": [
        "#### Display the conversion table\n",
        "The gt columns represents the keypoint names in the existing dataset. The MasterName represents the corresponding keypoints in SuperAnimal keypoint space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeA-NzDMynYV",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(conversion_table_path)\n",
        "df = df.dropna()\n",
        "\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhWakwQ0JKCQ"
      },
      "source": [
        "#### Adding the Conversion Table to your project's `config.yaml` file\n",
        "\n",
        "Once you've run keypoint matching, you can add the conversion table to your project's `config.yaml` file, and edit it if there are some matches you think are wrong. As an example, for a top-view mouse dataset with 4 bodyparts labeled (`'snout', 'leftear', 'rightear', 'tailbase'`), the conversion table mapping project bodyparts to SuperAnimal bodyparts would be added as:\n",
        "\n",
        "```yaml\n",
        "# Conversion tables to fine-tune SuperAnimal weights\n",
        "SuperAnimalConversionTables:\n",
        "  superanimal_topviewmouse:\n",
        "    snout: nose\n",
        "    leftear: left_ear\n",
        "    rightear: right_ear\n",
        "    tailbase: tail_base\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7nypq5IJKCQ"
      },
      "outputs": [],
      "source": [
        "create_conversion_table(\n",
        "    config=config_path,\n",
        "    super_animal=superanimal_name,\n",
        "    project_to_super_animal=read_conversion_table_from_csv(\n",
        "        conversion_table_path\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkfIo8zTxPoS"
      },
      "source": [
        "#### Prepare the training shuffle and weight initialization for (naive) fine-tuning with SuperAnimal weights\n",
        "\n",
        "Then, when you call `build_weight_init` with `with_decoder=True`, the conversion table in your project's `config.yaml` is used to get predictions for the correct bodyparts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEeM_hrOu6k8",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=True,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_naive_finetune_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZx6nr-ExPoS"
      },
      "source": [
        "#### Launch the training for (naive) fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3XAr6uRyXOD",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_naive_finetune_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXuRshzhxPoS"
      },
      "source": [
        "#### Evaluate the model obtained by (naive) fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VXfdKS-H2yqw",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(\n",
        "    config_path,\n",
        "    Shuffles=[superanimal_naive_finetune_shuffle],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_nUAMlbZ0Z4b"
      },
      "source": [
        "### Memory-replay fine-tuning with SuperAnimal (keeping full SuperAnimal keypoints)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6HPu6RaxPoS"
      },
      "source": [
        "**Catastrophic forgetting** describes a\n",
        "classic problemin continual learning. Indeed, amodel gradually loses\n",
        "its ability to solve previous tasks after it learns to solve new ones.\n",
        "Fine-tuning a SuperAnimal models falls into the category of continual\n",
        "learning: the downstream dataset defines potentially different\n",
        "keypoints than those learned by the models. Thus, the models might\n",
        "forget the keypoints they learned and only pick up those defined in the\n",
        "target dataset. Here, retraining with the original dataset and the new\n",
        "one, is not a feasible option as datasets cannot be easily shared and\n",
        "more computational resources would be required.\n",
        "To counter that, we treat zero-shot inference of the model as a\n",
        "memory buffer that stores knowledge from the original model. When\n",
        "we fine-tune a SuperAnimal model, we replace the model predicted\n",
        "keypoints with the ground-truth annotations, resulting in hybrid\n",
        "learning of old and new knowledge. The quality of the zero-shot predictions\n",
        "can vary and we use the confidence of prediction (0.7) as a\n",
        "threshold to filter out low-confidence predictions. With the threshold\n",
        "set to 1, memory replay fine-tuning becomes naive-fine-tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CSLmjlCIxPoS"
      },
      "source": [
        "#### Prepare training shuffle and weight initialization for memory-replay finetuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKEF76AI0Z4c",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "weight_init = build_weight_init(\n",
        "    cfg=auxiliaryfunctions.read_config(config_path),\n",
        "    super_animal=superanimal_name,\n",
        "    model_name=model_name,\n",
        "    detector_name=detector_name,\n",
        "    with_decoder=True,\n",
        "    memory_replay=True,\n",
        ")\n",
        "\n",
        "deeplabcut.create_training_dataset_from_existing_split(\n",
        "    config_path,\n",
        "    from_shuffle=imagenet_transfer_learning_shuffle,\n",
        "    shuffles=[superanimal_memory_replay_shuffle],\n",
        "    engine=deeplabcut.Engine.PYTORCH,\n",
        "    net_type=f\"top_down_{model_name}\",\n",
        "    detector_type=detector_name,\n",
        "    weight_init=weight_init,\n",
        "    userfeedback=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKwJiIyKxPoT"
      },
      "source": [
        "#### Launch the training for memory-replay fine-tuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ru8tIFmD2Mkv",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.train_network(\n",
        "    config_path,\n",
        "    detector_epochs=0,\n",
        "    epochs=50,\n",
        "    save_epochs=10,\n",
        "    batch_size=64,  # if you get a CUDA OOM error when training on a GPU, reduce to 32, 16, ...!\n",
        "    displayiters=10,\n",
        "    shuffle=superanimal_memory_replay_shuffle,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-2MBRDjxPoT"
      },
      "source": [
        "#### Evaluate the model obtained by memory-replay finetuning with SuperAnimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfMcK3gq8WxZ",
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "outputs": [],
      "source": [
        "deeplabcut.evaluate_network(config_path, Shuffles=[superanimal_memory_replay_shuffle])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "UeXjmtu40Z4X",
        "FvFzntDMxPoL",
        "6VEjHu-00Z4Y"
      ],
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}