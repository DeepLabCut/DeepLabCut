{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DeepLabCut 3.0+ - Human pose estimation in DeepLabCut 3.0 with RTMPose"
      ],
      "metadata": {
        "id": "t3P1R5BTwud1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some useful links:\n",
        "\n",
        "- DeepLabCut's GitHub: [github.com/DeepLabCut/DeepLabCut](https://github.com/DeepLabCut/DeepLabCut/tree/main)\n",
        "- DeepLabCut's Documentation: [deeplabcut.github.io/DeepLabCut](https://deeplabcut.github.io/DeepLabCut/README.html)\n",
        "\n",
        "This notebook illustrates how to use the cloud to run pose estimation on humans using a pre-trained [RTMPose](https://arxiv.org/abs/2303.07399) model. **It uses DeepLabCut's low-level interface, so may be suited for more experienced users.**\n",
        "\n",
        "RTMPose is a top-down pose estimation model, which means that bounding boxes must be obtained for individuals (which is usually done through an [object detection model](https://en.wikipedia.org/wiki/Object_detection)) before running pose estimation. We obtain bounding boxes using a pre-trained object detector provided by [`torchvision`](https://pytorch.org/vision/main/models.html#object-detection-instance-segmentation-and-person-keypoint-detection).\n",
        "\n",
        "## Selecting the Runtime and Installing DeepLabCut\n",
        "\n",
        "**First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\".**\n",
        "\n",
        "Next, we need to install DeepLabCut and its dependencies."
      ],
      "metadata": {
        "id": "tJm8QpTzyAEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this will take a couple of minutes to install all the dependencies!\n",
        "!pip install --pre deeplabcut"
      ],
      "metadata": {
        "id": "Aj7Fgm0Xx_fS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !) You will see this button at the output of the cells above ^.**"
      ],
      "metadata": {
        "id": "twiCWHbgzbwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing Packages and Downloading Model Snapshots"
      ],
      "metadata": {
        "id": "x6DugzWMzGoj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll need to import `deeplabcut`, `huggingface_hub` and other dependencies needed to run the demo."
      ],
      "metadata": {
        "id": "Y7jKbk_mzPJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
        "import huggingface_hub\n",
        "import matplotlib.collections as collections\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.models.detection as detection\n",
        "from PIL import Image\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "gbXwpGKXzF98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now download the pre-trained RTMPose model weights with which we'll run pose estimation."
      ],
      "metadata": {
        "id": "6KWKmWRxzX5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Folder in COLAB where snapshots will be saved\n",
        "model_files = Path(\"hf_files\").resolve()\n",
        "model_files.mkdir(exist_ok=True)\n",
        "\n",
        "# Download the snapshot and model configuration file\n",
        "#   This is generic code to download any snapshot from HuggingFace\n",
        "#   To download DeepLabCut SuperAnimal or Model Zoo models, check\n",
        "#   out dlclibrary!\n",
        "path_model_config = Path(\n",
        "    huggingface_hub.hf_hub_download(\n",
        "        \"DeepLabCut/HumanBody\",\n",
        "        \"rtmpose-x_simcc-body7_pytorch_config.yaml\",\n",
        "        local_dir=model_files,\n",
        "    )\n",
        ")\n",
        "path_snapshot = Path(\n",
        "    huggingface_hub.hf_hub_download(\n",
        "        \"DeepLabCut/HumanBody\",\n",
        "        \"rtmpose-x_simcc-body7.pt\",\n",
        "        local_dir=model_files,\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "L_V11iCszw3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now also define some parameters that we'll later use to plot predictions:\n",
        "\n",
        "- a colormap for the keypoints to plot\n",
        "- a colormap for the limbs of the skeleton\n",
        "- a skeleton for the model\n"
      ],
      "metadata": {
        "id": "eEqukXXy0coy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cmap_keypoints = plt.get_cmap(\"rainbow\")\n",
        "cmap_skeleton = plt.get_cmap(\"rainbow\")\n",
        "skeleton = [\n",
        "    [16, 14],\n",
        "    [14, 12],\n",
        "    [17, 15],\n",
        "    [15, 13],\n",
        "    [12, 13],\n",
        "    [6, 12],\n",
        "    [7, 13],\n",
        "    [6, 7],\n",
        "    [6, 8],\n",
        "    [7, 9],\n",
        "    [8, 10],\n",
        "    [9, 11],\n",
        "    [2, 3],\n",
        "    [1, 2],\n",
        "    [1, 3],\n",
        "    [2, 4],\n",
        "    [3, 5],\n",
        "    [4, 6],\n",
        "    [5, 7],\n",
        "]"
      ],
      "metadata": {
        "id": "Tam4rfJK0c_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Inference on Images"
      ],
      "metadata": {
        "id": "cCxkkd-b0EJq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, let's upload some images to run inference on. To do so, you can just run the cell below."
      ],
      "metadata": {
        "id": "dotn_xN-05gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "\n",
        "image_paths = [Path(filepath).resolve() for filepath in uploaded.keys()]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your image via the Files menu to the left and define\n",
        "# `image_paths` yourself with right `click` > `copy path` on the image:\n",
        "#\n",
        "# image_paths = [\n",
        "#   Path(\"/path/to/my/image_000.png\"),\n",
        "#   Path(\"/path/to/my/image_001.png\"),\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "mZtikE1H0D34"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nj-HtOBSwtdk"
      },
      "outputs": [],
      "source": [
        "# Define the device on which the models will run\n",
        "device = \"cuda\"  # e.g. cuda, cpu\n",
        "\n",
        "# The maximum number of detections to keep in an image\n",
        "max_detections = 10\n",
        "\n",
        "#############################################\n",
        "# Run a pretrained detector to get bounding boxes\n",
        "\n",
        "# Load the detector from torchvision\n",
        "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
        "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
        "    weights=weights, box_score_thresh=0.6,\n",
        ")\n",
        "detector.eval()\n",
        "detector.to(device)\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# The context is a list containing the bounding boxes predicted\n",
        "# for each image; it will be given to the RTMPose model alongside\n",
        "# the images.\n",
        "context = []\n",
        "\n",
        "print(\"Running object detection\")\n",
        "with torch.no_grad():\n",
        "    for image_path in tqdm(image_paths):\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        batch = [preprocess(image).to(device)]\n",
        "        predictions = detector(batch)[0]\n",
        "        bboxes = predictions[\"boxes\"].cpu().numpy()\n",
        "        labels = predictions[\"labels\"].cpu().numpy()\n",
        "\n",
        "        # Obtain the bounding boxes predicted for humans\n",
        "        human_bboxes = [\n",
        "            bbox for bbox, label in zip(bboxes, labels) if label == 1\n",
        "        ]\n",
        "\n",
        "        # Convert bounding boxes to xywh format\n",
        "        bboxes = np.zeros((0, 4))\n",
        "        if len(human_bboxes) > 0:\n",
        "            bboxes = np.stack(human_bboxes)\n",
        "        bboxes[:, 2] -= bboxes[:, 0]\n",
        "        bboxes[:, 3] -= bboxes[:, 1]\n",
        "\n",
        "        # Only keep the best N detections\n",
        "        bboxes = bboxes[:max_detections]\n",
        "\n",
        "        context.append({\"bboxes\": bboxes})\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Run inference on the images\n",
        "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
        "runner = dlc_torch.get_pose_inference_runner(\n",
        "    pose_cfg,\n",
        "    snapshot_path=path_snapshot,\n",
        "    batch_size=16,\n",
        "    max_individuals=max_detections,\n",
        ")\n",
        "\n",
        "print(\"Running pose estimation\")\n",
        "predictions = runner.inference(tqdm(zip(image_paths, context)))\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Create a DataFrame with the predictions, and save them to a CSV file.\n",
        "print(\"Saving the predictions to a CSV file\")\n",
        "df = dlc_torch.build_predictions_dataframe(\n",
        "    scorer=\"rtmpose-body7\",\n",
        "    predictions={\n",
        "        img_path: img_predictions\n",
        "        for img_path, img_predictions in zip(image_paths, predictions)\n",
        "    },\n",
        "    parameters=dlc_torch.PoseDatasetParameters(\n",
        "        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
        "        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
        "        individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
        "    )\n",
        ")\n",
        "\n",
        "# Save to CSV\n",
        "df.to_csv(\"image_predictions.csv\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can plot the predictions!"
      ],
      "metadata": {
        "id": "pWtdL4U52OBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############################################\n",
        "# Unpack and plot predictions\n",
        "plot_skeleton = True\n",
        "plot_pose_markers = True\n",
        "plot_bounding_boxes = True\n",
        "marker_size = 12\n",
        "\n",
        "for image_path, image_predictions in zip(image_paths, predictions):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    pose = image_predictions[\"bodyparts\"]\n",
        "    bboxes = image_predictions[\"bboxes\"]\n",
        "    num_individuals, num_bodyparts = pose.shape[:2]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.imshow(image)\n",
        "    ax.set_xlim(0, image.width)\n",
        "    ax.set_ylim(image.height, 0)\n",
        "    ax.axis(\"off\")\n",
        "    for idv_pose in pose:\n",
        "        if plot_skeleton:\n",
        "            bones = []\n",
        "            for bpt_1, bpt_2 in skeleton:\n",
        "                bones.append([idv_pose[bpt_1 - 1, :2], idv_pose[bpt_2 - 1, :2]])\n",
        "\n",
        "            bone_colors = cmap_skeleton\n",
        "            if not isinstance(cmap_skeleton, str):\n",
        "                bone_colors = cmap_skeleton(np.linspace(0, 1, len(skeleton)))\n",
        "\n",
        "            ax.add_collection(\n",
        "                collections.LineCollection(bones, colors=bone_colors)\n",
        "            )\n",
        "\n",
        "        if plot_pose_markers:\n",
        "            ax.scatter(\n",
        "                idv_pose[:, 0],\n",
        "                idv_pose[:, 1],\n",
        "                c=list(range(num_bodyparts)),\n",
        "                cmap=\"rainbow\",\n",
        "                s=marker_size,\n",
        "            )\n",
        "\n",
        "    if plot_bounding_boxes:\n",
        "        for x, y, w, h in bboxes:\n",
        "            ax.plot(\n",
        "                [x, x + w, x + w, x, x],\n",
        "                [y, y, y + h, y + h, y],\n",
        "                c=\"r\",\n",
        "            )\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "3slKu6Lr2MUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running Inference on a Video\n",
        "\n",
        "Running pose inference on a video is very similar! First, upload a video to Google Drive."
      ],
      "metadata": {
        "id": "wO18A_3m5Spk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "for filepath, content in uploaded.items():\n",
        "    print(f\"User uploaded file '{filepath}' with length {len(content)} bytes\")\n",
        "\n",
        "\n",
        "video_path = [Path(filepath).resolve() for filepath in uploaded.keys()][0]\n",
        "\n",
        "# If this cell fails (e.g., when using Safari in place of Google Chrome),\n",
        "# manually upload your video via the Files menu to the left and define\n",
        "# `video_path` yourself with right `click` > `copy path` on the video:\n",
        "#\n",
        "# video_path = Path(\"/path/to/my/video.mp4\")\n"
      ],
      "metadata": {
        "id": "d9a7gSe15bCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the device on which the models will run\n",
        "device = \"cuda\"  # e.g. cuda, cpu\n",
        "\n",
        "# The maximum number of individuals to detect in an image\n",
        "max_detections = 30\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Create a video iterator\n",
        "video = dlc_torch.VideoIterator(video_path)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Run a pretrained detector to get bounding boxes\n",
        "\n",
        "# Load the detector from torchvision\n",
        "weights = detection.FasterRCNN_MobileNet_V3_Large_FPN_Weights.DEFAULT\n",
        "detector = detection.fasterrcnn_mobilenet_v3_large_fpn(\n",
        "    weights=weights, box_score_thresh=0.6,\n",
        ")\n",
        "detector.eval()\n",
        "detector.to(device)\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "# The context is a list containing the bounding boxes predicted for each frame\n",
        "# in the video.\n",
        "context = []\n",
        "\n",
        "print(\"Running object detection\")\n",
        "with torch.no_grad():\n",
        "    for frame in tqdm(video):\n",
        "        batch = [preprocess(Image.fromarray(frame)).to(device)]\n",
        "        predictions = detector(batch)[0]\n",
        "        bboxes = predictions[\"boxes\"].cpu().numpy()\n",
        "        labels = predictions[\"labels\"].cpu().numpy()\n",
        "\n",
        "        # Obtain the bounding boxes predicted for humans\n",
        "        human_bboxes = [\n",
        "            bbox for bbox, label in zip(bboxes, labels) if label == 1\n",
        "        ]\n",
        "\n",
        "        # Convert bounding boxes to xywh format\n",
        "        bboxes = np.zeros((0, 4))\n",
        "        if len(human_bboxes) > 0:\n",
        "            bboxes = np.stack(human_bboxes)\n",
        "        bboxes[:, 2] -= bboxes[:, 0]\n",
        "        bboxes[:, 3] -= bboxes[:, 1]\n",
        "\n",
        "        # Only keep the top N bounding boxes\n",
        "        bboxes = bboxes[:max_detections]\n",
        "\n",
        "        context.append({\"bboxes\": bboxes})\n",
        "\n",
        "# Set the context for the video\n",
        "video.set_context(context)\n",
        "\n",
        "\n",
        "#############################################\n",
        "# Run inference on the images (in this case a single image)\n",
        "pose_cfg = dlc_torch.config.read_config_as_dict(path_model_config)\n",
        "runner = dlc_torch.get_pose_inference_runner(\n",
        "    pose_cfg,\n",
        "    snapshot_path=path_snapshot,\n",
        "    batch_size=16,\n",
        "    max_individuals=max_detections,\n",
        ")\n",
        "\n",
        "print(\"Running pose estimation\")\n",
        "predictions = runner.inference(tqdm(video))\n",
        "\n",
        "\n",
        "print(\"Saving the predictions to a CSV file\")\n",
        "df = dlc_torch.build_predictions_dataframe(\n",
        "    scorer=\"rtmpose-body7\",\n",
        "    predictions={\n",
        "        idx: img_predictions\n",
        "        for idx, img_predictions in enumerate(predictions)\n",
        "    },\n",
        "    parameters=dlc_torch.PoseDatasetParameters(\n",
        "        bodyparts=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
        "        unique_bpts=pose_cfg[\"metadata\"][\"unique_bodyparts\"],\n",
        "        individuals=[f\"idv_{i}\" for i in range(max_detections)]\n",
        "    )\n",
        ")\n",
        "df.to_csv(\"video_predictions.csv\")\n",
        "\n",
        "print(\"Done!\")"
      ],
      "metadata": {
        "id": "I885B01359qu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we can plot the predictions on the video! The labeled video output is saved in the `\"video_predictions.mp4\"` file, and can be downloaded to be viewed."
      ],
      "metadata": {
        "id": "altka3NGB_su"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deeplabcut.utils.make_labeled_video import CreateVideo\n",
        "from deeplabcut.utils.video_processor import VideoProcessorCV\n",
        "\n",
        "video_output_path = \"video_predictions.mp4\"\n",
        "\n",
        "clip = VideoProcessorCV(str(video_path), sname=video_output_path, codec=\"mp4v\")\n",
        "CreateVideo(\n",
        "    clip,\n",
        "    df,\n",
        "    pcutoff=0.4,\n",
        "    dotsize=3,\n",
        "    colormap=\"rainbow\",\n",
        "    bodyparts2plot=pose_cfg[\"metadata\"][\"bodyparts\"],\n",
        "    trailpoints=0,\n",
        "    cropping=False,\n",
        "    x1=0,\n",
        "    x2=clip.w,\n",
        "    y1=0,\n",
        "    y2=clip.h,\n",
        "    bodyparts2connect=skeleton,\n",
        "    skeleton_color=\"k\",\n",
        "    draw_skeleton=True,\n",
        "    displaycropped=False,\n",
        "    color_by=\"bodypart\",\n",
        ")"
      ],
      "metadata": {
        "id": "xRWxH0gO6oPg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}