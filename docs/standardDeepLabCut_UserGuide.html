
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>DeepLabCut User Guide (for single animal projects) &#8212; DeepLabCut</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.8ecb98da25f57f5357bf6f572d296f466b2cfe2517ffebfabe82451661e28f02.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'docs/standardDeepLabCut_UserGuide';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="DeepLabCut for Multi-Animal Projects" href="maDLC_UserGuide.html" />
    <link rel="prev" title="DeepLabCut Docker containers" href="docker.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../README.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="DeepLabCut - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="DeepLabCut - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../README.html">
                    Welcome! üëã
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Getting Started</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="UseOverviewGuide.html">ü•≥ Get started with DeepLabCut: our key recommendations</a></li>


<li class="toctree-l1"><a class="reference internal" href="course.html">DeepLabCut Self-paced Course</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Installation</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="installation.html">How To Install DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/installTips.html">Installation Tips</a></li>
<li class="toctree-l1"><a class="reference internal" href="docker.html">DeepLabCut Docker containers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Main User Guides</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">DeepLabCut User Guide (for single animal projects)</a></li>
<li class="toctree-l1"><a class="reference internal" href="maDLC_UserGuide.html">DeepLabCut for Multi-Animal Projects</a></li>

<li class="toctree-l1"><a class="reference internal" href="Overviewof3D.html">3D DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="HelperFunctions.html">Helper &amp; Advanced Optional Function Documentation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Graphical User Interfaces (GUIs)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="gui/PROJECT_GUI.html">Interactive Project Manager GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="gui/napari_GUI.html">napari labeling GUI</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DLC3 PyTorch Specific Docs</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="pytorch/user_guide.html">DeepLabCut 3.0 - PyTorch User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/pytorch_config.html">The PyTorch Configuration file</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/architectures.html">DeepLabCut 3.0 - PyTorch Model Architectures</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Quick Start Tutorials</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="quick-start/single_animal_quick_guide.html">QUICK GUIDE to single Animal Training:</a></li>
<li class="toctree-l1"><a class="reference internal" href="quick-start/tutorial_maDLC.html">Multi-animal pose estimation with DeepLabCut: A 5-minute tutorial</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Beginner's Guide to DeepLabCut</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="beginner-guides/beginners-guide.html">Using DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="beginner-guides/manage-project.html">Setting up what keypoints to track</a></li>
<li class="toctree-l1"><a class="reference internal" href="beginner-guides/labeling.html">Labeling GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="beginner-guides/Training-Evaluation.html">Neural Network training and evaluation in the GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="beginner-guides/video-analysis.html">Video Analysis with DeepLabCut</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Main Demo Notebooks</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_DEMO_SuperAnimal.html">DeepLabCut SuperAnimal models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_DEMO_mouse_openfield.html">DeepLabCut on Single Mouse Data Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_3miceDemo.html">DeepLabCut MultiMouse Data Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_HumanPose_with_RTMPose.html">DeepLabCut RTMPose human pose estimation demo</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Notebooks For Your Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_YOURDATA_SuperAnimal.html">DeepLabCut Model Zoo: SuperAnimal models</a></li>

<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.html">DeepLabCut for your standard (single animal) projects!</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_YOURDATA_maDLC_TrainNetwork_VideoAnalysis.html">DeepLabCut for your multi-animal projects!</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üöÄ Special Feature Demos</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_transformer_reID.html">Demo: How to use our Pose Transformer for unsupervised identity tracking of animals</a></li>

<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_BUCTD_and_CTD_tracking.html">DeepLabCut - Tutorial for BUCTD models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/JUPYTER/Demo_3D_DeepLabCut.html">3D DeepLabCut Toolbox</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples/COLAB/COLAB_DLC_ModelZoo.html">DeepLabCut Model Zoo user-contributed models</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">üßë‚Äçüç≥ Cookbook (detailed helper guides)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="convert_maDLC.html">How to convert a pre-2.2 project for use with DeepLabCut 2.2 or later</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/OtherData.html">How to use data labeled outside of DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/io.html">Input/output manipulations with DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/nn.html">Model training tips &amp; tricks</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/post.html">Some data processing recipes!</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/BatchProcessing.html">Automate training and video analysis: Batch Processing</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/DLCMethods.html">How to write a DLC Methods Section</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/ClusteringNapari.html">Clustering in the napari-DeepLabCut GUI</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/OpenVINO.html">Intel OpenVINO backend</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/flip_and_rotate.html">Improving network performance on unbalanced data via augmentation ü¶á</a></li>


<li class="toctree-l1"><a class="reference internal" href="recipes/pose_cfg_file_breakdown.html">The <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> Guideline Handbook</a></li>



<li class="toctree-l1"><a class="reference internal" href="recipes/publishing_notebooks_into_the_DLC_main_cookbook.html">Publishing Notebooks into the Main DLC Cookbook</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Hardware Tips</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="recipes/TechHardware.html">Technical (Hardware) Considerations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepLabCut-Live!</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="deeplabcutlive.html">DeepLabCut-Live!</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">ü¶Ñ DeepLabCut Model Zoo</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="ModelZoo.html">The DeepLabCut Model Zoo!</a></li>
<li class="toctree-l1"><a class="reference internal" href="recipes/UsingModelZooPupil.html">Using ModelZoo models on your own datasets</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">DeepLabCut Benchmarking</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="benchmark.html">DeepLabCut benchmark</a></li>
<li class="toctree-l1"><a class="reference internal" href="pytorch/Benchmarking_shuffle_guide.html">DeepLabCut Benchmarking - User Guide</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Mission &amp; Contribute</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="MISSION_AND_VALUES.html">Mission and Values of DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="roadmap.html">A development roadmap for DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="Governance.html">Governance Model of DeepLabCut</a></li>
<li class="toctree-l1"><a class="reference internal" href="../CONTRIBUTING.html">How to Contribute to DeepLabCut</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Citations for DeepLabCut</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="citation.html">How to Cite DeepLabCut</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/DeepLabCut/DeepLabCut" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/DeepLabCut/DeepLabCut/issues/new?title=Issue%20on%20page%20%2Fdocs/standardDeepLabCut_UserGuide.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/docs/standardDeepLabCut_UserGuide.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>DeepLabCut User Guide (for single animal projects)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut-project-manager-gui-recommended-for-beginners">DeepLabCut Project Manager GUI (recommended for beginners)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut-in-the-terminal-command-line-interface">DeepLabCut in the Terminal/Command line interface:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-create-a-new-project">(A) Create a New Project</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.create_project.new.create_new_project"><code class="docutils literal notranslate"><span class="pre">create_new_project()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-configure-the-project">(B) Configure the Project</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-select-frames-to-label">(C) Select Frames to Label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.frame_extraction.extract_frames"><code class="docutils literal notranslate"><span class="pre">extract_frames()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#d-label-frames">(D) Label Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#e-check-annotated-frames">(E) Check Annotated Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.check_labels"><code class="docutils literal notranslate"><span class="pre">check_labels()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f-create-training-dataset">(F) Create Training Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset"><code class="docutils literal notranslate"><span class="pre">create_training_dataset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_model_comparison"><code class="docutils literal notranslate"><span class="pre">create_training_model_comparison()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset_from_existing_split"><code class="docutils literal notranslate"><span class="pre">create_training_dataset_from_existing_split()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-train-the-network">(G) Train The Network</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.train_network"><code class="docutils literal notranslate"><span class="pre">train_network()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#h-evaluate-the-trained-network">(H) Evaluate the Trained Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.evaluate_network"><code class="docutils literal notranslate"><span class="pre">evaluate_network()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#i-analyze-new-videos">(I) Analyze new Videos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.analyze_videos"><code class="docutils literal notranslate"><span class="pre">analyze_videos()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#novel-video-analysis-extra-features">Novel Video Analysis: extra features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-cropping-of-videos">Dynamic-cropping of videos:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#j-filter-pose-data">(J) Filter Pose Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.post_processing.filtering.filterpredictions"><code class="docutils literal notranslate"><span class="pre">filterpredictions()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-plot-trajectories">(K) Plot Trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.utils.plotting.plot_trajectories"><code class="docutils literal notranslate"><span class="pre">plot_trajectories()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l-create-labeled-videos">(L) Create Labeled Videos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.utils.make_labeled_video.create_labeled_video"><code class="docutils literal notranslate"><span class="pre">create_labeled_video()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-skeleton-features">Extract ‚ÄúSkeleton‚Äù Features:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.post_processing.analyze_skeleton.analyzeskeleton"><code class="docutils literal notranslate"><span class="pre">analyzeskeleton()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#m-optional-active-learning-network-refinement-extract-outlier-frames">(M) Optional Active Learning -&gt; Network Refinement: Extract Outlier Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.refine_training_dataset.outlier_frames.extract_outlier_frames"><code class="docutils literal notranslate"><span class="pre">extract_outlier_frames()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-refine-labels-augmentation-of-the-training-dataset">(N) Refine Labels: Augmentation of the Training Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs-for-deeplabcut-refine-labels">API Docs for deeplabcut.refine_labels</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.gui.tabs.label_frames.refine_labels"><code class="docutils literal notranslate"><span class="pre">refine_labels()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs-for-deeplabcut-merge-datasets">API Docs for deeplabcut.merge_datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.refine_training_dataset.outlier_frames.merge_datasets"><code class="docutils literal notranslate"><span class="pre">merge_datasets()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jupyter-notebooks-for-demonstration-of-the-deeplabcut-workflow">Jupyter Notebooks for Demonstration of the DeepLabCut Workflow</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-toolbox">3D Toolbox</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-functions-some-are-yet-to-be-documented">Other functions, some are yet-to-be-documented:</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="deeplabcut-user-guide-for-single-animal-projects">
<span id="single-animal-userguide"></span><h1>DeepLabCut User Guide (for single animal projects)<a class="headerlink" href="#deeplabcut-user-guide-for-single-animal-projects" title="Link to this heading">#</a></h1>
<p>This document covers single/standard DeepLabCut use. If you have a complicated multi-animal scenario (i.e., they look
the same), then please see our <a class="reference internal" href="maDLC_UserGuide.html#multi-animal-userguide"><span class="std std-ref">maDLC user guide</span></a>.</p>
<p>To get started, you can use the GUI, or the terminal. See below.</p>
<section id="deeplabcut-project-manager-gui-recommended-for-beginners">
<h2>DeepLabCut Project Manager GUI (recommended for beginners)<a class="headerlink" href="#deeplabcut-project-manager-gui-recommended-for-beginners" title="Link to this heading">#</a></h2>
<p><strong>GUI:</strong></p>
<p>To begin, navigate to Anaconda Prompt Terminal and right-click to ‚Äúopen as admin ‚Äú(Windows), or simply launch
‚ÄúTerminal‚Äù (unix/MacOS) on your computer. We assume you have DeepLabCut installed (if not, see
<a class="reference internal" href="installation.html#how-to-install"><span class="std std-ref">install docs</span></a>!). Next, launch your conda env (i.e., for example <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">DEEPLABCUT</span></code>). Then,
simply run <code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">deeplabcut</span></code>. The below functions are available to you in an easy-to-use graphical user interface.
While most functionality is available, advanced users might want the additional flexibility that command line interface
offers. Read more below.</p>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>üö® If you use Windows, please always open the terminal with administrator privileges! Right click, and ‚Äúrun as administrator‚Äù.</p>
</div>
<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1572824438905-QY9XQKZ8LAJZG6BLPWOQ/ke17ZwdGBToddI8pDm48kIIa76w436aRzIF_cdFnEbEUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8N_N4V1vUb5AoIIIbLZhVYxCRW4BPu10St3TBAUQYVKcLthF_aOEGVRewCT7qiippiAuU5PSJ9SSYal26FEts0MmqyMIhpMOn8vJAUvOV4MI/guilaunch.jpg?format=1000w" width="60%">
</p>
<p>As a reminder, the core functions are described in our
<a class="reference external" href="https://www.nature.com/articles/s41596-019-0176-0">Nature Protocols paper</a> (published at the time of 2.0.6).
Additional functions and features are continually added to the package. Thus, we recommend you read over the protocol
and then please look at the following documentation and the doctrings. Thanks for using DeepLabCut!</p>
</section>
<section id="deeplabcut-in-the-terminal-command-line-interface">
<h2>DeepLabCut in the Terminal/Command line interface:<a class="headerlink" href="#deeplabcut-in-the-terminal-command-line-interface" title="Link to this heading">#</a></h2>
<p>To begin, navigate to Anaconda Prompt Terminal and right-click to ‚Äúopen as admin ‚Äú(Windows), or simply launch
‚ÄúTerminal‚Äù (unix/MacOS) on your computer. We assume you have DeepLabCut installed (if not, see Install docs!). Next,
launch your conda env (i.e., for example <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">activate</span> <span class="pre">DEEPLABCUT</span></code>) and then type <code class="docutils literal notranslate"><span class="pre">ipython</span></code>. Then type:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">deeplabcut</span>
</pre></div>
</div>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>üö® If you use Windows, please always open the terminal with administrator privileges! Right click, and ‚Äúrun as administrator‚Äù.</p>
</div>
<section id="a-create-a-new-project">
<h3>(A) Create a New Project<a class="headerlink" href="#a-create-a-new-project" title="Link to this heading">#</a></h3>
<p>The function <code class="docutils literal notranslate"><span class="pre">create_new_project</span></code> creates a new project directory, required subdirectories, and a basic project
configuration file. Each project is identified by the name of the project (e.g. Reaching), name of the experimenter
(e.g. YourName), as well as the date at creation.</p>
<p>Thus, this function requires the user to input the name of the project, the name of the experimenter, and the full
path of the videos that are (initially) used to create the training dataset.</p>
<p>Optional arguments specify the working directory, where the project directory will be created, and if the user wants
to copy the videos (to the project directory). If the optional argument <code class="docutils literal notranslate"><span class="pre">working_directory</span></code> is unspecified, the
project directory is created in the current working directory, and if <code class="docutils literal notranslate"><span class="pre">copy_videos</span></code> is unspecified symbolic links
for the videos are created in the videos directory. Each symbolic link creates a reference to a video and thus
eliminates the need to copy the entire video to the video directory (if the videos remain at the original location).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_new_project</span><span class="p">(</span>
    <span class="s2">&quot;Name of the project&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Name of the experimenter&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;Full path of video 1&quot;</span><span class="p">,</span> <span class="s2">&quot;Full path of video2&quot;</span><span class="p">,</span> <span class="s2">&quot;Full path of video3&quot;</span><span class="p">],</span>
    <span class="n">working_directory</span><span class="o">=</span><span class="s2">&quot;Full path of the working directory&quot;</span><span class="p">,</span>
    <span class="n">copy_videos</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">multianimal</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>Important path formatting note</strong></p>
<p>Windows users, you must input paths as: <code class="docutils literal notranslate"><span class="pre">r'C:\Users\computername\Videos\reachingvideo1.avi'</span></code> or
<code class="docutils literal notranslate"> <span class="pre">'C:\\Users\\computername\\Videos\\reachingvideo1.avi'</span></code></p>
<p>TIP: you can also place <code class="docutils literal notranslate"><span class="pre">config_path</span></code> in front of <code class="docutils literal notranslate"><span class="pre">deeplabcut.create_new_project</span></code> to create a variable that holds
the path to the config.yaml file, i.e. <code class="docutils literal notranslate"><span class="pre">config_path=deeplabcut.create_new_project(...)</span></code></p>
<p>This set of arguments will create a project directory with the name
<strong><Name of the project>+<name of the experimenter>+<date of creation of the project></strong> in the <strong>Working directory</strong> and
creates the symbolic links to videos in the <strong>videos</strong> directory. The project directory will have subdirectories:
<strong>dlc-models</strong>, <strong>dlc-models-pytorch</strong>, <strong>labeled-data</strong>, <strong>training-datasets</strong>, and <strong>videos</strong>.  All the outputs
generated during the course of a project will be stored in one of these subdirectories, thus allowing each project to be
curated in separation from other projects. The purpose of the subdirectories is as follows:</p>
<p><strong>dlc-models</strong> and <strong>dlc-models-pytorch</strong> have a similar structure; the first contains files for the TensorFlow engine
while the second contains files for the PyTorch engine. At the top level in these directories, there are directories
referring to different iterations of label refinement (see below): <strong>iteration-0</strong>, <strong>iteration-1</strong>, etc.
The iteration directories store shuffle directories, where each shuffle directory stores model data related to a
particular experiment: trained and tested on a particular training and testing sets, and with a particular model
architecture. Each shuffle directory contains the subdirectories <em>test</em> and <em>train</em>, each of which holds the meta
information with regard to the parameters of the feature detectors in configuration files. The configuration files are
YAML files, a common human-readable data serialization language. These files can be opened and edited with standard text
editors. The subdirectory <em>train</em> will store checkpoints (called snapshots) during training of the model. These
snapshots allow the user to reload the trained model without re-training it, or to pick-up training from a particular
saved checkpoint, in case the training was interrupted.</p>
<p><strong>labeled-data:</strong> This directory will store the frames used to create the training dataset. Frames from different videos
are stored in separate subdirectories. Each frame has a filename related to the temporal index within the corresponding
video, which allows the user to trace every frame back to its origin.</p>
<p><strong>training-datasets:</strong>  This directory will contain the training dataset used to train the network and metadata, which
contains information about how the training dataset was created.</p>
<p><strong>videos:</strong> Directory of video links or videos. When <strong>copy_videos</strong> is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>, this directory contains
symbolic links to the videos. If it is set to <code class="docutils literal notranslate"><span class="pre">True</span></code> then the videos will be copied to this directory. The default is
<code class="docutils literal notranslate"><span class="pre">False</span></code>. Additionally, if the user wants to add new videos to the project at any stage, the function
<strong>add_new_videos</strong> can be used. This will update the list of videos in the project‚Äôs configuration file.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">add_new_videos</span><span class="p">(</span>
    <span class="s2">&quot;Full path of the project configuration file*&quot;</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;full path of video 4&quot;</span><span class="p">,</span> <span class="s2">&quot;full path of video 5&quot;</span><span class="p">],</span>
    <span class="n">copy_videos</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
<p>*Please note, <em>Full path of the project configuration file</em> will be referenced as <code class="docutils literal notranslate"><span class="pre">config_path</span></code> throughout this
protocol.</p>
<p>The project directory also contains the main configuration file called <em>config.yaml</em>. The <em>config.yaml</em> file contains
many important parameters of the project. A complete list of parameters including their description can be found in
Box1.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">create_new_project</span></code> step writes the following parameters to the configuration file: <em>Task</em>, <em>scorer</em>, <em>date</em>,
<em>project_path</em> as well as a list of videos <em>video_sets</em>. The first three parameters should <strong>not</strong> be changed. The
list of videos can be changed by adding new videos or manually removing videos.</p>
<p><img alt="Box 1 - Single Animal Project Configuration File Glossary" src="../_images/box1-single.png" /></p>
</section>
<section id="api-docs">
<h3>API Docs<a class="headerlink" href="#api-docs" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.create_project.new.create_new_project">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.create_project.new.</span></span><span class="sig-name descname"><span class="pre">create_new_project</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">project</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">experimenter</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">working_directory</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_videos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">multianimal</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">individuals</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.create_project.new.create_new_project" title="Link to this definition">#</a></dt>
<dd><p>Create the necessary folders and files for a new project.</p>
<p>Creating a new project involves creating the project directory, sub-directories and
a basic configuration file. The configuration file is loaded with the default
values. Change its parameters to your projects need.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>project</strong><span class="classifier">string</span></dt><dd><p>The name of the project.</p>
</dd>
<dt><strong>experimenter</strong><span class="classifier">string</span></dt><dd><p>The name of the experimenter.</p>
</dd>
<dt><strong>videos</strong><span class="classifier">list[str]</span></dt><dd><p>A list of strings representing the full paths of the videos to include in the
project. If the strings represent a directory instead of a file, all videos of
<code class="docutils literal notranslate"><span class="pre">videotype</span></code> will be imported.</p>
</dd>
<dt><strong>working_directory</strong><span class="classifier">string, optional</span></dt><dd><p>The directory where the project will be created. The default is the
<code class="docutils literal notranslate"><span class="pre">current</span> <span class="pre">working</span> <span class="pre">directory</span></code>.</p>
</dd>
<dt><strong>copy_videos</strong><span class="classifier">bool, optional, Default: False.</span></dt><dd><p>If True, the videos are copied to the <code class="docutils literal notranslate"><span class="pre">videos</span></code> directory. If False, symlinks
of the videos will be created in the <code class="docutils literal notranslate"><span class="pre">project/videos</span></code> directory; in the event
of a failure to create symbolic links, videos will be moved instead.</p>
</dd>
<dt><strong>multianimal: bool, optional. Default: False.</strong></dt><dd><p>For creating a multi-animal project (introduced in DLC 2.2)</p>
</dd>
<dt><strong>individuals: list[str]|None = None,</strong></dt><dd><p>Relevant only if multianimal is True.
list of individuals to be used in the project configuration.
If None - defaults to [‚Äòindividual1‚Äô, ‚Äòindividual2‚Äô, ‚Äòindividual3‚Äô]</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>str</dt><dd><p>Path to the new project configuration file.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Linux/MacOS:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_new_project</span><span class="p">(</span>
<span class="go">        project=&#39;reaching-task&#39;,</span>
<span class="go">        experimenter=&#39;Linus&#39;,</span>
<span class="go">        videos=[</span>
<span class="go">            &#39;/data/videos/mouse1.avi&#39;,</span>
<span class="go">            &#39;/data/videos/mouse2.avi&#39;,</span>
<span class="go">            &#39;/data/videos/mouse3.avi&#39;</span>
<span class="go">        ],</span>
<span class="go">        working_directory=&#39;/analysis/project/&#39;,</span>
<span class="go">    )</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_new_project</span><span class="p">(</span>
<span class="go">        project=&#39;reaching-task&#39;,</span>
<span class="go">        experimenter=&#39;Linus&#39;,</span>
<span class="go">        videos=[&#39;/data/videos&#39;],</span>
<span class="go">        videotype=&#39;.mp4&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Windows:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_new_project</span><span class="p">(</span>
<span class="go">        &#39;reaching-task&#39;,</span>
<span class="go">        &#39;Bill&#39;,</span>
<span class="go">        [r&#39;C:\yourusername\rig-95\Videos\reachingvideo1.avi&#39;],</span>
<span class="go">        copy_videos=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Users must format paths with either:  r‚ÄôC:OR ‚ÄòC:\ &lt;- i.e. a double backslash )</p>
</dd></dl>

</div>
</section>
<section id="b-configure-the-project">
<h3>(B) Configure the Project<a class="headerlink" href="#b-configure-the-project" title="Link to this heading">#</a></h3>
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1588892210304-EW7WD46PYAU43WWZS4QZ/ke17ZwdGBToddI8pDm48kAXtGtTuS2U1SVcl-tYMBOAUqsxRUqqbr1mOJYKfIPR7LoDQ9mXPOjoJoqy81S2I8PaoYXhp6HxIwZIk7-Mi3Tsic-L2IOPH3Dwrhl-Ne3Z2YjE9w60pqfeJxDohDRZk1jXSVCSSfcEA7WmgMAGpjTehHAH51QaxKq4KdVMVBxpG/1nktc1kdgq2.jpg?format=1000w" width="175" title="colormaps" alt="DLC Utils" align="right" vspace = "50">
<p>Next, open the <strong>config.yaml</strong> file, which was created during  <strong>create_new_project</strong>. You can edit this file in any
text editor.  Familiarize yourself with the meaning of the parameters (Box 1). You can edit various parameters, in
particular you <strong>must add the list of <em>bodyparts</em> (or points of interest)</strong> that you want to track. You can also set the
<em>colormap</em> here that is used for all downstream steps (can also be edited at anytime), like labeling GUIs, videos, etc.
Here any <a class="reference external" href="https://matplotlib.org/tutorials/colors/colormaps.html">matplotlib colormaps</a> will do!
Please DO NOT have spaces in the names of bodyparts.</p>
<p><strong>bodyparts:</strong> are the bodyparts of each individual (in the above list).</p>
</section>
<section id="c-select-frames-to-label">
<h3>(C) Select Frames to Label<a class="headerlink" href="#c-select-frames-to-label" title="Link to this heading">#</a></h3>
<p><strong>CRITICAL:</strong> A good training dataset should consist of a sufficient number of frames that capture the breadth of the
behavior. This ideally implies to select the frames from different (behavioral) sessions, different lighting and
different animals, if those vary substantially (to train an invariant, robust feature detector). Thus for creating a
robust network that you can reuse in the laboratory, a good training dataset should reflect the diversity of the
behavior with respect to postures, luminance conditions, background conditions, animal identities,etc. of the data that
will be analyzed. For the simple lab behaviors comprising mouse reaching, open-field behavior and fly behavior, 100‚àí200
frames gave good results <a class="reference external" href="https://www.nature.com/articles/s41593-018-0209-y">Mathis et al, 2018</a>. However, depending on
the required accuracy, the nature of behavior, the video quality (e.g. motion blur, bad lighting) and the context, more
or less frames might be necessary to create a good network. Ultimately, in order to scale up the analysis to large
collections of videos with perhaps unexpected conditions, one can also refine the data set in an adaptive way (see
refinement below).</p>
<p>The function <code class="docutils literal notranslate"><span class="pre">extract_frames</span></code> extracts frames from all the videos in the project configuration file in order to create
a training dataset. The extracted frames from all the videos are stored in a separate subdirectory named after the video
file‚Äôs name under the ‚Äòlabeled-data‚Äô. This function also has various parameters that might be useful based on the user‚Äôs
need.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;automatic/manual&quot;</span><span class="p">,</span>
    <span class="n">algo</span><span class="o">=</span><span class="s2">&quot;uniform/kmeans&quot;</span><span class="p">,</span>
    <span class="n">crop</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">userfeedback</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>CRITICAL POINT:</strong> It is advisable to keep the frame size small, as large frames increase the training and
inference time. The cropping parameters for each video can be provided in the config.yaml file (and see below).
When running the function extract_frames, if the parameter crop=True, then you will be asked to draw a box within the
GUI (and this is written to the config.yaml file).</p>
<p><code class="docutils literal notranslate"><span class="pre">userfeedback</span></code> allows the user to specify which videos they wish to extract frames from. When set to <code class="docutils literal notranslate"><span class="pre">&quot;True&quot;</span></code>, a dialog
will be initiated, where the user is asked for each video if (additional/any) frames from this video should be
extracted. Use this, e.g. if you have already labeled some folders and want to extract data for new videos.</p>
<p>The provided function either selects frames from the videos that are randomly sampled from a uniform distribution
(uniform), by clustering based on visual appearance (k-means), or by manual selection. Random uniform selection of
frames works best for behaviors where the postures vary across the whole video. However, some behaviors might be sparse,
as in the case of reaching where the reach and pull are very fast and the mouse is not moving much between trials. In
such a case, the function that allows selecting frames based on k-means derived quantization would be useful. If the
user chooses to use k-means as a method to cluster the frames, then this function downsamples the video and clusters the
frames using k-means, where each frame is treated as a vector. Frames from different clusters are then selected. This
procedure makes sure that the frames look different. However, on large and long videos, this code is slow due to
computational complexity.</p>
<p><strong>CRITICAL POINT:</strong> It is advisable to extract frames from a period of the video that contains interesting
behaviors, and not extract the frames across the whole video. This can be achieved by using the start and stop
parameters in the config.yaml file. Also, the user can change the number of frames to extract from each video using
the numframes2extract in the config.yaml file.</p>
<p>However, picking frames is highly dependent on the data and the behavior being studied. Therefore, it is hard to
provide all purpose code that extracts frames to create a good training dataset for every behavior and animal. If the
user feels specific frames are lacking, they can extract hand selected frames of interest using the interactive GUI
provided along with the toolbox. This can be launched by using:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="s2">&quot;manual&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>The user can use the <em>Load Video</em> button to load one of the videos in the project configuration file, use the scroll
bar to navigate across the video and <em>Grab a Frame</em> (or a range of frames, as of version 2.0.5) to extract the frame(s).
The user can also look at the extracted frames and e.g. delete frames (from the directory) that are too similar before
reloading the set and then manually annotating them.</p>
<p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5c71bfbc71c10b4a23d20567/1550958540700/cropMANUAL.gif?format=750w" width="70%">
</p>
</section>
<section id="id1">
<h3>API Docs<a class="headerlink" href="#id1" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.generate_training_dataset.frame_extraction.extract_frames">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.generate_training_dataset.frame_extraction.</span></span><span class="sig-name descname"><span class="pre">extract_frames</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mode</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'automatic'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">algo</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">crop</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">userfeedback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_step</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_resizewidth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opencv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">slider_width</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">25</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">config3d</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extracted_cam</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos_list</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.generate_training_dataset.frame_extraction.extract_frames" title="Link to this definition">#</a></dt>
<dd><p>Extracts frames from the project videos.</p>
<p>Frames will be extracted from videos listed in the config.yaml file.</p>
<p>The frames are selected from the videos in a randomly and temporally uniformly
distributed way (<code class="docutils literal notranslate"><span class="pre">uniform</span></code>), by clustering based on visual appearance
(<code class="docutils literal notranslate"><span class="pre">k-means</span></code>), or by manual selection.</p>
<p>After frames have been extracted from all videos from one camera, matched frames
from other cameras can be extracted using <code class="docutils literal notranslate"><span class="pre">mode</span> <span class="pre">=</span> <span class="pre">&quot;match&quot;</span></code>. This is necessary if
you plan to use epipolar lines to improve labeling across multiple camera angles.
It will overwrite previously extracted images from the second camera angle if
necessary.</p>
<p>Please refer to the user guide for more details on methods and parameters
<a class="reference external" href="https://www.nature.com/articles/s41596-019-0176-0">https://www.nature.com/articles/s41596-019-0176-0</a> or the preprint:
<a class="reference external" href="https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf">https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf</a></p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file as a string.</p>
</dd>
<dt><strong>mode</strong><span class="classifier">string. Either <code class="docutils literal notranslate"><span class="pre">&quot;automatic&quot;</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;manual&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;match&quot;</span></code>.</span></dt><dd><p>String containing the mode of extraction. It must be either <code class="docutils literal notranslate"><span class="pre">&quot;automatic&quot;</span></code> or
<code class="docutils literal notranslate"><span class="pre">&quot;manual&quot;</span></code> to extract the initial set of frames. It can also be <code class="docutils literal notranslate"><span class="pre">&quot;match&quot;</span></code>
to match frames between the cameras in preparation for the use of epipolar line
during labeling; namely, extract from camera_1 first, then run this to extract
the matched frames in camera_2.</p>
<p>WARNING: if you use <code class="docutils literal notranslate"><span class="pre">&quot;match&quot;</span></code>, and you previously extracted and labeled
frames from the second camera, this will overwrite your data. This will require
you to delete the <code class="docutils literal notranslate"><span class="pre">collectdata(.h5/.csv)</span></code> files before labeling. Use with
caution!</p>
</dd>
<dt><strong>algo</strong><span class="classifier">string, Either <code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code> or <code class="docutils literal notranslate"><span class="pre">&quot;uniform&quot;</span></code>, Default: <cite>‚Äúkmeans‚Äù</cite>.</span></dt><dd><p>String specifying the algorithm to use for selecting the frames. Currently,
deeplabcut supports either <code class="docutils literal notranslate"><span class="pre">kmeans</span></code> or <code class="docutils literal notranslate"><span class="pre">uniform</span></code> based selection. This flag
is only required for <code class="docutils literal notranslate"><span class="pre">automatic</span></code> mode and the default is <code class="docutils literal notranslate"><span class="pre">kmeans</span></code>. For
<code class="docutils literal notranslate"><span class="pre">&quot;uniform&quot;</span></code>, frames are picked in temporally uniform way, <code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code>
performs clustering on downsampled frames (see user guide for details).</p>
<p>NOTE: Color information is discarded for <code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code>, thus e.g. for
camouflaged octopus clustering one might want to change this.</p>
</dd>
<dt><strong>crop</strong><span class="classifier">bool or str, optional</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, video frames are cropped according to the corresponding
coordinates stored in the project configuration file. Alternatively, if
cropping coordinates are not known yet, crop=``‚ÄùGUI‚Äù`` triggers a user
interface where the cropping area can be manually drawn and saved.</p>
</dd>
<dt><strong>userfeedback: bool, optional</strong></dt><dd><p>If this is set to <code class="docutils literal notranslate"><span class="pre">False</span></code> during <code class="docutils literal notranslate"><span class="pre">&quot;automatic&quot;</span></code> mode then frames for all
videos are extracted. The user can set this to <code class="docutils literal notranslate"><span class="pre">&quot;True&quot;</span></code>, which will result in
a dialog, where the user is asked for each video if (additional/any) frames
from this video should be extracted. Use this, e.g. if you have already labeled
some folders and want to extract data for new videos.</p>
</dd>
<dt><strong>cluster_resizewidth: int, default: 30</strong></dt><dd><p>For <code class="docutils literal notranslate"><span class="pre">&quot;k-means&quot;</span></code> one can change the width to which the images are downsampled
(aspect ratio is fixed).</p>
</dd>
<dt><strong>cluster_step: int, default: 1</strong></dt><dd><p>By default each frame is used for clustering, but for long videos one could
only use every nth frame (set using this parameter). This saves memory before
clustering can start, however, reading the individual frames takes longer due
to the skipping.</p>
</dd>
<dt><strong>cluster_color: bool, default: False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">&quot;False&quot;</span></code> then each downsampled image is treated as a grayscale vector
(discarding color information). If <code class="docutils literal notranslate"><span class="pre">&quot;True&quot;</span></code>, then the color channels are
considered. This increases the computational complexity.</p>
</dd>
<dt><strong>opencv: bool, default: True</strong></dt><dd><p>Uses openCV for loading &amp; extractiong (otherwise moviepy (legacy)).</p>
</dd>
<dt><strong>slider_width: int, default: 25</strong></dt><dd><p>Width of the video frames slider, in percent of window.</p>
</dd>
<dt><strong>config3d: string, optional</strong></dt><dd><p>Path to the project configuration file in the 3D project. This will be used to
match frames extracted from all cameras present in the field ‚Äòcamera_names‚Äô to
the frames extracted from the camera given by the parameter ‚Äòextracted_cam‚Äô.</p>
</dd>
<dt><strong>extracted_cam: int, default: 0</strong></dt><dd><p>The index of the camera that already has extracted frames. This will match
frame numbers to extract for all other cameras. This parameter is necessary if
you wish to use epipolar lines in the labeling toolbox. Only use if
<code class="docutils literal notranslate"><span class="pre">mode='match'</span></code> and <code class="docutils literal notranslate"><span class="pre">config3d</span></code> is provided.</p>
</dd>
<dt><strong>videos_list: list[str], Default: None</strong></dt><dd><p>A list of the string containing full paths to videos to extract frames for. If
this is left as <code class="docutils literal notranslate"><span class="pre">None</span></code> all videos specified in the config file will have
frames extracted. Otherwise one can select a subset by passing those paths.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Use the function <code class="docutils literal notranslate"><span class="pre">add_new_videos</span></code> at any stage of the project to add new videos
to the config file and extract their frames.</p>
<p>The following parameters for automatic extraction are used from the config file</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">numframes2pick</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">start</span></code> and <code class="docutils literal notranslate"><span class="pre">stop</span></code></p></li>
</ul>
<p>While selecting the frames manually, you do not need to specify the <code class="docutils literal notranslate"><span class="pre">crop</span></code>
parameter in the command. Rather, you will get a prompt in the graphic user
interface to choose if you need to crop or not.</p>
<p class="rubric">Examples</p>
<p>To extract frames automatically with ‚Äòkmeans‚Äô and then crop the frames</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        config=&#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        mode=&#39;automatic&#39;,</span>
<span class="go">        algo=&#39;kmeans&#39;,</span>
<span class="go">        crop=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To extract frames automatically with ‚Äòkmeans‚Äô and then defining the cropping area
using a GUI</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        &#39;automatic&#39;,</span>
<span class="go">        &#39;kmeans&#39;,</span>
<span class="go">        &#39;GUI&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To consider the color information when extracting frames automatically with
‚Äòkmeans‚Äô</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        &#39;automatic&#39;,</span>
<span class="go">        &#39;kmeans&#39;,</span>
<span class="go">        cluster_color=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To extract frames automatically with ‚Äòuniform‚Äô and then crop the frames</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        &#39;automatic&#39;,</span>
<span class="go">        &#39;uniform&#39;,</span>
<span class="go">        crop=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To extract frames manually</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;, &#39;manual&#39;</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To extract frames manually, with a 60% wide frames slider</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;, &#39;manual&#39;, slider_width=60,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To extract frames from a second camera that match the frames extracted from the
first</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        mode=&#39;match&#39;,</span>
<span class="go">        extracted_cam=0,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="d-label-frames">
<h3>(D) Label Frames<a class="headerlink" href="#d-label-frames" title="Link to this heading">#</a></h3>
<p>The toolbox provides a function <strong>label_frames</strong> which helps the user to easily label
all the extracted frames using an interactive graphical user interface (GUI). The user
should have already named the bodyparts to label (points of interest) in the
project‚Äôs configuration file by providing a list. The following command invokes the
napari-deeplabcut labelling GUI. Checkout the <a class="reference internal" href="gui/napari_GUI.html#napari-gui"><span class="std std-ref">napari-deeplabcut docs</span></a> for
more information about the labelling workflow.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">label_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<p><a class="reference external" href="https://youtu.be/hsA9IB5r73E">üé• DEMO</a></p>
<p>HOT KEYS IN THE Labeling GUI (also see ‚Äúhelp‚Äù in GUI):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Ctrl</span> <span class="o">+</span> <span class="n">C</span><span class="p">:</span> <span class="n">Copy</span> <span class="n">labels</span> <span class="kn">from</span><span class="w"> </span><span class="nn">previous</span> <span class="n">frame</span><span class="o">.</span>
<span class="n">Keyboard</span> <span class="n">arrows</span><span class="p">:</span> <span class="n">advance</span> <span class="n">frames</span><span class="o">.</span>
<span class="n">Delete</span> <span class="n">key</span><span class="p">:</span> <span class="n">delete</span> <span class="n">label</span><span class="o">.</span>
</pre></div>
</div>
<p><img alt="hot keys" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/192345a5-e411-4d56-b718-ef52f91e195e/Qwerty.png?format=2500w" /></p>
<p><strong>CRITICAL POINT:</strong> It is advisable to <strong>consistently label similar spots</strong> (e.g., on a wrist that is very large, try
to label the same location). In general, invisible or occluded points should not be labeled by the user. They can
simply be skipped by not applying the label anywhere on the frame.</p>
<p>OPTIONAL: In the event of adding more labels to the existing labeled dataset, the user need to append the new
labels to the bodyparts in the config.yaml file. Thereafter, the user can call the function <strong>label_frames</strong>. As of
2.0.5+: then a box will pop up and ask the user if they wish to display all parts, or only add in the new labels.
Saving the labels after all the images are labelled will append the new labels to the existing labeled dataset.</p>
<p>For more information, checkout the <a class="reference internal" href="gui/napari_GUI.html#napari-gui"><span class="std std-ref">napari-deeplabcut docs</span></a> for
more information about the labelling workflow.</p>
</section>
<section id="e-check-annotated-frames">
<h3>(E) Check Annotated Frames<a class="headerlink" href="#e-check-annotated-frames" title="Link to this heading">#</a></h3>
<p>OPTIONAL: Checking if the labels were created and stored correctly is beneficial for training, since labeling
is one of the most critical parts for creating the training dataset. The DeepLabCut toolbox provides a function
‚Äòcheck_labels‚Äô to do so. It is used as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">check_labels</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">visualizeindividuals</span><span class="o">=</span><span class="kc">True</span><span class="o">/</span><span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<p>For each video directory in labeled-data this function creates a subdirectory with <strong>labeled</strong> as a suffix. Those
directories contain the frames plotted with the annotated body parts. The user can double check if the body parts are
labeled correctly. If they are not correct, the user can reload the frames (i.e. <code class="docutils literal notranslate"><span class="pre">deeplabcut.label_frames</span></code>), move them
around, and click save again.</p>
</section>
<section id="id2">
<h3>API Docs<a class="headerlink" href="#id2" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.generate_training_dataset.trainingsetmanipulation.check_labels">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.generate_training_dataset.trainingsetmanipulation.</span></span><span class="sig-name descname"><span class="pre">check_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Labels</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['+',</span> <span class="pre">'.',</span> <span class="pre">'x']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">scale</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dpi</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">draw_skeleton</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">visualizeindividuals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.check_labels" title="Link to this definition">#</a></dt>
<dd><p>Check the labeled frames.</p>
<p>Double check if the labels were at the correct locations and stored in the proper
file format.</p>
<p>This creates a new subdirectory for each video under the ‚Äòlabeled-data‚Äô and all the
frames are plotted with the labels.</p>
<p>Make sure that these labels are fine.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file as a string.</p>
</dd>
<dt><strong>Labels: list, default=‚Äô+‚Äô</strong></dt><dd><p>List of at least 3 matplotlib markers. The first one will be used to indicate
the human ground truth location (Default: +)</p>
</dd>
<dt><strong>scale</strong><span class="classifier">float, default=1</span></dt><dd><p>Change the relative size of the output images.</p>
</dd>
<dt><strong>dpi</strong><span class="classifier">int, optional, default=100</span></dt><dd><p>Output resolution in dpi.</p>
</dd>
<dt><strong>draw_skeleton: bool, default=True</strong></dt><dd><p>Plot skeleton overlaid over body parts.</p>
</dd>
<dt><strong>visualizeindividuals: bool, default: True.</strong></dt><dd><p>For a multianimal project, if True, the different individuals have different
colors (and all bodyparts the same). If False, the colors change over bodyparts
rather than individuals.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">check_labels</span><span class="p">(</span><span class="s1">&#39;/analysis/project/reaching-task/config.yaml&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="f-create-training-dataset">
<span id="create-training-dataset"></span><h3>(F) Create Training Dataset<a class="headerlink" href="#f-create-training-dataset" title="Link to this heading">#</a></h3>
<p><strong>CRITICAL POINT:</strong> Only run this step <strong>where</strong> you are going to train the network. If you label on your laptop but
move your project folder to Google Colab or AWS, lab server, etc, then run the step below on that platform! If you
labeled on a Windows machine but train on Linux, this is fine as of 2.0.4 onwards it will be done automatically (it
saves file sets as both Linux and Windows for you).</p>
<ul class="simple">
<li><p>If you move your project folder, you must only change the <code class="docutils literal notranslate"><span class="pre">project_path</span></code> (which is done automatically) in the main
config.yaml file - that‚Äôs it - no need to change the video paths, etc! Your project is fully portable.</p></li>
<li><p>Be aware you select your neural network backbone at this stage. As of DLC3+ we support PyTorch (and TensorFlow, but
this will be phased out).</p></li>
</ul>
<p><strong>OVERVIEW:</strong> This function combines the labeled datasets from all the videos and splits them to create train and test
datasets. The training data will be used to train the network, while the test data set will be used for evaluating the
network.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_training_dataset</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>OPTIONAL: If the user wishes to benchmark the performance of the DeepLabCut, they can create multiple training
datasets by specifying an integer value to the <code class="docutils literal notranslate"><span class="pre">num_shuffles</span></code>; see the docstring for more details.</p></li>
</ul>
<p>The function creates a new shuffle(s) directory in the <strong>dlc-models-pytorch</strong> directory
(<strong>dlc-models</strong> if using Tensorflow), in the current ‚Äúiteration‚Äù directory.
The <code class="docutils literal notranslate"><span class="pre">train</span></code> and <code class="docutils literal notranslate"><span class="pre">test</span></code> directories each have a configuration file
(<strong>pytorch_config.yaml</strong> in <strong>train</strong> and <strong>pose_cfg.yaml</strong> in <strong>test</strong> for Pytorch models,
<strong>pose_cfg.yaml</strong> in <strong>train</strong> and <strong>test</strong> for Tensorflow models).
Specifically, the user can edit the <strong>pytorch_config.yaml</strong> (or <strong>pose_cfg.yaml</strong>) within the <strong>train</strong> subdirectory
before starting the training. These configuration files contain meta information with regard to the parameters
of the feature detectors. For more information about the <strong>pytorch_config.yaml</strong> file, see <a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref">here</span></a>
(for TensorFlow-based models, see key parameters
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/pose_cfg.yaml">here</a>).</p>
<p><strong>CRITICAL POINT:</strong> At this step, for <strong>create_training_dataset</strong> you select the network you want to use, and any
additional data augmentation (beyond our defaults). You can set <code class="docutils literal notranslate"><span class="pre">net_type</span></code>, <code class="docutils literal notranslate"><span class="pre">detector_type</span></code> (if using a detector)
and <code class="docutils literal notranslate"><span class="pre">augmenter_type</span></code> when you call the function.</p>
<ul class="simple">
<li><p>Networks: ImageNet pre-trained networks OR SuperAnimal pre-trained networks weights will be downloaded, as you
select. You can decide to do transfer-learning (recommended) or ‚Äúfine-tune‚Äù both the backbone and the decoder head. We
suggest seeing our <a class="reference internal" href="pytorch/architectures.html#dlc3-architectures"><span class="std std-ref">dedicated documentation on models</span></a> for more information (
or the <a class="reference internal" href="recipes/nn.html#what-neural-network-should-i-use"><span class="std std-ref">this page on selecting models</span></a> for the TensorFlow engine).</p></li>
</ul>
<div class="admonition hint">
<p class="admonition-title">Hint</p>
<p>üö® If they do not download (you will see this downloading in the terminal), then you may not have permission to do
so - be sure to open your terminal ‚Äúas an admin‚Äù (This is only something we have seen with some Windows users - see
the <strong><a class="reference internal" href="recipes/nn.html#tf-training-tips-and-tricks"><span class="std std-ref">docs for more help!</span></a></strong>).</p>
</div>
<p><strong>DATA AUGMENTATION:</strong> At this stage you can also decide what type of augmentation to
use. Once you‚Äôve called <code class="docutils literal notranslate"><span class="pre">create_training_dataset</span></code>, you can edit the
<a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref"><strong>pytorch_config.yaml</strong></span></a> file that was created (or for the
TensorFlow engine, the <a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/main/deeplabcut/pose_cfg.yaml"><strong>pose_cfg.yaml</strong></a> file).</p>
<ul class="simple">
<li><p>PyTorch Engine: <a class="reference external" href="https://albumentations.ai/docs/">Albumentations</a> is used for data
augmentation. Look at the <a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref"><strong>pytorch_config.yaml</strong></span></a> for more
information about image augmentation options.</p></li>
<li><p>TensorFlow Engine: The default augmentation works well for most tasks (as shown on
<a class="reference external" href="http://www.deeplabcut.org">www.deeplabcut.org</a>), but there are many options, more data augmentation, intermediate
supervision, etc. Here are the available loaders:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">imgaug</span></code>: a lot of augmentation possibilities, efficient code for target map creation &amp; batch sizes &gt;1 supported.
You can set the parameters such as the <code class="docutils literal notranslate"><span class="pre">batch_size</span></code> in the <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> file for the model you are training. This
is the recommended default!</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">crop_scale</span></code>: our standard DLC 2.0 introduced in Nature Protocols variant (scaling, auto-crop augmentation)</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorpack</span></code>: a lot of augmentation possibilities, multi CPU support for fast processing, target maps are created
less efficiently than in imgaug, does not allow batch size&gt;1</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deterministic</span></code>: only useful for testing, freezes numpy seed; otherwise like default.</p></li>
</ul>
</li>
</ul>
<p><strong>MODEL COMPARISON</strong>: You can also test several models by creating the same train/test
split for different networks.
You can easily do this in the Project Manager GUI (by selecting the ‚ÄúUse an existing
data split‚Äù option), which also lets you compare PyTorch and TensorFlow models.</p>
<div class="versionadded">
<p><span class="versionmodified added">Added in version 3.0.0: </span>You can now create new shuffles using the same train/test split as
existing shuffles with <code class="docutils literal notranslate"><span class="pre">create_training_dataset_from_existing_split</span></code>. This allows you to
compare model performance (between different architectures or when using different
training hyper-parameters) as the shuffles were trained on the same data, and evaluated
on the same test data!</p>
<p>Example usage - creating 3 new shuffles (with indices 10, 11 and 12) for a ResNet 50
pose estimation model, using the same data split as was used for shuffle 0:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_training_dataset_from_existing_split</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">from_shuffle</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">shuffles</span><span class="o">=</span><span class="p">[</span><span class="mi">10</span><span class="p">,</span> <span class="mi">11</span><span class="p">,</span> <span class="mi">12</span><span class="p">],</span>
    <span class="n">net_type</span><span class="o">=</span><span class="s2">&quot;resnet_50&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs for deeplabcut.create_training_dataset</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.generate_training_dataset.trainingsetmanipulation.</span></span><span class="sig-name descname"><span class="pre">create_training_dataset</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_shuffles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Shuffles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">windows2linux</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">userfeedback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainIndices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">testIndices</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">net_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augmenter_type</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posecfg_template</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">superanimal_name</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">WeightInitialization</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Engine</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctd_conditions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset" title="Link to this definition">#</a></dt>
<dd><p>Creates a training dataset.</p>
<p>Labels from all the extracted frames are merged into a single .h5 file.
Only the videos included in the config file are used to create this dataset.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file as a string.</p>
</dd>
<dt><strong>num_shuffles</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>Number of shuffles of training dataset to create, i.e. <code class="docutils literal notranslate"><span class="pre">[1,2,3]</span></code> for
<code class="docutils literal notranslate"><span class="pre">num_shuffles=3</span></code>.</p>
</dd>
<dt><strong>Shuffles: list[int], optional</strong></dt><dd><p>Alternatively the user can also give a list of shuffles.</p>
</dd>
<dt><strong>userfeedback: bool, optional, default=True</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, all requested train/test splits are created (no matter if they
already exist). If you want to assure that previous splits etc. are not
overwritten, set this to <code class="docutils literal notranslate"><span class="pre">True</span></code> and you will be asked for each split.</p>
</dd>
<dt><strong>trainIndices: list of lists, optional, default=None</strong></dt><dd><p>List of one or multiple lists containing train indexes.
A list containing two lists of training indexes will produce two splits.</p>
</dd>
<dt><strong>testIndices: list of lists, optional, default=None</strong></dt><dd><p>List of one or multiple lists containing test indexes.</p>
</dd>
<dt><strong>net_type: list, optional, default=None</strong></dt><dd><p>Type of networks. The options available depend on which engine is used.
Currently supported options are:</p>
<blockquote>
<div><dl class="simple">
<dt>TensorFlow</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_50</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_101</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_152</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.75</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.35</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b4</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b6</span></code></p></li>
</ul>
</dd>
</dl>
<p>PyTorch (call <code class="docutils literal notranslate"><span class="pre">deeplabcut.pose_estimation_pytorch.available_models()</span></code> for
a complete list)</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">animaltokenpose_base</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cspnext_m</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cspnext_s</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">cspnext_x</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_coam_w32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_coam_w48</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_cspnext_m</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_cspnext_x</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_rtmpose_x_human</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_hrnet_w32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_hrnet_w48</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_rtmpose_m</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_rtmpose_x</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ctd_prenet_rtmpose_x_human</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dekr_w18</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dekr_w32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dekr_w48</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dlcrnet_stride16_ms5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dlcrnet_stride32_ms5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hrnet_w18</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hrnet_w32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">hrnet_w48</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_101</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_50</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rtmpose_m</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rtmpose_s</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">rtmpose_x</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_cspnext_m</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_cspnext_s</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_cspnext_x</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_hrnet_w18</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_hrnet_w32</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_hrnet_w48</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_resnet_101</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">top_down_resnet_50</span></code></p></li>
</ul>
</div></blockquote>
</div></blockquote>
</dd>
<dt><strong>detector_type: string, optional, default=None</strong></dt><dd><p>Only for the PyTorch engine.
When passing creating shuffles for top-down models, you can specify which
detector you want. If the detector_type is None, the <code class="docutils literal notranslate"><span class="pre">`ssdlite`</span></code> will be used.
The list of all available detectors can be obtained by calling
<code class="docutils literal notranslate"><span class="pre">deeplabcut.pose_estimation_pytorch.available_detectors()</span></code>. Supported options:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ssdlite</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fasterrcnn_mobilenet_v3_large_fpn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fasterrcnn_resnet50_fpn_v2</span></code></p></li>
</ul>
</div></blockquote>
</dd>
<dt><strong>augmenter_type: string, optional, default=None</strong></dt><dd><p>Type of augmenter. The options available depend on which engine is used.
Currently supported options are:</p>
<blockquote>
<div><dl class="simple">
<dt>TensorFlow</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">default</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scalecrop</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imgaug</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorpack</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deterministic</span></code></p></li>
</ul>
</dd>
<dt>PyTorch</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">albumentations</span></code></p></li>
</ul>
</dd>
</dl>
</div></blockquote>
</dd>
<dt><strong>posecfg_template: string, optional, default=None</strong></dt><dd><p>Only for the TensorFlow engine.
Path to a <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> file to use as a template for generating the new
one for the current iteration. Useful if you would like to start with the same
parameters a previous training iteration. None uses the default
<code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code>.</p>
</dd>
<dt><strong>superanimal_name: string, optional, default=‚Äù‚Äù</strong></dt><dd><p>Only for the TensorFlow engine. For the PyTorch engine, use the <code class="docutils literal notranslate"><span class="pre">weight_init</span></code>
parameter.
Specify the superanimal name is transfer learning with superanimal is desired.
This makes sure the pose config template uses superanimal configs as template.</p>
</dd>
<dt><strong>weight_init: WeightInitialisation, optional, default=None</strong></dt><dd><p>PyTorch engine only. Specify how model weights should be initialized. The
default mode uses transfer learning from ImageNet weights.</p>
</dd>
<dt><strong>engine: Engine, optional</strong></dt><dd><p>Whether to create a pose config for a Tensorflow or PyTorch model. Defaults to
the value specified in the project configuration file. If no engine is specified
for the project, defaults to <code class="docutils literal notranslate"><span class="pre">deeplabcut.compat.DEFAULT_ENGINE</span></code>.</p>
</dd>
<dt><strong>ctd_conditions: int | str | Path | tuple[int, str] | tuple[int, int] | None, default = None,</strong></dt><dd><p>If using a conditional-top-down (CTD) net_type, this argument should be
specified. It defines the conditions that will be used with the CTD model.
It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><dl class="simple">
<dt>A shuffle number (ctd_conditions: int), which must correspond to a</dt><dd><p>bottom-up (BU) network type.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>A predictions file path (ctd_conditions: string | Path), which must</dt><dd><p>correspond to a .json or .h5 predictions file.</p>
</dd>
</dl>
</li>
<li><dl class="simple">
<dt>A shuffle number and a particular snapshot</dt><dd><p>(ctd_conditions: tuple[int, str] | tuple[int, int]), which respectively
correspond to a bottom-up (BU) network type and a particular snapshot
name or index.</p>
</dd>
</dl>
</li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl>
<dt>list(tuple) or None</dt><dd><p>If training dataset was successfully created, a list of tuples is returned.
The first two elements in each tuple represent the training fraction and the
shuffle value. The last two elements in each tuple are arrays of integers
representing the training and test indices.</p>
<p>Returns None if training dataset could not be created.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notes</p>
<p>Use the function <code class="docutils literal notranslate"><span class="pre">add_new_videos</span></code> at any stage of the project to add more videos
to the project.</p>
<p class="rubric">Examples</p>
<p>Linux/MacOS:
&gt;&gt;&gt; deeplabcut.create_training_dataset(</p>
<blockquote>
<div><blockquote>
<div><p>‚Äò/analysis/project/reaching-task/config.yaml‚Äô, num_shuffles=1,</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_training_dataset</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;, Shuffles=[2], engine=deeplabcut.Engine.TF,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Windows:
&gt;&gt;&gt; deeplabcut.create_training_dataset(</p>
<blockquote>
<div><blockquote>
<div><p>‚ÄòC:UsersUlflooming-taskconfig.yaml‚Äô, Shuffles=[3,17,5],</p>
</div></blockquote>
<p>)</p>
</div></blockquote>
</dd></dl>

</div>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs for deeplabcut.create_training_model_comparison</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_model_comparison">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.generate_training_dataset.trainingsetmanipulation.</span></span><span class="sig-name descname"><span class="pre">create_training_model_comparison</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainindex</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_shuffles</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">net_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['resnet_50']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augmenter_types</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">['imgaug']</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">userfeedback</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">windows2linux</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_model_comparison" title="Link to this definition">#</a></dt>
<dd><p>Creates a training dataset to compare networks and augmentation types.</p>
<p>The datasets are created such that the shuffles have same training and testing
indices. Therefore, this function is useful for benchmarking the performance of
different network and augmentation types on the same training/testdata.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>trainindex: int, optional, default=0</strong></dt><dd><p>Either (in case uniform = True) indexes which element of TrainingFraction in
the config file should be used (note it is a list!).
Alternatively (uniform = False) indexes which folder is dropped, i.e. the first
if trainindex=0, the second if trainindex=1, etc.</p>
</dd>
<dt><strong>num_shuffles</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>Number of shuffles of training dataset to create,
i.e. [1,2,3] for num_shuffles=3.</p>
</dd>
<dt><strong>net_types: list[str], optional, default=[‚Äúresnet_50‚Äù]</strong></dt><dd><p>Currently supported networks are</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;resnet_50&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;resnet_101&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;resnet_152&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mobilenet_v2_1.0&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mobilenet_v2_0.75&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mobilenet_v2_0.5&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;mobilenet_v2_0.35&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b0&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b1&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b2&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b3&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b4&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b5&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;efficientnet-b6&quot;</span></code></p></li>
</ul>
</dd>
<dt><strong>augmenter_types: list[str], optional, default=[‚Äúimgaug‚Äù]</strong></dt><dd><p>Currently supported augmenters are</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;default&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;imgaug&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;tensorpack&quot;</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">&quot;deterministic&quot;</span></code></p></li>
</ul>
</dd>
<dt><strong>userfeedback: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, then all requested train/test splits are created, no matter if
they already exist. If you want to assure that previous splits etc. are not
overwritten, then set this to True and you will be asked for each split.</p>
</dd>
<dt><strong>windows2linux</strong></dt><dd><dl class="simple">
<dt>..deprecated::</dt><dd><p>Has no effect since 2.2.0.4 and will be removed in 2.2.1.</p>
</dd>
</dl>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>shuffle_list: list</dt><dd><p>List of indices corresponding to the trainingsplits/models that were created.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>On Linux/MacOS</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shuffle_list</span> <span class="o">=</span> <span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_training_model_comparison</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        num_shuffles=1,</span>
<span class="go">        net_types=[&#39;resnet_50&#39;,&#39;resnet_152&#39;],</span>
<span class="go">        augmenter_types=[&#39;tensorpack&#39;,&#39;deterministic&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>On Windows</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">shuffle_list</span> <span class="o">=</span> <span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_training_model_comparison</span><span class="p">(</span>
<span class="go">        &#39;C:\Users\Ulf\looming-task\config.yaml&#39;,</span>
<span class="go">        num_shuffles=1,</span>
<span class="go">        net_types=[&#39;resnet_50&#39;,&#39;resnet_152&#39;],</span>
<span class="go">        augmenter_types=[&#39;tensorpack&#39;,&#39;deterministic&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>See <code class="docutils literal notranslate"><span class="pre">examples/testscript_openfielddata_augmentationcomparison.py</span></code> for an example
of how to use <code class="docutils literal notranslate"><span class="pre">shuffle_list</span></code>.</p>
</dd></dl>

</div>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs for deeplabcut.create_training_dataset_from_existing_split</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset_from_existing_split">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.generate_training_dataset.trainingsetmanipulation.</span></span><span class="sig-name descname"><span class="pre">create_training_dataset_from_existing_split</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">from_shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">from_trainsetindex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">num_shuffles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">userfeedback</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">net_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">augmenter_type</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ctd_conditions</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">posecfg_template</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">superanimal_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">weight_init</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">WeightInitialization</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Engine</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span> <span class="sig-return"><span class="sig-return-icon">&#x2192;</span> <span class="sig-return-typehint"><span class="pre">None</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span></span><a class="headerlink" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset_from_existing_split" title="Link to this definition">#</a></dt>
<dd><p>Labels from all the extracted frames are merged into a single .h5 file.
Only the videos included in the config file are used to create this dataset.</p>
<dl>
<dt>Args:</dt><dd><p>config: Full path of the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file as a string.</p>
<p>from_shuffle: The index of the shuffle from which to copy the train/test split.</p>
<dl>
<dt>from_trainsetindex: The trainset index of the shuffle from which to use the data</dt><dd><p>split. Default is 0.</p>
</dd>
<dt>num_shuffles: Number of shuffles of training dataset to create, used if</dt><dd><p><code class="docutils literal notranslate"><span class="pre">shuffles</span></code> is None.</p>
</dd>
<dt>shuffles: If defined, <code class="docutils literal notranslate"><span class="pre">num_shuffles</span></code> is ignored and a shuffle is created for</dt><dd><p>each index given in the list.</p>
</dd>
<dt>userfeedback: If <code class="docutils literal notranslate"><span class="pre">False</span></code>, all requested train/test splits are created (no</dt><dd><p>matter if they already exist). If you want to assure that previous splits
etc. are not overwritten, set this to <code class="docutils literal notranslate"><span class="pre">True</span></code> and you will be asked for
each existing split if you want to overwrite it.</p>
</dd>
<dt>net_type: The type of network to create the shuffle for. Currently supported</dt><dd><dl class="simple">
<dt>options for engine=Engine.TF are:</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_50</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_101</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">resnet_152</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_1.0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.75</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">mobilenet_v2_0.35</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b0</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b1</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b2</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b3</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b4</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b5</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">efficientnet-b6</span></code></p></li>
</ul>
</dd>
</dl>
<p>Currently supported  options for engine=Engine.TF can be obtained by calling
<code class="docutils literal notranslate"><span class="pre">deeplabcut.pose_estimation_pytorch.available_models()</span></code>.</p>
</dd>
<dt>detector_type: string, optional, default=None</dt><dd><p>Only for the PyTorch engine.
When passing creating shuffles for top-down models, you can specify which
detector you want. If the detector_type is None, the <code class="docutils literal notranslate"><span class="pre">`ssdlite`</span></code> will be
used. The list of all available detectors can be obtained by calling
<code class="docutils literal notranslate"><span class="pre">deeplabcut.pose_estimation_pytorch.available_detectors()</span></code>. Supported
options:</p>
<blockquote>
<div><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ssdlite</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fasterrcnn_mobilenet_v3_large_fpn</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fasterrcnn_resnet50_fpn_v2</span></code></p></li>
</ul>
</div></blockquote>
</dd>
<dt>augmenter_type: Type of augmenter. Currently supported augmenters for</dt><dd><dl class="simple">
<dt>engine=Engine.TF are</dt><dd><ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">default</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">scalecrop</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">imgaug</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">tensorpack</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">deterministic</span></code></p></li>
</ul>
</dd>
</dl>
<p>The only supported augmenter for Engine.PYTORCH is <code class="docutils literal notranslate"><span class="pre">albumentations</span></code>.</p>
</dd>
<dt>posecfg_template: Only for Engine.TF. Path to a <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> file to use as</dt><dd><p>a template for generating the new one for the current iteration. Useful if
you would like to start with the same parameters a previous training
iteration. None uses the default <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code>.</p>
</dd>
<dt>superanimal_name: Specify the superanimal name is transfer learning with</dt><dd><p>superanimal is desired. This makes sure the pose config template uses
superanimal configs as template.</p>
</dd>
<dt>weight_init: Only for Engine.PYTORCH. Specify how model weights should be</dt><dd><p>initialized. The default mode uses transfer learning from ImageNet weights.</p>
</dd>
<dt>engine: Whether to create a pose config for a Tensorflow or PyTorch model.</dt><dd><p>Defaults to the value specified in the project configuration file. If no
engine is specified for the project, defaults to
<code class="docutils literal notranslate"><span class="pre">deeplabcut.compat.DEFAULT_ENGINE</span></code>.</p>
</dd>
<dt>ctd_conditions: int | str | Path | tuple[int, str] | tuple[int, int] | None, default = None,</dt><dd><p>If using a conditional-top-down (CTD) net_type, this argument should be
specified. It defines the conditions that will be used with the CTD model.
It can be either:</p>
<blockquote>
<div><ul class="simple">
<li><p>A shuffle number (ctd_conditions: int), which must correspond to a
bottom-up (BU) network type.</p></li>
<li><p>A predictions file path (ctd_conditions: string | Path), which must
correspond to a .json or .h5 predictions file.</p></li>
<li><p>A shuffle number and a particular snapshot
(ctd_conditions: tuple[int, str] | tuple[int, int]), which
respectively correspond to a bottom-up (BU) network type and a
particular snapshot name or index.</p></li>
</ul>
</div></blockquote>
</dd>
</dl>
</dd>
<dt>Returns:</dt><dd><p>If training dataset was successfully created, a list of tuples is returned.
The first two elements in each tuple represent the training fraction and the
shuffle value. The last two elements in each tuple are arrays of integers
representing the training and test indices.</p>
<p>Returns None if training dataset could not be created.</p>
</dd>
<dt>Raises:</dt><dd><p>ValueError: If the shuffle from which to copy the data split doesn‚Äôt exist.</p>
</dd>
</dl>
</dd></dl>

</div>
</section>
<section id="g-train-the-network">
<h3>(G) Train The Network<a class="headerlink" href="#g-train-the-network" title="Link to this heading">#</a></h3>
<p>The function ‚Äòtrain_network‚Äô helps the user in training the network. It is used as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<p>The set of arguments in the function starts training the network for the dataset created
for one specific shuffle. Note that you can change training parameters in the
<a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref"><strong>pytorch_config.yaml</strong></span></a> file (or <strong>pose_cfg.yaml</strong> for TensorFlow
models) of the model that you want to train (before you start training).</p>
<p>At user specified iterations during training checkpoints are stored in the subdirectory
<em>train</em> under the respective iteration &amp; shuffle directory.</p>
<div class="dropdown admonition">
<p class="admonition-title">Tips on training models with the PyTorch Engine</p>
<p>Example parameters that one can call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">,</span>
    <span class="n">max_snapshots_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">displayiters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">save_epochs</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Pytorch models in DeepLabCut 3.0 are trained for a set number of epochs, instead of a
maximum number of iterations (which is what was used for TensorFlow models). An epoch
is a single pass through the training dataset, which means your model has seen each
training image exactly once. So if you have 64 training images for your network, an
epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2, 16 with
batch size 4, etc.).</p>
<p>By default, the pretrained networks are not in the DeepLabCut toolbox (as they can be
more than 100MB), but they get downloaded automatically before you train.</p>
<p>If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable <code class="docutils literal notranslate"><span class="pre">resume_training_from</span></code> in the <a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref">
<strong>pytorch_config.yaml</strong></span></a> file (checkout the ‚ÄúRestarting Training at a Specific Checkpoint‚Äù
section of the docs) under the <em>train</em> subdirectory.</p>
<p><strong>CRITICAL POINT:</strong> It is recommended to train the networks <strong>until the loss plateaus</strong>
(depending on the dataset, model architecture and training hyper-parameters this happens
after 100 to 250 epochs of training).</p>
<p>The variables <code class="docutils literal notranslate"><span class="pre">display_iters</span></code> and <code class="docutils literal notranslate"><span class="pre">save_epochs</span></code> in the <a class="reference internal" href="pytorch/pytorch_config.html#dlc3-pytorch-config"><span class="std std-ref"><strong>pytorch_config.yaml</strong></span></a> file allows the user to alter how often the loss is displayed
and how often the weights are stored. We suggest saving every 5 to 25 epochs.</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Tips on training models with the TensorFlow Engine</p>
<p>Example parameters that one can call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">gputouse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">max_snapshots_to_keep</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">autotune</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">displayiters</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">saveiters</span><span class="o">=</span><span class="mi">25000</span><span class="p">,</span>
    <span class="n">maxiters</span><span class="o">=</span><span class="mi">300000</span><span class="p">,</span>
    <span class="n">allow_growth</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>By default, the pretrained networks are not in the DeepLabCut toolbox (as they are
around 100MB each), but they get downloaded before you train. However, if not previously
downloaded from the TensorFlow model weights, it will be downloaded and stored in a
subdirectory <em>pre-trained</em> under the subdirectory <em>models</em> in
<em>Pose_Estimation_Tensorflow</em>. At user specified iterations during training checkpoints
are stored in the subdirectory <em>train</em> under the respective iteration directory.</p>
<p>If the user wishes to restart the training at a specific checkpoint they can specify the
full path of the checkpoint to the variable <code class="docutils literal notranslate"><span class="pre">init_weights</span></code> in the <strong>pose_cfg.yaml</strong>
file under the <em>train</em> subdirectory (see Box 2).</p>
<p><strong>CRITICAL POINT:</strong> It is recommended to train the networks for thousands of iterations
until the loss plateaus (typically around <strong>500,000</strong>) if you use batch size 1. If you
want to batch train, we recommend using Adam,
<a class="reference internal" href="recipes/nn.html#tf-custom-image-augmentation"><span class="std std-ref">see more here</span></a>.</p>
<p>The variables <code class="docutils literal notranslate"><span class="pre">display_iters</span></code> and <code class="docutils literal notranslate"><span class="pre">save_iters</span></code> in the <strong>pose_cfg.yaml</strong> file allows
the user to alter how often the loss is displayed and how often the weights are stored.</p>
<p><strong>maDeepLabCut CRITICAL POINT:</strong> For multi-animal projects we are using not only
different and new output layers, but also new data augmentation, optimization, learning
rates, and batch training defaults. Thus, please use a lower <code class="docutils literal notranslate"><span class="pre">save_iters</span></code> and
<code class="docutils literal notranslate"><span class="pre">maxiters</span></code>. I.e. we suggest saving every 10K-15K iterations, and only training until
50K-100K iterations. We recommend you look closely at the loss to not overfit on your
data. The bonus, training time is much less!!!</p>
</div>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs for train_network</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.compat.train_network">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.compat.</span></span><span class="sig-name descname"><span class="pre">train_network</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">max_snapshots_to_keep</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displayiters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">saveiters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">maxiters</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_growth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gputouse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">autotune</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keepdeconvweights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">superanimal_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">superanimal_transfer_learning</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Engine</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">snapshot_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_batch_size</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">detector_save_epochs</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pose_threshold</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pytorch_cfg_updates</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">dict</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.compat.train_network" title="Link to this definition">#</a></dt>
<dd><p>Trains the network with the labels in the training dataset.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file as a string.</p>
</dd>
<dt><strong>shuffle: int, optional, default=1</strong></dt><dd><p>Integer value specifying the shuffle index to select for training.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>max_snapshots_to_keep: int or None</strong></dt><dd><p>Sets how many snapshots are kept, i.e. states of the trained network. Every
saving iteration many times a snapshot is stored, however only the last
<code class="docutils literal notranslate"><span class="pre">max_snapshots_to_keep</span></code> many are kept! If you change this to None, then all
are kept.
See: <a class="github reference external" href="https://github.com/DeepLabCut/DeepLabCut/issues/8#issuecomment-387404835">DeepLabCut/DeepLabCut#8</a></p>
</dd>
<dt><strong>displayiters: optional, default=None</strong></dt><dd><p>This variable is actually set in <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code>. However, you can
overwrite it with this hack. Don‚Äôt use this regularly, just if you are too lazy
to dig out the <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code> file for the corresponding project. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, the value from there is used, otherwise it is overwritten!</p>
</dd>
<dt><strong>saveiters: optional, default=None</strong></dt><dd><p>Only for the TensorFlow engine (for the PyTorch engine see the <code class="docutils literal notranslate"><span class="pre">torch_kwargs</span></code>:
you can use <code class="docutils literal notranslate"><span class="pre">save_epochs</span></code>).
This variable is actually set in <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code>. However, you can
overwrite it with this hack. Don‚Äôt use this regularly, just if you are too lazy
to dig out the <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code> file for the corresponding project.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the value from there is used, otherwise it is overwritten!</p>
</dd>
<dt><strong>maxiters: optional, default=None</strong></dt><dd><p>Only for the TensorFlow engine (for the PyTorch engine see the <code class="docutils literal notranslate"><span class="pre">torch_kwargs</span></code>:
you can use <code class="docutils literal notranslate"><span class="pre">epochs</span></code>).
This variable is actually set in <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code>. However, you can
overwrite it with this hack. Don‚Äôt use this regularly, just if you are too lazy
to dig out the <code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code> file for the corresponding project.
If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the value from there is used, otherwise it is overwritten!</p>
</dd>
<dt><strong>epochs: optional, default=None</strong></dt><dd><p>Only for the PyTorch engine (equivalent to the <cite>maxiters</cite> parameter for the
TensorFlow engine). The maximum number of epochs to train the model for. If
None, the value will be read from the <cite>pytorch_config.yaml</cite> file. An epoch is a
single pass through the training dataset, which means your model has seen each
training image exactly once. So if you have 64 training images for your network,
an epoch is 64 iterations with batch size 1 (or 32 iterations with batch size 2,
16 with batch size 4, etc.).</p>
</dd>
<dt><strong>save_epochs: optional, default=None</strong></dt><dd><p>Only for the PyTorch engine (equivalent to the <cite>saveiters</cite> parameter for the
TensorFlow engine). The number of epochs between each snapshot save. If
None, the value will be read from the <cite>pytorch_config.yaml</cite> file.</p>
</dd>
<dt><strong>allow_growth: bool, optional, default=True.</strong></dt><dd><p>Only for the TensorFlow engine.
For some smaller GPUs the memory issues happen. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the memory
allocator does not pre-allocate the entire specified GPU memory region, instead
starting small and growing as needed.
See issue: <a class="reference external" href="https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2">https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2</a></p>
</dd>
<dt><strong>gputouse: optional, default=None</strong></dt><dd><p>Only for the TensorFlow engine (for the PyTorch engine see the <code class="docutils literal notranslate"><span class="pre">torch_kwargs</span></code>:
you can use <code class="docutils literal notranslate"><span class="pre">device</span></code>).
Natural number indicating the number of your GPU (see number in nvidia-smi).
If you do not have a GPU put None.
See: <a class="reference external" href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries">https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries</a></p>
</dd>
<dt><strong>autotune: bool, optional, default=False</strong></dt><dd><p>Only for the TensorFlow engine.
Property of TensorFlow, somehow faster if <code class="docutils literal notranslate"><span class="pre">False</span></code>
(as Eldar found out, see <a class="github reference external" href="https://github.com/tensorflow/tensorflow/issues/13317">tensorflow/tensorflow#13317</a>).</p>
</dd>
<dt><strong>keepdeconvweights: bool, optional, default=True</strong></dt><dd><p>Also restores the weights of the deconvolution layers (and the backbone) when
training from a snapshot. Note that if you change the number of bodyparts, you
need to set this to false for re-training.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>superanimal_name: str, optional, default =‚Äù‚Äù</strong></dt><dd><p>Only for the TensorFlow engine. For the PyTorch engine, you need to specify
this through the <code class="docutils literal notranslate"><span class="pre">weight_init</span></code> when creating the training dataset.
Specified if transfer learning with superanimal is desired</p>
</dd>
<dt><strong>superanimal_transfer_learning: bool, optional, default = False.</strong></dt><dd><p>Only for the TensorFlow engine. For the PyTorch engine, you need to specify
this through the <code class="docutils literal notranslate"><span class="pre">weight_init</span></code> when creating the training dataset.
If set true, the training is transfer learning (new decoding layer). If set
false, and superanimal_name is True, then the training is fine-tuning (reusing
the decoding layer)</p>
</dd>
<dt><strong>engine: Engine, optional, default = None.</strong></dt><dd><p>The default behavior loads the engine for the shuffle from the metadata. You can
overwrite this by passing the engine as an argument, but this should generally
not be done.</p>
</dd>
<dt><strong>device: str, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The device to run the training on (e.g. ‚Äúcuda:0‚Äù)</p>
</dd>
<dt><strong>snapshot_path: str or Path, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The path to the pose model snapshot to resume training from.</p>
</dd>
<dt><strong>detector_path: str or Path, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The path to the detector model snapshot to resume training from.</p>
</dd>
<dt><strong>batch_size: int, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The batch size to use while training.</p>
</dd>
<dt><strong>detector_batch_size: int, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The batch size to use while training the detector.</p>
</dd>
<dt><strong>detector_epochs: int, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The number of epochs to train the detector for.</p>
</dd>
<dt><strong>detector_save_epochs: int, optional, default = None.</strong></dt><dd><p>Only for the PyTorch engine. The number of epochs between each detector snapshot save.</p>
</dd>
<dt><strong>pose_threshold: float, optional, default = 0.1.</strong></dt><dd><dl class="simple">
<dt>Only for the PyTorch engine. Used for memory-replay. Pseudo-predictions with confidence lower</dt><dd><p>than this threshold are discarded for memory-replay</p>
</dd>
</dl>
</dd>
<dt><strong>pytorch_cfg_updates: dict, optional, default = None.</strong></dt><dd><p>A dictionary of updates to the pytorch config. The keys are the dot-separated
paths to the values to update in the config.
For example, to update the gpus to run the training on, you can use:
<code class="docutils literal notranslate"><span class="pre">`</span>
<span class="pre">pytorch_cfg_updates={&quot;runner.gpus&quot;:</span> <span class="pre">[0,1,2,3]}</span>
<span class="pre">`</span></code></p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>To train the network for first shuffle of the training dataset</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span><span class="s1">&#39;/analysis/project/reaching-task/config.yaml&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>To train the network for second shuffle of the training dataset</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        shuffle=2,</span>
<span class="go">        keepdeconvweights=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>To train the network for shuffle created with a PyTorch engine, while overriding the
number of epochs, batch size and other parameters.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        shuffle=1,</span>
<span class="go">        batch_size=8,</span>
<span class="go">        epochs=100,</span>
<span class="go">        save_epochs=10,</span>
<span class="go">        displayiters=50,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="h-evaluate-the-trained-network">
<h3>(H) Evaluate the Trained Network<a class="headerlink" href="#h-evaluate-the-trained-network" title="Link to this heading">#</a></h3>
<p>It is important to evaluate the performance of the trained network. This performance is measured by computing
the average root mean square error (RMSE) between the manual labels and the ones predicted by DeepLabCut.
The RMSE is saved as a comma separated file and displayed for all pairs and only likely pairs (&gt;p-cutoff).
This helps to exclude, for example, occluded body parts. One of the strengths of DeepLabCut is that due to the
probabilistic output of the scoremap, it can, if sufficiently trained, also reliably report if a body part is visible
in a given frame. (see discussions of finger tips in reaching and the Drosophila legs during 3D behavior in
[Mathis et al, 2018]). The evaluation results are computed by typing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">Shuffles</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">plotting</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>Setting <code class="docutils literal notranslate"><span class="pre">plotting</span></code> to true plots all the testing and training frames with the manual and predicted labels. The user
should visually check the labeled test (and training) images that are created in the ‚Äòevaluation-results‚Äô directory.
Ideally, DeepLabCut labeled unseen (test images) according to the user‚Äôs required accuracy, and the average train
and test errors are comparable (good generalization). What (numerically) comprises an acceptable RMSE depends on
many factors (including the size of the tracked body parts, the labeling variability, etc.). Note that the test error
can also be larger than the training error due to human variability (in labeling, see Figure 2 in Mathis et al, Nature
Neuroscience 2018).</p>
<p><strong>Optional parameters:</strong></p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">Shuffles:</span> <span class="pre">list,</span> <span class="pre">optional</span></code> - List of integers specifying the shuffle indices of the training dataset.
The default is [1]</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">plotting:</span> <span class="pre">bool,</span> <span class="pre">optional</span></code> - Plots the predictions on the train and test images. The default is <code class="docutils literal notranslate"><span class="pre">False</span></code>;
if provided it must be either <code class="docutils literal notranslate"><span class="pre">True</span></code> or <code class="docutils literal notranslate"><span class="pre">False</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">show_errors:</span> <span class="pre">bool,</span> <span class="pre">optional</span></code> - Display train and test errors. The default is <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">comparisonbodyparts:</span> <span class="pre">list</span> <span class="pre">of</span> <span class="pre">bodyparts,</span> <span class="pre">Default</span> <span class="pre">is</span> <span class="pre">all</span></code> - The average error will be computed for those body parts
only (Has to be a subset of the body parts).</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">gputouse:</span> <span class="pre">int,</span> <span class="pre">optional</span></code> - Natural number indicating the number of your GPU (see number in nvidia-smi). If you do not
have a GPU, put None. See: <a class="reference external" href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries">https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries</a></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">pcutoff:</span> <span class="pre">float</span> <span class="pre">|</span> <span class="pre">list[float]</span> <span class="pre">|</span> <span class="pre">dict[str,</span> <span class="pre">float],</span> <span class="pre">optional</span></code>
(Only applicable when using the PyTorch engine. For TensorFlow, set <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code> in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file.)
Specifies the cutoff value(s) used to compute evaluation metrics.</p>
<ul>
<li><p>If <code class="docutils literal notranslate"><span class="pre">None</span></code> (default), the cutoff will be loaded from the project configuration.</p></li>
<li><p>To apply a single cutoff value to all bodyparts, provide a <code class="docutils literal notranslate"><span class="pre">float</span></code>.</p></li>
<li><p>To specify different cutoffs per bodypart, provide either:</p>
<ul>
<li><p>A <code class="docutils literal notranslate"><span class="pre">list[float]</span></code>: one value per bodypart, with an additional value for each unique bodypart if applicable.</p></li>
<li><p>A <code class="docutils literal notranslate"><span class="pre">dict[str,</span> <span class="pre">float]</span></code>: where keys are bodypart names and values are the corresponding cutoff values.
If a bodypart is not included in the provided dictionary, a default <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code> of <code class="docutils literal notranslate"><span class="pre">0.6</span></code> will be used for that bodypart.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The plots can be customized by editing the <strong>config.yaml</strong> file (i.e., the colormap, scale, marker size (dotsize), and
transparency of labels (alphavalue) can be modified). By default each body part is plotted in a different color
(governed by the colormap) and the plot labels indicate their source. Note that by default the human labels are
plotted as plus (‚Äò+‚Äô), DeepLabCut‚Äôs predictions either as ‚Äò.‚Äô (for confident predictions with likelihood &gt; p-cutoff) and
‚Äôx‚Äô for (likelihood &lt;= <code class="docutils literal notranslate"><span class="pre">pcutoff</span></code>).</p>
<p>The evaluation results for each shuffle of the training dataset are stored in a unique subdirectory in a newly created
directory ‚Äòevaluation-results-pytorch‚Äô (‚Äòevaluation-results‚Äô for tensorflow models) in the project directory.
The user can visually inspect if the distance between the labeled and the predicted body parts are acceptable.
In the event of benchmarking with different shuffles of same training dataset, the user can provide multiple shuffle
indices to evaluate the corresponding network.
Note that with multi-animal projects additional distance statistics aggregated over animals or bodyparts are also stored
in that directory. This aims at providing a finer quantitative evaluation of multi-animal prediction performance
before animal tracking. If the generalization is not sufficient, the user might want to:</p>
<p>‚Ä¢ check if the labels were imported correctly; i.e., invisible points are not labeled and the points of interest are
labeled accurately</p>
<p>‚Ä¢ make sure that the loss has already converged</p>
<p>‚Ä¢ consider labeling additional images and make another iteration of the training data set</p>
<p><strong>OPTIONAL:</strong> You can also plot the scoremaps, locref layers, and PAFs:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_save_all_maps</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle</span><span class="p">,</span> <span class="n">Indices</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</pre></div>
</div>
<p>you can drop ‚ÄúIndices‚Äù to run this on all training/testing images (this is slow!)</p>
</section>
<section id="id3">
<h3>API Docs<a class="headerlink" href="#id3" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.compat.evaluate_network">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.compat.</span></span><span class="sig-name descname"><span class="pre">evaluate_network</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Shuffles</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Iterable</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(1,)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plotting</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">show_errors</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comparisonbodyparts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gputouse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">rescale</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">per_keypoint_evaluation</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">snapshots_to_evaluate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pcutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">dict</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Engine</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">torch_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.compat.evaluate_network" title="Link to this definition">#</a></dt>
<dd><p>Evaluates the network.</p>
<p>Evaluates the network based on the saved models at different stages of the training
network. The evaluation results are stored in the .h5 and .csv file under the
subdirectory ‚Äòevaluation_results‚Äô. Change the snapshotindex parameter in the config
file to ‚Äòall‚Äô in order to evaluate all the saved models.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>Shuffles: list, optional, default=[1]</strong></dt><dd><p>List of integers specifying the shuffle indices of the training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int or str, optional, default=0</strong></dt><dd><p>Integer specifying which ‚ÄúTrainingsetFraction‚Äù to use.
Note that ‚ÄúTrainingFraction‚Äù is a list in config.yaml. This variable can also
be set to ‚Äúall‚Äù.</p>
</dd>
<dt><strong>plotting: bool or str, optional, default=False</strong></dt><dd><p>Plots the predictions on the train and test images.
If provided it must be either <code class="docutils literal notranslate"><span class="pre">True</span></code>, <code class="docutils literal notranslate"><span class="pre">False</span></code>, <code class="docutils literal notranslate"><span class="pre">&quot;bodypart&quot;</span></code>, or
<code class="docutils literal notranslate"><span class="pre">&quot;individual&quot;</span></code>. Setting to <code class="docutils literal notranslate"><span class="pre">True</span></code> defaults as <code class="docutils literal notranslate"><span class="pre">&quot;bodypart&quot;</span></code> for
multi-animal projects.
If a detector is used, the predicted bounding boxes will also be plotted.</p>
</dd>
<dt><strong>show_errors: bool, optional, default=True</strong></dt><dd><p>Display train and test errors.</p>
</dd>
<dt><strong>comparisonbodyparts: str or list, optional, default=‚Äùall‚Äù</strong></dt><dd><p>The average error will be computed for those body parts only.
The provided list has to be a subset of the defined body parts.</p>
</dd>
<dt><strong>gputouse: int or None, optional, default=None</strong></dt><dd><p>Indicates the GPU to use (see number in <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>). If you do not have a
GPU put <cite>None`</cite>.
See: <a class="reference external" href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries">https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries</a></p>
</dd>
<dt><strong>rescale: bool, optional, default=False</strong></dt><dd><p>Evaluate the model at the <code class="docutils literal notranslate"><span class="pre">'global_scale'</span></code> variable (as set in the
<code class="docutils literal notranslate"><span class="pre">pose_config.yaml</span></code> file for a particular project). I.e. every image will be
resized according to that scale and prediction will be compared to the resized
ground truth. The error will be reported in pixels at rescaled to the
<em>original</em> size. I.e. For a [200,200] pixel image evaluated at
<code class="docutils literal notranslate"><span class="pre">global_scale=.5</span></code>, the predictions are calculated on [100,100] pixel images,
compared to 1/2*ground truth and this error is then multiplied by 2!.
The evaluation images are also shown for the original size!</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>per_keypoint_evaluation: bool, default=False</strong></dt><dd><p>Compute the train and test RMSE for each keypoint, and save the results to
a {model_name}-keypoint-results.csv in the evalution-results folder</p>
</dd>
<dt><strong>snapshots_to_evaluate: List[str], optional, default=None</strong></dt><dd><p>List of snapshot names to evaluate (e.g. [‚Äúsnapshot-5000‚Äù, ‚Äúsnapshot-7500‚Äù]).</p>
</dd>
<dt><strong>pcutoff: float | list[float] | dict[str, float] | None, default=None</strong></dt><dd><p>Only for the PyTorch engine. For the TensorFlow engine, please set the pcutoff
in the <cite>config.yaml</cite> file.
The cutoff to use for computing evaluation metrics. When <cite>None</cite> (default), the
cutoff will be loaded from the project config. If a list is provided, there
should be one value for each bodypart and one value for each unique bodypart
(if there are any). If a dict is provided, the keys should be bodyparts
mapping to pcutoff values for each bodypart. Bodyparts that are not defined
in the dict will have pcutoff set to 0.6.</p>
</dd>
<dt><strong>engine: Engine, optional, default = None.</strong></dt><dd><p>The default behavior loads the engine for the shuffle from the metadata. You can
overwrite this by passing the engine as an argument, but this should generally
not be done.</p>
</dd>
<dt><strong>torch_kwargs:</strong></dt><dd><p>You can add any keyword arguments for the deeplabcut.pose_estimation_pytorch
evaluate_network function here. These arguments are passed to the downstream
function. Available parameters are <cite>snapshotindex</cite>, which overrides the
<cite>snapshotindex</cite> parameter in the project configuration file. For top-down models
the <cite>detector_snapshot_index</cite> parameter can override the index of the detector
to use for evaluation in the project configuration file.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>If you do not want to plot and evaluate with shuffle set to 1.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;, Shuffles=[1],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>If you want to plot and evaluate with shuffle set to 0 and 1.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        Shuffles=[0, 1],</span>
<span class="go">        plotting=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>If you want to plot assemblies for a maDLC project</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        Shuffles=[1],</span>
<span class="go">        plotting=&quot;individual&quot;,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>If you have a PyTorch model for which you want to set a different p-cutoff for
‚Äúleft_ear‚Äù and ‚Äúright_ear‚Äù bodyparts, and keep the one set in the project config
for other bodyparts:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">evaluate_network</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;/analysis/project/reaching-task/config.yaml&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">Shuffles</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">pcutoff</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;left_ear&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">&quot;right_ear&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">},</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<p>Note: This defaults to standard plotting for single-animal projects.</p>
</dd></dl>

</div>
</section>
<section id="i-analyze-new-videos">
<h3>(I) Analyze new Videos<a class="headerlink" href="#i-analyze-new-videos" title="Link to this heading">#</a></h3>
<p>The trained network can be used to analyze new videos. Novel/new videos <strong>DO NOT have to be in the config file!</strong>.
You can analyze new videos anytime by simply using the following line of code:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;fullpath/analysis/project/videos/reachingvideo1.avi&quot;</span><span class="p">],</span>
    <span class="n">save_as_csv</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p>There are several other optional inputs, such as:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">videos</span><span class="p">,</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;avi&quot;</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">gputouse</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">save_as_csv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">destfolder</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">dynamic</span><span class="o">=</span><span class="p">(</span><span class="kc">True</span><span class="p">,</span> <span class="mf">.5</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The user can choose a checkpoint for analyzing the videos. For this, the user can enter the corresponding index of the
checkpoint to the variable snapshotindex in the config.yaml file. By default, the most recent checkpoint (i.e. last) is
used for analyzing the video.
The labels are stored in a MultiIndex <a class="reference external" href="http://pandas.pydata.org">Pandas</a> Array, which contains the name of the network,
body part name, (x, y) label position in pixels, and the likelihood for each frame per body part. These arrays are
stored in an efficient Hierarchical Data Format (HDF) in the same directory, where the video is stored.
However, if the flag <code class="docutils literal notranslate"><span class="pre">save_as_csv</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, the data can also be exported in comma-separated values format
(.csv), which in turn can be imported in many programs, such as MATLAB, R, Prism, etc.; This flag is set to <code class="docutils literal notranslate"><span class="pre">False</span></code>
by default. You can also set a destination folder (<code class="docutils literal notranslate"><span class="pre">destfolder</span></code>) for the output files by passing a path of the folder
you wish to write to.</p>
</section>
<section id="id4">
<h3>API Docs<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.compat.analyze_videos">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.compat.</span></span><span class="sig-name descname"><span class="pre">analyze_videos</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gputouse</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_as_csv</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">in_random_order</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">batchsize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cropping</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">TFGPUinference</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dynamic</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">tuple</span><span class="p"><span class="pre">[</span></span><span class="pre">bool</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">int</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">(False,</span> <span class="pre">0.5,</span> <span class="pre">10)</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">robust_nframes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">allow_growth</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_shelve</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">auto_track</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">n_tracks</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">animal_names</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">calibrate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">identity_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">use_openvino</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">engine</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">Engine</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="o"><span class="pre">**</span></span><span class="n"><span class="pre">torch_kwargs</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.compat.analyze_videos" title="Link to this definition">#</a></dt>
<dd><p>Makes prediction based on a trained network.</p>
<p>The index of the trained network is specified by parameters in the config file
(in particular the variable ‚Äòsnapshotindex‚Äô).</p>
<p>The labels are stored as MultiIndex Pandas Array, which contains the name of
the network, body part name, (x, y) label position in pixels, and the
likelihood for each frame per body part. These arrays are stored in an
efficient Hierarchical Data Format (HDF) in the same directory where the video
is stored. However, if the flag save_as_csv is set to True, the data can also
be exported in comma-separated values format (.csv), which in turn can be
imported in many programs, such as MATLAB, R, Prism, etc.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>videos: list[str]</strong></dt><dd><p>A list of strings containing the full paths to videos for analysis or a path to
the directory, where all the videos with same extension are stored.</p>
</dd>
<dt><strong>videotype: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Checks for the extension of the video in case the input to the video is a
directory. Only videos with this extension are analyzed. If left unspecified,
videos with common extensions (‚Äòavi‚Äô, ‚Äòmp4‚Äô, ‚Äòmov‚Äô, ‚Äòmpeg‚Äô, ‚Äòmkv‚Äô) are kept.</p>
</dd>
<dt><strong>shuffle: int, optional, default=1</strong></dt><dd><p>An integer specifying the shuffle index of the training dataset used for
training the network.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
By default the first (note that TrainingFraction is a list in config.yaml).</p>
</dd>
<dt><strong>gputouse: int or None, optional, default=None</strong></dt><dd><p>Only for the TensorFlow engine (for the PyTorch engine see the <code class="docutils literal notranslate"><span class="pre">torch_kwargs</span></code>:
you can use <code class="docutils literal notranslate"><span class="pre">device</span></code>).
Indicates the GPU to use (see number in <code class="docutils literal notranslate"><span class="pre">nvidia-smi</span></code>). If you do not have a
GPU put <code class="docutils literal notranslate"><span class="pre">None</span></code>.
See: <a class="reference external" href="https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries">https://nvidia.custhelp.com/app/answers/detail/a_id/3751/~/useful-nvidia-smi-queries</a></p>
</dd>
<dt><strong>save_as_csv: bool, optional, default=False</strong></dt><dd><p>Saves the predictions in a .csv file.</p>
</dd>
<dt><strong>in_random_order: bool, optional (default=True)</strong></dt><dd><p>Whether or not to analyze videos in a random order.
This is only relevant when specifying a video directory in <cite>videos</cite>.</p>
</dd>
<dt><strong>destfolder: string or None, optional, default=None</strong></dt><dd><p>Specifies the destination folder for analysis data. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of
the video is used. Note that for subsequent analysis this folder also needs to
be passed.</p>
</dd>
<dt><strong>batchsize: int or None, optional, default=None</strong></dt><dd><p>Currently not supported by the PyTorch engine.
Change batch size for inference; if given overwrites value in <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code>.</p>
</dd>
<dt><strong>cropping: list or None, optional, default=None</strong></dt><dd><p>List of cropping coordinates as [x1, x2, y1, y2].
Note that the same cropping parameters will then be used for all videos.
If different video crops are desired, run <code class="docutils literal notranslate"><span class="pre">analyze_videos</span></code> on individual
videos with the corresponding cropping coordinates.</p>
</dd>
<dt><strong>TFGPUinference: bool, optional, default=True</strong></dt><dd><p>Only for the TensorFlow engine.
Perform inference on GPU with TensorFlow code. Introduced in ‚ÄúPretraining
boosts out-of-domain robustness for pose estimation‚Äù by Alexander Mathis,
Mert Y√ºksekg√∂n√ºl, Byron Rogers, Matthias Bethge, Mackenzie W. Mathis.
Source: <a class="reference external" href="https://arxiv.org/abs/1909.11229">https://arxiv.org/abs/1909.11229</a></p>
</dd>
<dt><strong>dynamic: tuple(bool, float, int) triple containing (state, det_threshold, margin)</strong></dt><dd><p>If the state is true, then dynamic cropping will be performed. That means that
if an object is detected (i.e. any body part &gt; detectiontreshold), then object
boundaries are computed according to the smallest/largest x position and
smallest/largest y position of all body parts. This  window is expanded by the
margin and from then on only the posture within this crop is analyzed (until the
object is lost, i.e. &lt;detectiontreshold). The current position is utilized for
updating the crop window for the next frame (this is why the margin is important
and should be set large enough given the movement of the animal).</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>robust_nframes: bool, optional, default=False</strong></dt><dd><p>Evaluate a video‚Äôs number of frames in a robust manner.
This option is slower (as the whole video is read frame-by-frame),
but does not rely on metadata, hence its robustness against file corruption.</p>
</dd>
<dt><strong>allow_growth: bool, optional, default=False.</strong></dt><dd><p>Only for the TensorFlow engine.
For some smaller GPUs the memory issues happen. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, the memory
allocator does not pre-allocate the entire specified GPU memory region, instead
starting small and growing as needed.
See issue: <a class="reference external" href="https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2">https://forum.image.sc/t/how-to-stop-running-out-of-vram/30551/2</a></p>
</dd>
<dt><strong>use_shelve: bool, optional, default=False</strong></dt><dd><p>By default, data are dumped in a pickle file at the end of the video analysis.
Otherwise, data are written to disk on the fly using a ‚Äúshelf‚Äù; i.e., a
pickle-based, persistent, database-like object by default, resulting in
constant memory footprint.</p>
</dd>
<dt><strong>The following parameters are only relevant for multi-animal projects:</strong></dt><dd></dd>
<dt><strong>auto_track: bool, optional, default=True</strong></dt><dd><p>By default, tracking and stitching are automatically performed, producing the
final h5 data file. This is equivalent to the behavior for single-animal
projects.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, one must run <code class="docutils literal notranslate"><span class="pre">convert_detections2tracklets</span></code> and
<code class="docutils literal notranslate"><span class="pre">stitch_tracklets</span></code> afterwards, in order to obtain the h5 file.</p>
</dd>
<dt><strong>This function has 3 related sub-calls:</strong></dt><dd></dd>
<dt><strong>identity_only: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> and animal identity was learned by the model, assembly and tracking
rely exclusively on identity prediction.</p>
</dd>
<dt><strong>calibrate: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, use training data to calibrate the animal assembly procedure. This
improves its robustness to wrong body part links, but requires very little
missing data.</p>
</dd>
<dt><strong>n_tracks: int or None, optional, default=None</strong></dt><dd><p>Number of tracks to reconstruct. By default, taken as the number of individuals
defined in the config.yaml. Another number can be passed if the number of
animals in the video is different from the number of animals the model was
trained on.</p>
</dd>
<dt><strong>animal_names: list[str], optional</strong></dt><dd><p>If you want the names given to individuals in the labeled data file, you can
specify those names as a list here. If given and <cite>n_tracks</cite> is None, <cite>n_tracks</cite>
will be set to <cite>len(animal_names)</cite>. If <cite>n_tracks</cite> is not None, then it must be
equal to <cite>len(animal_names)</cite>. If it is not given, then <cite>animal_names</cite> will
be loaded from the <cite>individuals</cite> in the project config.yaml file.</p>
</dd>
<dt><strong>use_openvino: str, optional</strong></dt><dd><p>Only for the TensorFlow engine.
Use ‚ÄúCPU‚Äù for inference if OpenVINO is available in the Python environment.</p>
</dd>
<dt><strong>engine: Engine, optional, default = None.</strong></dt><dd><p>The default behavior loads the engine for the shuffle from the metadata. You can
overwrite this by passing the engine as an argument, but this should generally
not be done.</p>
</dd>
<dt><strong>torch_kwargs:</strong></dt><dd><p>Any extra parameters to pass to the PyTorch API, such as <code class="docutils literal notranslate"><span class="pre">device</span></code> which can
be used to specify the CUDA device to use for training.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>DLCScorer: str</dt><dd><p>the scorer used to analyze the videos</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Analyzing a single video on Windows</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\yourusername\rig-95\Videos\reachingvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Analyzing a single video on Linux/MacOS</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos/reachingvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Analyze all videos of type <code class="docutils literal notranslate"><span class="pre">avi</span></code> in a folder</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos&#39;],</span>
<span class="go">        videotype=&#39;.avi&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Analyze multiple videos</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo1.avi&#39;,</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo2.avi&#39;,</span>
<span class="go">        ],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Analyze multiple videos with <code class="docutils literal notranslate"><span class="pre">shuffle=2</span></code></p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo1.avi&#39;,</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo2.avi&#39;,</span>
<span class="go">        ],</span>
<span class="go">        shuffle=2,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Analyze multiple videos with <code class="docutils literal notranslate"><span class="pre">shuffle=2</span></code>, save results as an additional csv file</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyze_videos</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo1.avi&#39;,</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo2.avi&#39;,</span>
<span class="go">        ],</span>
<span class="go">        shuffle=2,</span>
<span class="go">        save_as_csv=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="novel-video-analysis-extra-features">
<h3>Novel Video Analysis: extra features<a class="headerlink" href="#novel-video-analysis-extra-features" title="Link to this heading">#</a></h3>
</section>
<section id="dynamic-cropping-of-videos">
<h3>Dynamic-cropping of videos:<a class="headerlink" href="#dynamic-cropping-of-videos" title="Link to this heading">#</a></h3>
<p>As of 2.1+ we have a dynamic cropping option. Namely, if you have large frames and the animal/object occupies a smaller
fraction, you can crop around your animal/object to make processing speeds faster. For example, if you have a large open
field experiment but only track the mouse, this will speed up your analysis (also helpful for real-time applications).
To use this simply add <code class="docutils literal notranslate"><span class="pre">dynamic=(True,.5,10)</span></code> when you call <code class="docutils literal notranslate"><span class="pre">analyze_videos</span></code>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dynamic</span><span class="p">:</span> <span class="n">triple</span> <span class="n">containing</span> <span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">detectiontreshold</span><span class="p">,</span> <span class="n">margin</span><span class="p">)</span>

    <span class="n">If</span> <span class="n">the</span> <span class="n">state</span> <span class="ow">is</span> <span class="n">true</span><span class="p">,</span> <span class="n">then</span> <span class="n">dynamic</span> <span class="n">cropping</span> <span class="n">will</span> <span class="n">be</span> <span class="n">performed</span><span class="o">.</span>
    <span class="n">That</span> <span class="n">means</span> <span class="n">that</span> <span class="k">if</span> <span class="n">an</span> <span class="nb">object</span> <span class="ow">is</span> <span class="n">detected</span> <span class="p">(</span><span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span> <span class="nb">any</span> <span class="n">body</span> <span class="n">part</span> <span class="o">&gt;</span> <span class="n">detectiontreshold</span><span class="p">),</span>
    <span class="n">then</span> <span class="nb">object</span> <span class="n">boundaries</span> <span class="n">are</span> <span class="n">computed</span> <span class="n">according</span> <span class="n">to</span> <span class="n">the</span> <span class="n">smallest</span><span class="o">/</span><span class="n">largest</span> <span class="n">x</span> <span class="n">position</span> <span class="ow">and</span>
    <span class="n">smallest</span><span class="o">/</span><span class="n">largest</span> <span class="n">y</span> <span class="n">position</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">body</span> <span class="n">parts</span><span class="o">.</span> <span class="n">This</span> <span class="n">window</span> <span class="ow">is</span> <span class="n">expanded</span> <span class="n">by</span> <span class="n">the</span> <span class="n">margin</span>
    <span class="ow">and</span> <span class="kn">from</span><span class="w"> </span><span class="nn">then</span> <span class="n">on</span> <span class="n">only</span> <span class="n">the</span> <span class="n">posture</span> <span class="n">within</span> <span class="n">this</span> <span class="n">crop</span> <span class="ow">is</span> <span class="n">analyzed</span> <span class="p">(</span><span class="n">until</span> <span class="n">the</span> <span class="nb">object</span> <span class="ow">is</span> <span class="n">lost</span><span class="p">;</span>
    <span class="n">i</span><span class="o">.</span><span class="n">e</span><span class="o">.</span><span class="p">,</span> <span class="o">&lt;</span> <span class="n">detectiontreshold</span><span class="p">)</span><span class="o">.</span> <span class="n">The</span> <span class="n">current</span> <span class="n">position</span> <span class="ow">is</span> <span class="n">utilized</span> <span class="k">for</span> <span class="n">updating</span> <span class="n">the</span> <span class="n">crop</span> <span class="n">window</span>
    <span class="k">for</span> <span class="n">the</span> <span class="nb">next</span> <span class="n">frame</span> <span class="p">(</span><span class="n">this</span> <span class="ow">is</span> <span class="n">why</span> <span class="n">the</span> <span class="n">margin</span> <span class="ow">is</span> <span class="n">important</span> <span class="ow">and</span> <span class="n">should</span> <span class="n">be</span> <span class="nb">set</span> <span class="n">large</span> <span class="n">enough</span>
    <span class="n">given</span> <span class="n">the</span> <span class="n">movement</span> <span class="n">of</span> <span class="n">the</span> <span class="n">animal</span><span class="p">)</span><span class="o">.</span>
</pre></div>
</div>
</section>
<section id="j-filter-pose-data">
<h3>(J) Filter Pose Data<a class="headerlink" href="#j-filter-pose-data" title="Link to this heading">#</a></h3>
<p>You can also filter the predictions with a median filter (default) or with a <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html">SARIMAX model</a>, if you wish. This creates a new .h5 file with the ending <em>_filtered</em> that you can use in create_labeled_data and/or plot trajectories.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/analysis/project/videos/reachingvideo1.avi&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>An example call:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/analysis/project/videos&quot;</span><span class="p">],</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;.mp4&quot;</span><span class="p">,</span>
    <span class="n">filtertype</span><span class="o">=</span><span class="s2">&quot;arima&quot;</span><span class="p">,</span>
    <span class="n">ARdegree</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
    <span class="n">MAdegree</span><span class="o">=</span><span class="mi">2</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here are parameters you can modify and pass:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/analysis/project/videos/reachingvideo1.avi&quot;</span><span class="p">],</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">filtertype</span><span class="o">=</span><span class="s2">&quot;arima&quot;</span><span class="p">,</span>
    <span class="n">p_bound</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span>
    <span class="n">ARdegree</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
    <span class="n">MAdegree</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">alpha</span><span class="o">=</span><span class="mf">0.01</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here is an example of how this can be applied to a video:</p>
 <p align="center">
<img src="https://static1.squarespace.com/static/57f6d51c9f74566f55ecf271/t/5ccc8b8ae6e8df000100a995/1556908943893/filter_example-01.png?format=1000w" width="70%">
</p>
</section>
<section id="id5">
<h3>API Docs<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.post_processing.filtering.filterpredictions">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.post_processing.filtering.</span></span><span class="sig-name descname"><span class="pre">filterpredictions</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">video</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filtertype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'median'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">windowlength</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.001</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ARdegree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">MAdegree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_as_csv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.post_processing.filtering.filterpredictions" title="Link to this definition">#</a></dt>
<dd><p>Fits frame-by-frame pose predictions.</p>
<p>The pose predictions are fitted with ARIMA model (filtertype=‚Äôarima‚Äô) or median
filter (default).</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>video</strong><span class="classifier">string</span></dt><dd><p>Full path of the video to extract the frame from. Make sure that this video is
already analyzed.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>The shuffle index of training dataset. The extracted frames will be stored in
the labeled-dataset for the corresponding shuffle of training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>filtertype: string, optional, default=‚Äùmedian‚Äù.</strong></dt><dd><p>The filter type - ‚Äòarima‚Äô, ‚Äòmedian‚Äô or ‚Äòspline‚Äô.</p>
</dd>
<dt><strong>windowlength: int, optional, default=5</strong></dt><dd><p>For filtertype=‚Äômedian‚Äô filters the input array using a local window-size given
by windowlength. The array will automatically be zero-padded.
<a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html">https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.medfilt.html</a>.
The windowlenght should be an odd number.
If filtertype=‚Äôspline‚Äô, windowlength is the maximal gap size to fill.</p>
</dd>
<dt><strong>p_bound: float between 0 and 1, optional, default=0.001</strong></dt><dd><p>For filtertype ‚Äòarima‚Äô this parameter defines the likelihood below,
below which a body part will be consided as missing data for filtering purposes.</p>
</dd>
<dt><strong>ARdegree: int, optional, default=3</strong></dt><dd><p>For filtertype ‚Äòarima‚Äô Autoregressive degree of Sarimax model degree.
see <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html">https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html</a></p>
</dd>
<dt><strong>MAdegree: int, optional, default=1</strong></dt><dd><p>For filtertype ‚Äòarima‚Äô Moving Average degree of Sarimax model degree.
See <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html">https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html</a></p>
</dd>
<dt><strong>alpha: float, optional, default=0.01</strong></dt><dd><p>Significance level for detecting outliers based on confidence interval of fitted SARIMAX model.</p>
</dd>
<dt><strong>save_as_csv: bool, optional, default=True</strong></dt><dd><p>Saves the predictions in a .csv file.</p>
</dd>
<dt><strong>destfolder: string, optional, default=None</strong></dt><dd><p>Specifies the destination folder for analysis data. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of
the video is used by default. Note that for subsequent analysis this folder
also needs to be passed.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>track_method: string, optional, default=‚Äù‚Äù</strong></dt><dd><p>Specifies the tracker used to generate the data.
Empty by default (corresponding to a single animal project).
For multiple animals, must be either ‚Äòbox‚Äô, ‚Äòskeleton‚Äô, or ‚Äòellipse‚Äô and will
be taken from the config.yaml file if none is given.</p>
</dd>
<dt><strong>return_data: bool, optional, default=False</strong></dt><dd><p>If True, returns a dictionary of the filtered data keyed by video names.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>video_to_filtered_df</dt><dd><p>Dictionary mapping video filepaths to filtered dataframes.</p>
<ul class="simple">
<li><p>If no videos exist, the dictionary will be empty.</p></li>
<li><p>If a video is not analyzed, the corresponding value in the dictionary will be
None.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Arima model:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\myproject\trailtracking-task\test.mp4&#39;],</span>
<span class="go">        shuffle=3,</span>
<span class="go">        filterype=&#39;arima&#39;,</span>
<span class="go">        ARdegree=5,</span>
<span class="go">        MAdegree=2,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Use median filter over 10 bins:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">filterpredictions</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\myproject\trailtracking-task\test.mp4&#39;],</span>
<span class="go">        shuffle=3,</span>
<span class="go">        windowlength=10,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>One can then use the filtered rather than the frame-by-frame predictions by calling:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">plot_trajectories</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\myproject\trailtracking-task\test.mp4&#39;],</span>
<span class="go">        shuffle=3,</span>
<span class="go">        filtered=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\myproject\trailtracking-task\test.mp4&#39;],</span>
<span class="go">        shuffle=3,</span>
<span class="go">        filtered=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="k-plot-trajectories">
<h3>(K) Plot Trajectories<a class="headerlink" href="#k-plot-trajectories" title="Link to this heading">#</a></h3>
<p>The plotting components of this toolbox utilizes matplotlib. Therefore, these plots can easily be customized by
the end user. We also provide a function to plot the trajectory of the extracted poses across the analyzed video, which
can be called by typing:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>deeplabcut.plot_trajectories(config_path, [‚Äòfullpath/analysis/project/videos/reachingvideo1.avi‚Äô])
</pre></div>
</div>
<p>It creates a folder called <code class="docutils literal notranslate"><span class="pre">plot-poses</span></code> (in the directory of the video). The plots display the coordinates of body parts
vs. time, likelihoods vs time, the x- vs. y- coordinate of the body parts, as well as histograms of consecutive
coordinate differences. These plots help the user to quickly assess the tracking performance for a video. Ideally, the
likelihood stays high and the histogram of consecutive coordinate differences has values close to zero (i.e. no jumps in
body part detections across frames). Here are example plot outputs on a demo video (left):</p>
<p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559946148685-WHDO5IG9MMCHU0T7RC62/ke17ZwdGBToddI8pDm48kEOb1vFO6oRDmR8SXh4iL21Zw-zPPgdn4jUwVcJE1ZvWEtT5uBSRWt4vQZAgTJucoTqqXjS3CfNDSuuf31e0tVG1gXK66ltnjKh4U2immgm7AVAdfOWODmXNLQLqbLRZ2DqWIIaSPh2v08GbKqpiV54/file0289.png?format=500w" height="240">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559939762886-CCB0R107I2HXAHZLHECP/ke17ZwdGBToddI8pDm48kNeA8e5AnyMqj80u4_mB0hV7gQa3H78H3Y0txjaiv_0fDoOvxcdMmMKkDsyUqMSsMWxHk725yiiHCCLfrh8O1z5QPOohDIaIeljMHgDF5CVlOqpeNLcJ80NK65_fV7S1UcpboONgOQYHLzaUWEI1Ir9fXt7Ehyn7DSgU3GCReAA-ZDqXZYzu2fuaodM4POSZ4w/plot_poses-01.png?format=1000w" height="250">
</p>
</section>
<section id="id6">
<h3>API Docs<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.utils.plotting.plot_trajectories">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.utils.plotting.</span></span><span class="sig-name descname"><span class="pre">plot_trajectories</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filtered</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displayedbodyparts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displayedindividuals</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">showfigures</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">imagetype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'.png'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">resolution</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">100</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">linewidth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1.0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pcutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.utils.plotting.plot_trajectories" title="Link to this definition">#</a></dt>
<dd><p>Plots the trajectories of various bodyparts across the video.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>videos: list[str]</strong></dt><dd><p>Full paths to videos for analysis or a path to the directory, where all the
videos with same extension are stored.</p>
</dd>
<dt><strong>videotype: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Checks for the extension of the video in case the input to the video is a
directory. Only videos with this extension are analyzed.
If left unspecified, videos with common extensions
(‚Äòavi‚Äô, ‚Äòmp4‚Äô, ‚Äòmov‚Äô, ‚Äòmpeg‚Äô, ‚Äòmkv‚Äô) are kept.</p>
</dd>
<dt><strong>shuffle: int, optional, default=1</strong></dt><dd><p>Integer specifying the shuffle index of the training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>filtered: bool, optional, default=False</strong></dt><dd><p>Boolean variable indicating if filtered output should be plotted rather than
frame-by-frame predictions. Filtered version can be calculated with
<code class="docutils literal notranslate"><span class="pre">deeplabcut.filterpredictions</span></code>.</p>
</dd>
<dt><strong>displayedbodyparts: list[str] or str, optional, default=‚Äùall‚Äù</strong></dt><dd><p>This select the body parts that are plotted in the video.
Either <code class="docutils literal notranslate"><span class="pre">all</span></code>, then all body parts from config.yaml are used,
or a list of strings that are a subset of the full list.
E.g. [‚Äòhand‚Äô,‚ÄôJoystick‚Äô] for the demo Reaching-Mackenzie-2018-08-30/config.yaml
to select only these two body parts.</p>
</dd>
<dt><strong>showfigures: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> then plots are also displayed.</p>
</dd>
<dt><strong>destfolder: string or None, optional, default=None</strong></dt><dd><p>Specifies the destination folder that was used for storing analysis data. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of the video is used.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>imagetype: string, optional, default=‚Äù.png‚Äù</strong></dt><dd><p>Specifies the output image format - ‚Äò.tif‚Äô, ‚Äò.jpg‚Äô, ‚Äò.svg‚Äô and ‚Äú.png‚Äù.</p>
</dd>
<dt><strong>resolution: int, optional, default=100</strong></dt><dd><p>Specifies the resolution (in dpi) of saved figures.
Note higher resolution figures take longer to generate.</p>
</dd>
<dt><strong>linewidth: float, optional, default=1.0</strong></dt><dd><p>Specifies width of line for line and histogram plots.</p>
</dd>
<dt><strong>track_method: string, optional, default=‚Äù‚Äù</strong></dt><dd><p>Specifies the tracker used to generate the data.
Empty by default (corresponding to a single animal project).
For multiple animals, must be either ‚Äòbox‚Äô, ‚Äòskeleton‚Äô, or ‚Äòellipse‚Äô and will
be taken from the config.yaml file if none is given.</p>
</dd>
<dt><strong>pcutoff: string, optional, default=None</strong></dt><dd><p>Overrides the pcutoff set in the project configuration to plot the trajectories.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>To label the frames</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">plot_trajectories</span><span class="p">(</span>
<span class="go">        &#39;home/alex/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/home/alex/analysis/project/videos/reachingvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="l-create-labeled-videos">
<h3>(L) Create Labeled Videos<a class="headerlink" href="#l-create-labeled-videos" title="Link to this heading">#</a></h3>
<p>Additionally, the toolbox provides a function to create labeled videos based on the extracted poses by plotting the
labels on top of the frame and creating a video. There are two modes to create videos: FAST and SLOW (but higher
quality!). One can use the command as follows to create multiple labeled videos:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/analysis/project/videos/reachingvideo1.avi&quot;</span><span class="p">,</span>
     <span class="s2">&quot;fullpath/analysis/project/videos/reachingvideo2.avi&quot;</span><span class="p">],</span>
    <span class="n">save_frames</span> <span class="o">=</span> <span class="kc">True</span><span class="o">/</span><span class="kc">False</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Optionally, if you want to use the filtered data for a video or directory of filtered videos pass <code class="docutils literal notranslate"><span class="pre">filtered=True</span></code>,
i.e.:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/afolderofvideos&quot;</span><span class="p">],</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;.mp4&quot;</span><span class="p">,</span>
    <span class="n">filtered</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can also optionally add a skeleton to connect points and/or add a history of points for visualization. To set the
‚Äútrailing points‚Äù you need to pass <code class="docutils literal notranslate"><span class="pre">trailpoints</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/afolderofvideos&quot;</span><span class="p">],</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;.mp4&quot;</span><span class="p">,</span>
    <span class="n">trailpoints</span><span class="o">=</span><span class="mi">10</span>
<span class="p">)</span>
</pre></div>
</div>
<p>To draw a skeleton, you need to first define the pairs of connected nodes (in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file) and set the
skeleton color (in the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> file). There is also a GUI to help you do this, use by calling
<code class="docutils literal notranslate"><span class="pre">deeplabcut.SkeletonBuilder(configpath)</span></code>!</p>
<p>Here is how the <code class="docutils literal notranslate"><span class="pre">config.yaml</span></code> additions/edits should look (for example, on the Openfield demo data we provide):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plotting configuration</span>
<span class="n">skeleton</span><span class="p">:</span>
  <span class="o">-</span> <span class="p">[</span><span class="s2">&quot;snout&quot;</span><span class="p">,</span> <span class="s2">&quot;leftear&quot;</span><span class="p">]</span>
  <span class="o">-</span> <span class="p">[</span><span class="s2">&quot;snout&quot;</span><span class="p">,</span> <span class="s2">&quot;rightear&quot;</span><span class="p">]</span>
  <span class="o">-</span> <span class="p">[</span><span class="s2">&quot;leftear&quot;</span><span class="p">,</span> <span class="s2">&quot;tailbase&quot;</span><span class="p">]</span>
  <span class="o">-</span> <span class="p">[</span><span class="s2">&quot;leftear&quot;</span><span class="p">,</span> <span class="s2">&quot;rightear&quot;</span><span class="p">]</span>
  <span class="o">-</span> <span class="p">[</span><span class="s2">&quot;rightear&quot;</span><span class="p">,</span> <span class="s2">&quot;tailbase&quot;</span><span class="p">]</span>
<span class="n">skeleton_color</span><span class="p">:</span> <span class="n">white</span>
<span class="n">pcutoff</span><span class="p">:</span> <span class="mf">0.4</span>
<span class="n">dotsize</span><span class="p">:</span> <span class="mi">4</span>
<span class="n">alphavalue</span><span class="p">:</span> <span class="mf">0.5</span>
<span class="n">colormap</span><span class="p">:</span> <span class="n">jet</span>
</pre></div>
</div>
<p>Then pass <code class="docutils literal notranslate"><span class="pre">draw_skeleton=True</span></code> with the command:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="p">[</span><span class="s2">&quot;fullpath/afolderofvideos&quot;</span><span class="p">],</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;.mp4&quot;</span><span class="p">,</span>
    <span class="n">draw_skeleton</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>NEW</strong> as of 2.2b8: You can create a video with only the ‚Äúdots‚Äù plotted, i.e., in the
<a class="reference external" href="https://link.springer.com/article/10.1007/BF00309043">style of Johansson</a>, by passing <code class="docutils literal notranslate"><span class="pre">keypoints_only=True</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,[</span><span class="s2">&quot;fullpath/afolderofvideos&quot;</span><span class="p">],</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;.mp4&quot;</span><span class="p">,</span>
    <span class="n">keypoints_only</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)</span>
</pre></div>
</div>
<p><strong>PRO TIP:</strong> that the <strong>best quality videos</strong> are created when <code class="docutils literal notranslate"><span class="pre">fastmode=False</span></code> is passed. Therefore, when
<code class="docutils literal notranslate"><span class="pre">trailpoints</span></code> and <code class="docutils literal notranslate"><span class="pre">draw_skeleton</span></code> are used, we <strong>highly</strong> recommend you also pass <code class="docutils literal notranslate"><span class="pre">fastmode=False</span></code>!</p>
 <p align="center">
<img src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1559935526258-KFYZC8BDHK01ZIDPNVIX/ke17ZwdGBToddI8pDm48kJbosy0LGK_KqcAZRQ_Qph1Zw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzkC6kmM1CbNgeHQVxASNv0wiXikHv274BIFe4LR7nd1rKmAka4uxYMJ9FupazBoaU/mouse_skel_trail.gif?format=750w" width="40%">
</p>
<p>This function has various other parameters, in particular the user can set the <code class="docutils literal notranslate"><span class="pre">colormap</span></code>, the <code class="docutils literal notranslate"><span class="pre">dotsize</span></code>, and
<code class="docutils literal notranslate"><span class="pre">alphavalue</span></code> of the labels in <strong>config.yaml</strong> file.</p>
</section>
<section id="id7">
<h3>API Docs<a class="headerlink" href="#id7" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.utils.make_labeled_video.create_labeled_video">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.utils.make_labeled_video.</span></span><span class="sig-name descname"><span class="pre">create_labeled_video</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filtered</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">fastmode</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_frames</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">keypoints_only</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">Frames2plot</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">int</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displayedbodyparts</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displayedindividuals</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span><span class="p"><span class="pre">[</span></span><span class="pre">str</span><span class="p"><span class="pre">]</span></span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">codec</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'mp4v'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outputframerate</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">draw_skeleton</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trailpoints</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">displaycropped</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">color_by</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'bodypart'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">init_weights</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_method</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">superanimal_name</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">pcutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skeleton</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">list</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">[]</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">skeleton_color</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'white'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dotsize</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">int</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">8</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">colormap</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">'rainbow'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alphavalue</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">0.5</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">overwrite</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">confidence_to_alpha</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Callable</span><span class="p"><span class="pre">[</span></span><span class="p"><span class="pre">[</span></span><span class="pre">float</span><span class="p"><span class="pre">]</span></span><span class="p"><span class="pre">,</span></span><span class="w"> </span><span class="pre">float</span><span class="p"><span class="pre">]</span></span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">plot_bboxes</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">bool</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bboxes_pcutoff</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">float</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.utils.make_labeled_video.create_labeled_video" title="Link to this definition">#</a></dt>
<dd><p>Labels the bodyparts in a video.</p>
<p>Make sure the video is already analyzed by the function
<code class="docutils literal notranslate"><span class="pre">deeplabcut.analyze_videos</span></code>.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config</strong><span class="classifier">string</span></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>videos</strong><span class="classifier">list[str]</span></dt><dd><p>A list of strings containing the full paths to videos for analysis or a path
to the directory, where all the videos with same extension are stored.</p>
</dd>
<dt><strong>videotype: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Checks for the extension of the video in case the input to the video is a
directory. Only videos with this extension are analyzed.
If left unspecified, videos with common extensions
(‚Äòavi‚Äô, ‚Äòmp4‚Äô, ‚Äòmov‚Äô, ‚Äòmpeg‚Äô, ‚Äòmkv‚Äô) are kept.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>Number of shuffles of training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>filtered: bool, optional, default=False</strong></dt><dd><p>Boolean variable indicating if filtered output should be plotted rather than
frame-by-frame predictions. Filtered version can be calculated with
<code class="docutils literal notranslate"><span class="pre">deeplabcut.filterpredictions</span></code>.</p>
</dd>
<dt><strong>fastmode: bool, optional, default=True</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, uses openCV (much faster but less customization of video) instead
of matplotlib if <code class="docutils literal notranslate"><span class="pre">False</span></code>. You can also ‚Äúsave_frames‚Äù individually or not in
the matplotlib mode (if you set the ‚Äúsave_frames‚Äù variable accordingly).
However, using matplotlib to create the frames it therefore allows much more
flexible (one can set transparency of markers, crop, and easily customize).</p>
</dd>
<dt><strong>save_frames: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, creates each frame individual and then combines into a video.
Setting this to <code class="docutils literal notranslate"><span class="pre">True</span></code> is relatively slow as it stores all individual frames.</p>
</dd>
<dt><strong>keypoints_only: bool, optional, default=False</strong></dt><dd><p>By default, both video frames and keypoints are visible. If <code class="docutils literal notranslate"><span class="pre">True</span></code>, only the
keypoints are shown. These clips are an hommage to Johansson movies,
see <a class="reference external" href="https://www.youtube.com/watch?v=1F5ICP9SYLU">https://www.youtube.com/watch?v=1F5ICP9SYLU</a> and of course his seminal
paper: ‚ÄúVisual perception of biological motion and a model for its analysis‚Äù
by Gunnar Johansson in Perception &amp; Psychophysics 1973.</p>
</dd>
<dt><strong>Frames2plot: List[int] or None, optional, default=None</strong></dt><dd><p>If not <code class="docutils literal notranslate"><span class="pre">None</span></code> and <code class="docutils literal notranslate"><span class="pre">save_frames=True</span></code> then the frames corresponding to the
index will be plotted. For example, <code class="docutils literal notranslate"><span class="pre">Frames2plot=[0,11]</span></code> will plot the first
and the 12th frame.</p>
</dd>
<dt><strong>displayedbodyparts: list[str] or str, optional, default=‚Äùall‚Äù</strong></dt><dd><p>This selects the body parts that are plotted in the video. If <code class="docutils literal notranslate"><span class="pre">all</span></code>, then all
body parts from config.yaml are used. If a list of strings that are a subset of
the full list. E.g. [‚Äòhand‚Äô,‚ÄôJoystick‚Äô] for the demo
Reaching-Mackenzie-2018-08-30/config.yaml to select only these body parts.</p>
</dd>
<dt><strong>displayedindividuals: list[str] or str, optional, default=‚Äùall‚Äù</strong></dt><dd><p>Individuals plotted in the video.
By default, all individuals present in the config will be shown.</p>
</dd>
<dt><strong>codec: str, optional, default=‚Äùmp4v‚Äù</strong></dt><dd><p>Codec for labeled video. For available options, see
<a class="reference external" href="http://www.fourcc.org/codecs.php">http://www.fourcc.org/codecs.php</a>. Note that this depends on your ffmpeg
installation.</p>
</dd>
<dt><strong>outputframerate: int or None, optional, default=None</strong></dt><dd><p>Positive number, output frame rate for labeled video (only available for the
mode with saving frames.) If <code class="docutils literal notranslate"><span class="pre">None</span></code>, which results in the original video
rate.</p>
</dd>
<dt><strong>destfolder: Path, string or None, optional, default=None</strong></dt><dd><p>Specifies the destination folder that was used for storing analysis data. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of the video file is used.</p>
</dd>
<dt><strong>draw_skeleton: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> adds a line connecting the body parts making a skeleton on each
frame. The body parts to be connected and the color of these connecting lines
are specified in the config file.</p>
</dd>
<dt><strong>trailpoints: int, optional, default=0</strong></dt><dd><p>Number of previous frames whose body parts are plotted in a frame
(for displaying history).</p>
</dd>
<dt><strong>displaycropped: bool, optional, default=False</strong></dt><dd><p>Specifies whether only cropped frame is displayed (with labels analyzed
therein), or the original frame with the labels analyzed in the cropped subset.</p>
</dd>
<dt><strong>color_by</strong><span class="classifier">string, optional, default=‚Äôbodypart‚Äô</span></dt><dd><p>Coloring rule. By default, each bodypart is colored differently.
If set to ‚Äòindividual‚Äô, points belonging to a single individual are colored the
same.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>init_weights: str,</strong></dt><dd><p>Checkpoint path to the super model</p>
</dd>
<dt><strong>track_method: string, optional, default=‚Äù‚Äù</strong></dt><dd><p>Specifies the tracker used to generate the data.
Empty by default (corresponding to a single animal project).
For multiple animals, must be either ‚Äòbox‚Äô, ‚Äòskeleton‚Äô, or ‚Äòellipse‚Äô and will
be taken from the config.yaml file if none is given.</p>
</dd>
<dt><strong>superanimal_name: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Name of the superanimal model.</p>
</dd>
<dt><strong>pcutoff: float, optional, default=None</strong></dt><dd><p>Overrides the pcutoff set in the project configuration to plot the trajectories.</p>
</dd>
<dt><strong>skeleton: list, optional, default=[],</strong></dt><dd></dd>
<dt><strong>skeleton_color: string, optional, default=‚Äùwhite‚Äù,</strong></dt><dd><p>Color for the skeleton</p>
</dd>
<dt><strong>dotsize, int, optional, default=8,</strong></dt><dd><p>Size of label dots tu use</p>
</dd>
<dt><strong>colormap: str, optional, default=‚Äùrainbow‚Äù,</strong></dt><dd><p>Colormap to use for the labels</p>
</dd>
<dt><strong>alphavalue: float, optional, default=0.5,</strong></dt><dd></dd>
<dt><strong>overwrite: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code> overwrites existing labeled videos.</p>
</dd>
<dt><strong>confidence_to_alpha: Union[bool, Callable[[float], float], default=False</strong></dt><dd><p>If False, all keypoints will be plot with alpha=1. Otherwise, this can be
defined as a function f: [0, 1] -&gt; [0, 1] such that the alpha value for a
keypoint will be set as a function of its score: alpha = f(score). The default
function used when True is f(x) = max(0, (x - pcutoff)/(1 - pcutoff)).</p>
</dd>
<dt><strong>plot_bboxes: bool, optional, default=True</strong></dt><dd><p>If using Pytorch and in Top-Down mode, setting this to true will also plot the bounding boxes</p>
</dd>
<dt><strong>bboxes_pcutoff, float, optional, default=None:</strong></dt><dd><p>If plotting bounding boxes, this overrides the bboxes_pcutoff set in the model configuration.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt><strong>results</strong><span class="classifier">list[bool]</span></dt><dd></dd>
<dt><code class="docutils literal notranslate"><span class="pre">True</span></code> if the video is successfully created for each item in <code class="docutils literal notranslate"><span class="pre">videos</span></code>.</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Create the labeled video for a single video</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos/reachingvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Create the labeled video for a single video and store the individual frames</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos/reachingvideo1.avi&#39;],</span>
<span class="go">        fastmode=True,</span>
<span class="go">        save_frames=True,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Create the labeled video for multiple videos</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo1.avi&#39;,</span>
<span class="go">            &#39;/analysis/project/videos/reachingvideo2.avi&#39;,</span>
<span class="go">        ],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Create the labeled video for all the videos with an .avi extension in a directory.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos/&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Create the labeled video for all the videos with an .mp4 extension in a directory.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">create_labeled_video</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/videos/&#39;],</span>
<span class="go">        videotype=&#39;mp4&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="extract-skeleton-features">
<h3>Extract ‚ÄúSkeleton‚Äù Features:<a class="headerlink" href="#extract-skeleton-features" title="Link to this heading">#</a></h3>
<p>NEW, as of 2.0.7+: You can save the ‚Äúskeleton‚Äù that was applied in <code class="docutils literal notranslate"><span class="pre">create_labeled_videos</span></code> for more computations.
Namely,  it extracts length and orientation of each ‚Äúbone‚Äù of the skeleton as defined in the <strong>config.yaml</strong> file. You
can use the function by:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">analyzeskeleton</span><span class="p">(</span>
    <span class="n">config</span><span class="p">,</span>
    <span class="n">video</span><span class="p">,</span>
    <span class="n">videotype</span><span class="o">=</span><span class="s2">&quot;avi&quot;</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">trainingsetindex</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">save_as_csv</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">destfolder</span><span class="o">=</span><span class="kc">None</span>
<span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3>API Docs<a class="headerlink" href="#id8" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.post_processing.analyze_skeleton.analyzeskeleton">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.post_processing.analyze_skeleton.</span></span><span class="sig-name descname"><span class="pre">analyzeskeleton</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">filtered</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">save_as_csv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">return_data</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.post_processing.analyze_skeleton.analyzeskeleton" title="Link to this definition">#</a></dt>
<dd><p>Extracts length and orientation of each ‚Äúbone‚Äù of the skeleton.</p>
<p>The bone and skeleton information is defined in the config file.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>videos: list[str]</strong></dt><dd><p>The full paths to videos for analysis or a path to the directory, where all the
videos with same extension are stored.</p>
</dd>
<dt><strong>videotype: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Checks for the extension of the video in case the input to the video is a
directory. Only videos with this extension are analyzed.
If left unspecified, videos with common extensions
(‚Äòavi‚Äô, ‚Äòmp4‚Äô, ‚Äòmov‚Äô, ‚Äòmpeg‚Äô, ‚Äòmkv‚Äô) are kept.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>The shuffle index of training dataset. The extracted frames will be stored in
the labeled-dataset for the corresponding shuffle of training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>filtered: bool, optional, default=False</strong></dt><dd><p>Boolean variable indicating if filtered output should be plotted rather than
frame-by-frame predictions. Filtered version can be calculated with
<code class="docutils literal notranslate"><span class="pre">deeplabcut.filterpredictions</span></code>.</p>
</dd>
<dt><strong>save_as_csv: bool, optional, default=False</strong></dt><dd><p>Saves the predictions in a .csv file.</p>
</dd>
<dt><strong>destfolder: string or None, optional, default=None</strong></dt><dd><p>Specifies the destination folder for analysis data. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of
the video is used. Note that for subsequent analysis this folder also needs to
be passed.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>track_method: string, optional, default=‚Äù‚Äù</strong></dt><dd><p>Specifies the tracker used to generate the data.
Empty by default (corresponding to a single animal project).
For multiple animals, must be either ‚Äòbox‚Äô, ‚Äòskeleton‚Äô, or ‚Äòellipse‚Äô and will
be taken from the config.yaml file if none is given.</p>
</dd>
<dt><strong>return_data: bool, optional, default=False</strong></dt><dd><p>If True, returns a dictionary of the filtered data keyed by video names.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>video_to_skeleton_df</dt><dd><p>Dictionary mapping video filepaths to skeleton dataframes.</p>
<ul class="simple">
<li><p>If no videos exist, the dictionary will be empty.</p></li>
<li><p>If a video is not analyzed, the corresponding value in the dictionary will be
None.</p></li>
</ul>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
</section>
<section id="m-optional-active-learning-network-refinement-extract-outlier-frames">
<span id="active-learning"></span><h3>(M) Optional Active Learning -&gt; Network Refinement: Extract Outlier Frames<a class="headerlink" href="#m-optional-active-learning-network-refinement-extract-outlier-frames" title="Link to this heading">#</a></h3>
<p>While DeepLabCut typically generalizes well across datasets, one might want to optimize its performance in various,
perhaps unexpected, situations. For generalization to large data sets, images with insufficient labeling performance
can be extracted, manually corrected by adjusting the labels to increase the training set and iteratively improve the
feature detectors. Such an active learning framework can be used to achieve a predefined level of confidence for all
images with minimal labeling cost (discussed in Mathis et al 2018). Then, due to the large capacity of the neural network that underlies the feature detectors, one can continue training the network with these additional examples. One does not
necessarily need to correct all errors as common errors could be eliminated by relabeling a few examples and then
re-training. A priori, given that there is no ground truth data for analyzed videos, it is challenging to find putative
‚Äúoutlier frames‚Äù. However, one can use heuristics such as the continuity of body part trajectories, to identify images
where the decoder might make large errors.</p>
<p>All this can be done for a specific video by typing (see other optional inputs below):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;videofile_path&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>We provide various frame-selection methods for this purpose. In particular
the user can set:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">outlieralgorithm</span><span class="p">:</span> <span class="s2">&quot;fitting&quot;</span><span class="p">,</span> <span class="s2">&quot;jump&quot;</span><span class="p">,</span> <span class="ow">or</span> <span class="s2">&quot;uncertain&quot;</span>
</pre></div>
</div>
<p>‚Ä¢ <code class="docutils literal notranslate"><span class="pre">outlieralgorithm=&quot;uncertain&quot;</span></code>: select frames if the likelihood of a particular or all body parts lies below <code class="docutils literal notranslate"><span class="pre">p_bound</span></code>
(note this could also be due to occlusions rather than errors).</p>
<p>‚Ä¢ <code class="docutils literal notranslate"><span class="pre">outlieralgorithm=&quot;jump&quot;</span></code>: select frames where a particular body part or all body parts jumped more than <code class="docutils literal notranslate"><span class="pre">epsilon</span></code>
pixels from the last frame.</p>
<p>‚Ä¢ <code class="docutils literal notranslate"><span class="pre">outlieralgorithm=&quot;fitting&quot;</span></code>: select frames if the predicted body part location deviates from a state-space model fit
to the time series of individual body parts. Specifically, this method fits an Auto Regressive Integrated Moving Average
(ARIMA) model to the time series for each body part. Thereby each body part detection with a likelihood smaller than
<code class="docutils literal notranslate"><span class="pre">p_bound</span></code> is treated as missing data.  Putative outlier frames are then identified as time points, where the average
body part estimates are at least <code class="docutils literal notranslate"><span class="pre">epsilon</span></code> pixels away from the fits. The parameters of this method are <code class="docutils literal notranslate"><span class="pre">epsilon</span></code>,
<code class="docutils literal notranslate"><span class="pre">p_bound</span></code>, the ARIMA parameters as well as the list of body parts to average over (can also be <code class="docutils literal notranslate"><span class="pre">all</span></code>).</p>
<p>‚Ä¢ <code class="docutils literal notranslate"><span class="pre">outlieralgorithm=&quot;manual&quot;</span></code>: manually select outlier frames based on visual inspection from the user.</p>
<p>As an example:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span><span class="n">config_path</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;videofile_path&quot;</span><span class="p">],</span> <span class="n">outlieralgorithm</span><span class="o">=</span><span class="s2">&quot;manual&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>In general, depending on the parameters, these methods might return much more frames than the user wants to
extract (<code class="docutils literal notranslate"><span class="pre">numframes2pick</span></code>). Thus, this list is then used to select outlier frames either by randomly sampling from
this list (<code class="docutils literal notranslate"><span class="pre">extractionalgorithm=&quot;uniform&quot;</span></code>), by performing <code class="docutils literal notranslate"><span class="pre">extractionalgorithm=&quot;kmeans&quot;</span></code> clustering on the
corresponding frames.</p>
<p>In the automatic configuration, before the frame selection happens, the user is informed about the amount of frames
satisfying the criteria and asked if the selection should proceed. This step allows the user to perhaps change the
parameters of the frame-selection heuristics first (i.e. to make sure that not too many frames are qualified). The user
can run the <code class="docutils literal notranslate"><span class="pre">extract_outlier_frames</span></code> method iteratively, and (even) extract additional frames from the same video.
Once enough outlier frames are extracted the refinement GUI can be used to adjust the labels based on user feedback
(see below).</p>
</section>
<section id="id9">
<h3>API Docs<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.refine_training_dataset.outlier_frames.extract_outlier_frames">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.refine_training_dataset.outlier_frames.</span></span><span class="sig-name descname"><span class="pre">extract_outlier_frames</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videos</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">videotype</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">shuffle</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">trainingsetindex</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">outlieralgorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'jump'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">frames2use</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">comparisonbodyparts</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'all'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">epsilon</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">20</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">p_bound</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">ARdegree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">3</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">MAdegree</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">1</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">alpha</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0.01</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">extractionalgorithm</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">'kmeans'</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">automatic</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_resizewidth</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">30</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">cluster_color</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">opencv</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">savelabeled</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">copy_videos</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">destfolder</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">modelprefix</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">track_method</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">''</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.refine_training_dataset.outlier_frames.extract_outlier_frames" title="Link to this definition">#</a></dt>
<dd><p>Extracts the outlier frames.</p>
<p>Extracts the outlier frames if the predictions are not correct for a certain video
from the cropped video running from start to stop as defined in config.yaml.</p>
<p>Another crucial parameter in config.yaml is how many frames to extract
<code class="docutils literal notranslate"><span class="pre">numframes2extract</span></code>.</p>
<dl class="field-list">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl>
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>videos</strong><span class="classifier">list[str]</span></dt><dd><p>The full paths to videos for analysis or a path to the directory, where all the
videos with same extension are stored.</p>
</dd>
<dt><strong>videotype: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Checks for the extension of the video in case the input to the video is a
directory. Only videos with this extension are analyzed.
If left unspecified, videos with common extensions
(‚Äòavi‚Äô, ‚Äòmp4‚Äô, ‚Äòmov‚Äô, ‚Äòmpeg‚Äô, ‚Äòmkv‚Äô) are kept.</p>
</dd>
<dt><strong>shuffle</strong><span class="classifier">int, optional, default=1</span></dt><dd><p>The shuffle index of training dataset. The extracted frames will be stored in
the labeled-dataset for the corresponding shuffle of training dataset.</p>
</dd>
<dt><strong>trainingsetindex: int, optional, default=0</strong></dt><dd><p>Integer specifying which TrainingsetFraction to use.
Note that TrainingFraction is a list in config.yaml.</p>
</dd>
<dt><strong>outlieralgorithm: str, optional, default=‚Äùjump‚Äù.</strong></dt><dd><p>String specifying the algorithm used to detect the outliers.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'fitting'</span></code> fits an Auto Regressive Integrated Moving Average model to the
data and computes the distance to the estimated data. Larger distances than
epsilon are then potentially identified as outliers</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'jump'</span></code> identifies larger jumps than ‚Äòepsilon‚Äô in any body part</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'uncertain'</span></code> looks for frames with confidence below p_bound</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'manual'</span></code> launches a GUI from which the user can choose the frames</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'list'</span></code> looks for user to provide a list of frame numbers to use, ‚Äòframes2use‚Äô. In this case, <code class="docutils literal notranslate"><span class="pre">'extractionalgorithm'</span></code> is forced to be <code class="docutils literal notranslate"><span class="pre">'uniform.'</span></code></p></li>
</ul>
</dd>
<dt><strong>frames2use: list[str], optional, default=None</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">'outlieralgorithm'</span></code> is <code class="docutils literal notranslate"><span class="pre">'list'</span></code>, provide the list of frames here.</p>
</dd>
<dt><strong>comparisonbodyparts: list[str] or str, optional, default=‚Äùall‚Äù</strong></dt><dd><p>This selects the body parts for which the comparisons with the outliers are
carried out. If <code class="docutils literal notranslate"><span class="pre">&quot;all&quot;</span></code>, then all body parts from config.yaml are used. If a
list of strings that are a subset of the full list E.g. [‚Äòhand‚Äô,‚ÄôJoystick‚Äô] for
the demo Reaching-Mackenzie-2018-08-30/config.yaml to select only these body
parts.</p>
</dd>
<dt><strong>p_bound: float between 0 and 1, optional, default=0.01</strong></dt><dd><p>For outlieralgorithm <code class="docutils literal notranslate"><span class="pre">'uncertain'</span></code> this parameter defines the likelihood
below which a body part will be flagged as a putative outlier.</p>
</dd>
<dt><strong>epsilon: float, optional, default=20</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">'outlieralgorithm'</span></code> is <code class="docutils literal notranslate"><span class="pre">'fitting'</span></code>, this is the float bound according
to which frames are picked when the (average) body part estimate deviates from
model fit.</p>
<p>If <code class="docutils literal notranslate"><span class="pre">'outlieralgorithm'</span></code> is <code class="docutils literal notranslate"><span class="pre">'jump'</span></code>, this is the float bound specifying the
distance by which body points jump from one frame to next (Euclidean distance).</p>
</dd>
<dt><strong>ARdegree: int, optional, default=3</strong></dt><dd><p>For outlieralgorithm <code class="docutils literal notranslate"><span class="pre">'fitting'</span></code>: Autoregressive degree of ARIMA model degree.
(Note we use SARIMAX without exogeneous and seasonal part)
See <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html">https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html</a></p>
</dd>
<dt><strong>MAdegree: int, optional, default=1</strong></dt><dd><p>For outlieralgorithm <code class="docutils literal notranslate"><span class="pre">'fitting'</span></code>: MovingAvarage degree of ARIMA model degree.
(Note we use SARIMAX without exogeneous and seasonal part)
See <a class="reference external" href="https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html">https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html</a></p>
</dd>
<dt><strong>alpha: float, optional, default=0.01</strong></dt><dd><p>Significance level for detecting outliers based on confidence interval of
fitted ARIMA model. Only the distance is used however.</p>
</dd>
<dt><strong>extractionalgorithm</strong><span class="classifier">str, optional, default=‚Äùkmeans‚Äù</span></dt><dd><p>String specifying the algorithm to use for selecting the frames from the
identified putatative outlier frames. Currently, deeplabcut supports either
<code class="docutils literal notranslate"><span class="pre">kmeans</span></code> or <code class="docutils literal notranslate"><span class="pre">uniform</span></code> based selection (same logic as for extract_frames).</p>
</dd>
<dt><strong>automatic</strong><span class="classifier">bool, optional, default=False</span></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, extract outliers without being asked for user feedback.</p>
</dd>
<dt><strong>cluster_resizewidth: number, default=30</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">&quot;extractionalgorithm&quot;</span></code> is <code class="docutils literal notranslate"><span class="pre">&quot;kmeans&quot;</span></code>, one can change the width to which
the images are downsampled (aspect ratio is fixed).</p>
</dd>
<dt><strong>cluster_color: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">False</span></code>, each downsampled image is treated as a grayscale vector
(discarding color information). If <code class="docutils literal notranslate"><span class="pre">True</span></code>, then the color channels are
considered. This increases the computational complexity.</p>
</dd>
<dt><strong>opencv: bool, optional, default=True</strong></dt><dd><p>Uses openCV for loading &amp; extractiong (otherwise moviepy (legacy)).</p>
</dd>
<dt><strong>savelabeled: bool, optional, default=False</strong></dt><dd><p>If <code class="docutils literal notranslate"><span class="pre">True</span></code>, frame are saved with predicted labels in each folder.</p>
</dd>
<dt><strong>copy_videos: bool, optional, default=False</strong></dt><dd><p>If True, newly-added videos (from which outlier frames are extracted) are
copied to the project folder. By default, symbolic links are created instead.</p>
</dd>
<dt><strong>destfolder: str or None, optional, default=None</strong></dt><dd><p>Specifies the destination folder that was used for storing analysis data. If
<code class="docutils literal notranslate"><span class="pre">None</span></code>, the path of the video is used.</p>
</dd>
<dt><strong>modelprefix: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Directory containing the deeplabcut models to use when evaluating the network.
By default, the models are assumed to exist in the project folder.</p>
</dd>
<dt><strong>track_method: str, optional, default=‚Äù‚Äù</strong></dt><dd><p>Specifies the tracker used to generate the data.
Empty by default (corresponding to a single animal project).
For multiple animals, must be either ‚Äòbox‚Äô, ‚Äòskeleton‚Äô, or ‚Äòellipse‚Äô and will
be taken from the config.yaml file if none is given.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Returns<span class="colon">:</span></dt>
<dd class="field-even"><dl class="simple">
<dt>None</dt><dd></dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Extract the frames with default settings on Windows.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span>
<span class="go">        &#39;C:\myproject\reaching-task\config.yaml&#39;,</span>
<span class="go">        [&#39;C:\yourusername\rig-95\Videos\reachingvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Extract the frames with default settings on Linux/MacOS.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/video/reachinvideo1.avi&#39;],</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Extract the frames using the ‚Äúkmeans‚Äù algorithm.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/video/reachinvideo1.avi&#39;],</span>
<span class="go">        extractionalgorithm=&#39;kmeans&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
<p>Extract the frames using the ‚Äúkmeans‚Äù algorithm and <code class="docutils literal notranslate"><span class="pre">&quot;epsilon=5&quot;</span></code> pixels.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">extract_outlier_frames</span><span class="p">(</span>
<span class="go">        &#39;/analysis/project/reaching-task/config.yaml&#39;,</span>
<span class="go">        [&#39;/analysis/project/video/reachinvideo1.avi&#39;],</span>
<span class="go">        epsilon=5,</span>
<span class="go">        extractionalgorithm=&#39;kmeans&#39;,</span>
<span class="go">    )</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="n-refine-labels-augmentation-of-the-training-dataset">
<h3>(N) Refine Labels: Augmentation of the Training Dataset<a class="headerlink" href="#n-refine-labels-augmentation-of-the-training-dataset" title="Link to this heading">#</a></h3>
<p>Based on the performance of DeepLabCut, four scenarios are possible:</p>
<p>(A) Visible body part with accurate DeepLabCut prediction. These labels do not need any modifications.</p>
<p>(B) Visible body part but wrong DeepLabCut prediction. Move the label‚Äôs location to the actual position of the
body part.</p>
<p>(C) Invisible, occluded body part. Remove the predicted label by DeepLabCut with a middle click. Every predicted
label is shown, even when DeepLabCut is uncertain. This is necessary, so that the user can potentially move
the predicted label. However, to help the user to remove all invisible body parts the low-likelihood predictions
are shown as open circles (rather than disks).</p>
<p>(D) Invalid images: In the unlikely event that there are any invalid images, the user should remove such an image
and their corresponding predictions, if any. Here, the GUI will prompt the user to remove an image identified
as invalid.</p>
<p>The labels for extracted putative outlier frames can be refined by opening the GUI:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">refine_labels</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<p>This will launch a GUI where the user can refine the labels.</p>
<p>Please refer to the <a class="reference internal" href="gui/napari_GUI.html#napari-gui"><span class="std std-ref">napari-deeplabcut docs</span></a> for more information about the labelling workflow.</p>
<p>After correcting the labels for all the frames in each of the subdirectories, the users should merge the data set to
create a new dataset. In this step the iteration parameter in the config.yaml file is automatically updated.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">merge_datasets</span><span class="p">(</span><span class="n">config_path</span><span class="p">)</span>
</pre></div>
</div>
<p>Once the dataset is merged, the user can test if the merging process was successful by plotting all the labels (Step E).
Next, with this expanded training set the user can now create a novel training set and train the network as described
in Steps F and G. The training dataset will be stored in the same place as before but under a different <code class="docutils literal notranslate"><span class="pre">iteration-#</span></code>
subdirectory, where the <code class="docutils literal notranslate"><span class="pre">#</span></code> is the new value of <code class="docutils literal notranslate"><span class="pre">iteration</span></code> variable stored in the project‚Äôs configuration file
(this is automatically done).</p>
<p>Now you can run <code class="docutils literal notranslate"><span class="pre">create_training_dataset</span></code>, then <code class="docutils literal notranslate"><span class="pre">train_network</span></code>, etc. If your original labels were adjusted at all,
start from fresh weights (the typically recommended path anyhow), otherwise consider using your already trained network
weights (see Box 2).</p>
<p>If after training the network generalizes well to the data, proceed to analyze new videos. Otherwise, consider labeling
more data.</p>
</section>
<section id="api-docs-for-deeplabcut-refine-labels">
<h3>API Docs for deeplabcut.refine_labels<a class="headerlink" href="#api-docs-for-deeplabcut-refine-labels" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.gui.tabs.label_frames.refine_labels">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.gui.tabs.label_frames.</span></span><span class="sig-name descname"><span class="pre">refine_labels</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config_path</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">Path</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">image_folder</span></span><span class="p"><span class="pre">:</span></span><span class="w"> </span><span class="n"><span class="pre">str</span><span class="w"> </span><span class="p"><span class="pre">|</span></span><span class="w"> </span><span class="pre">None</span></span><span class="w"> </span><span class="o"><span class="pre">=</span></span><span class="w"> </span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.gui.tabs.label_frames.refine_labels" title="Link to this definition">#</a></dt>
<dd><p>Launches the napari-deeplabcut labelling GUI.</p>
<dl class="simple">
<dt>For more information on labelling data with napari-deeplabcut, see our docs:</dt><dd><p><a class="github reference external" href="https://github.com/DeepLabCut/napari-deeplabcut?tab=readme-ov-file#usage">DeepLabCut/napari-deeplabcut</a></p>
</dd>
</dl>
<p>If no parameters are given, the napari-deeplabcut labelling GUI is simply open,
and the folder containing the images to label can be dropped into the GUI.</p>
<p>If the <cite>config_path</cite> and the <cite>image_folder</cite> are given as arguments, the given
<cite>image_folder</cite> for the project is opened in the napari-deeplabcut GUI to be labeled.
If only the <cite>config_path</cite> is given, the first image folder is opened.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config_path: str, Path, None</strong></dt><dd><p>Full path of the project config.yaml file.</p>
</dd>
<dt><strong>image_folder: str, None</strong></dt><dd><p>Name of the image folder to open for labelling.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<p>Opening the napari-deeplabcut annotation GUI without opening a specific folder of
images to label. You then need to drag-and-drop your image folder into the GUI.
See the napari-deeplabcut docs linked above for more information about labelling in
napari-deeplabcut.
&gt;&gt;&gt; import deeplabcut
&gt;&gt;&gt; deeplabcut.label_frames()</p>
<p>Opening the images extracted from the ‚Äú2025-01-01-experiment7‚Äù video in
napari-deeplabcut on Windows. The project‚Äôs folder structure should look as follows:
reaching-task/                    # project root directory
‚îú‚îÄ‚îÄ config.yaml                   # project configuration file
‚îî‚îÄ‚îÄ labeled-data/                 # folder containing all extracted image folders</p>
<blockquote>
<div><p>‚îú‚îÄ‚îÄ ‚Ä¶
‚îú‚îÄ‚îÄ 2025-01-01-experiment7    # folder containing the images to label
‚îî‚îÄ‚îÄ ‚Ä¶</p>
</div></blockquote>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">label_frames</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;C:\myproject</span><span class="se">\r</span><span class="s2">eaching-task\config.yaml&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="s2">&quot;2025-01-01-experiment7&quot;</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
</pre></div>
</div>
<p>Opening the images extracted from the first video listed in the project
configuration in napari-deeplabcut on a Unix system.
&gt;&gt;&gt; deeplabcut.label_frames(‚Äú/users/john/project/config.yaml‚Äù)</p>
</dd></dl>

</div>
</section>
<section id="api-docs-for-deeplabcut-merge-datasets">
<h3>API Docs for deeplabcut.merge_datasets<a class="headerlink" href="#api-docs-for-deeplabcut-merge-datasets" title="Link to this heading">#</a></h3>
<div class="dropdown admonition">
<p class="admonition-title">Click the button to see API Docs</p>
<dl class="py function">
<dt class="sig sig-object py" id="deeplabcut.refine_training_dataset.outlier_frames.merge_datasets">
<span class="sig-prename descclassname"><span class="pre">deeplabcut.refine_training_dataset.outlier_frames.</span></span><span class="sig-name descname"><span class="pre">merge_datasets</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">config</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">forceiterate</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="headerlink" href="#deeplabcut.refine_training_dataset.outlier_frames.merge_datasets" title="Link to this definition">#</a></dt>
<dd><p>Merge the original training dataset with the newly refined data.</p>
<p>Checks if the original training dataset can be merged with the newly refined
training dataset. To do so it will check if the frames in all extracted video sets
were relabeled.</p>
<p>If this is the case then the <code class="docutils literal notranslate"><span class="pre">&quot;iteration&quot;</span></code> variable is advanced by 1.</p>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>config: str</strong></dt><dd><p>Full path of the config.yaml file.</p>
</dd>
<dt><strong>forceiterate: int or None, optional, default=None</strong></dt><dd><p>If an integer is given the iteration variable is set to this value
This is only done if all datasets were labeled or refined.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Examples</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">merge_datasets</span><span class="p">(</span><span class="s1">&#39;/analysis/project/reaching-task/config.yaml&#39;</span><span class="p">)</span>
</pre></div>
</div>
</dd></dl>

</div>
</section>
<section id="jupyter-notebooks-for-demonstration-of-the-deeplabcut-workflow">
<h3>Jupyter Notebooks for Demonstration of the DeepLabCut Workflow<a class="headerlink" href="#jupyter-notebooks-for-demonstration-of-the-deeplabcut-workflow" title="Link to this heading">#</a></h3>
<p>We also provide two Jupyter notebooks for using DeepLabCut on both a pre-labeled dataset, and on the end user‚Äôs
own dataset. Firstly, we prepared an interactive Jupyter notebook called
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/blob/main/examples/JUPYTER/Demo_yourowndata.ipynb">Demo_yourowndata.ipynb</a>
that can serve as a template for the user to develop a project. Furthermore, we provide a notebook for an already
started project with labeled data. The example project, named as
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/tree/main/examples/Reaching-Mackenzie-2018-08-30">Reaching-Mackenzie-2018-08-30</a>
consists of a project configuration file with default parameters and 20 images, which are cropped around the region of
interest as an example dataset. These images are extracted from a video, which was recorded in a study of skilled motor
control in mice. Some example labels for these images are also provided. See more details
<a class="reference external" href="https://github.com/DeepLabCut/DeepLabCut/tree/main/examples">here</a>.</p>
</section>
</section>
<section id="d-toolbox">
<h2>3D Toolbox<a class="headerlink" href="#d-toolbox" title="Link to this heading">#</a></h2>
<p>Please see <a class="reference internal" href="Overviewof3D.html#d-overview"><span class="std std-ref">3D overview</span></a> for information on using the 3D toolbox of
DeepLabCut (as of 2.0.7+).</p>
</section>
<section id="other-functions-some-are-yet-to-be-documented">
<h2>Other functions, some are yet-to-be-documented:<a class="headerlink" href="#other-functions-some-are-yet-to-be-documented" title="Link to this heading">#</a></h2>
<p>We suggest you <a class="reference internal" href="HelperFunctions.html#helper-functions"><span class="std std-ref">check out these additional helper functions</span></a>, that
could be useful (they are all optional).</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="docker.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">DeepLabCut Docker containers</p>
      </div>
    </a>
    <a class="right-next"
       href="maDLC_UserGuide.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">DeepLabCut for Multi-Animal Projects</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut-project-manager-gui-recommended-for-beginners">DeepLabCut Project Manager GUI (recommended for beginners)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut-in-the-terminal-command-line-interface">DeepLabCut in the Terminal/Command line interface:</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#a-create-a-new-project">(A) Create a New Project</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.create_project.new.create_new_project"><code class="docutils literal notranslate"><span class="pre">create_new_project()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#b-configure-the-project">(B) Configure the Project</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#c-select-frames-to-label">(C) Select Frames to Label</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.frame_extraction.extract_frames"><code class="docutils literal notranslate"><span class="pre">extract_frames()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#d-label-frames">(D) Label Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#e-check-annotated-frames">(E) Check Annotated Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.check_labels"><code class="docutils literal notranslate"><span class="pre">check_labels()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#f-create-training-dataset">(F) Create Training Dataset</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset"><code class="docutils literal notranslate"><span class="pre">create_training_dataset()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_model_comparison"><code class="docutils literal notranslate"><span class="pre">create_training_model_comparison()</span></code></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.generate_training_dataset.trainingsetmanipulation.create_training_dataset_from_existing_split"><code class="docutils literal notranslate"><span class="pre">create_training_dataset_from_existing_split()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#g-train-the-network">(G) Train The Network</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.train_network"><code class="docutils literal notranslate"><span class="pre">train_network()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#h-evaluate-the-trained-network">(H) Evaluate the Trained Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.evaluate_network"><code class="docutils literal notranslate"><span class="pre">evaluate_network()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#i-analyze-new-videos">(I) Analyze new Videos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.compat.analyze_videos"><code class="docutils literal notranslate"><span class="pre">analyze_videos()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#novel-video-analysis-extra-features">Novel Video Analysis: extra features</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#dynamic-cropping-of-videos">Dynamic-cropping of videos:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#j-filter-pose-data">(J) Filter Pose Data</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.post_processing.filtering.filterpredictions"><code class="docutils literal notranslate"><span class="pre">filterpredictions()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#k-plot-trajectories">(K) Plot Trajectories</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.utils.plotting.plot_trajectories"><code class="docutils literal notranslate"><span class="pre">plot_trajectories()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#l-create-labeled-videos">(L) Create Labeled Videos</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.utils.make_labeled_video.create_labeled_video"><code class="docutils literal notranslate"><span class="pre">create_labeled_video()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extract-skeleton-features">Extract ‚ÄúSkeleton‚Äù Features:</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.post_processing.analyze_skeleton.analyzeskeleton"><code class="docutils literal notranslate"><span class="pre">analyzeskeleton()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#m-optional-active-learning-network-refinement-extract-outlier-frames">(M) Optional Active Learning -&gt; Network Refinement: Extract Outlier Frames</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">API Docs</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.refine_training_dataset.outlier_frames.extract_outlier_frames"><code class="docutils literal notranslate"><span class="pre">extract_outlier_frames()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#n-refine-labels-augmentation-of-the-training-dataset">(N) Refine Labels: Augmentation of the Training Dataset</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs-for-deeplabcut-refine-labels">API Docs for deeplabcut.refine_labels</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.gui.tabs.label_frames.refine_labels"><code class="docutils literal notranslate"><span class="pre">refine_labels()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#api-docs-for-deeplabcut-merge-datasets">API Docs for deeplabcut.merge_datasets</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#deeplabcut.refine_training_dataset.outlier_frames.merge_datasets"><code class="docutils literal notranslate"><span class="pre">merge_datasets()</span></code></a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#jupyter-notebooks-for-demonstration-of-the-deeplabcut-workflow">Jupyter Notebooks for Demonstration of the DeepLabCut Workflow</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#d-toolbox">3D Toolbox</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#other-functions-some-are-yet-to-be-documented">Other functions, some are yet-to-be-documented:</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The DeepLabCut Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      ¬© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
<div class="extra_footer">
  <div>Powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</div>

</div>
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>