{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(fmpose3d-recipe)=\n",
        "# 3D Pose Estimation with FMPose3D\n",
        "\n",
        "## Overview\n",
        "**[FMPose3D: monocular 3D pose estimation via flow matching](https://arxiv.org/abs/2602.05755)** by Ti Wang, Xiaohang Yu, and Mackenzie Weygandt Mathis.\n",
        "\n",
        "| [Paper](https://arxiv.org/abs/2602.05755) | [Project Page](https://xiu-cs.github.io/FMPose3D/) | [GitHub](https://github.com/AdaptiveMotorControlLab/FMPose3D) | [PyPI](https://pypi.org/project/fmpose3d/) |\n",
        "\n",
        "FMPose3D lifts 2D keypoints from a single image into 3D poses using **flow matching** — a generative technique based on ODE sampling. It generates multiple plausible 3D pose hypotheses in just a few steps, then aggregates them using a reprojection-based Bayesian module (RPEA) for accurate predictions, achieving state-of-the-art results on human and animal 3D pose benchmarks.\n",
        "\n",
        "\n",
        "This recipe shows how to use FMPose3D in DeepLabCut for monocular 3D pose\n",
        "estimation. Two pipelines are available:\n",
        "\n",
        "| Pipeline | 2D Estimator | Skeleton | Joints |\n",
        "|----------|-------------|----------|--------|\n",
        "| **Human** | HRNet + YOLO | H36M | 17 |\n",
        "| **Animal** | DeepLabCut SuperAnimal | Animal3D | 26 |\n",
        "\n",
        "Model weights are hosted on HuggingFace Hub and downloaded automatically on\n",
        "first use.\n",
        "\n",
        "```{admonition} Prerequisites\n",
        ":class: note\n",
        "\n",
        "Install the `fmpose3d` package before running this notebook:\n",
        "\n",
        "    pip install fmpose3d\n",
        "\n",
        "A GPU is recommended but not required — CPU inference works out of the box.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "Import the DeepLabCut convenience wrapper and a few helpers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "from deeplabcut.modelzoo.fmpose_3d.fmpose3d import get_fmpose3d_inference_api"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Human Pose Estimation (end-to-end)\n",
        "\n",
        "The simplest way to get 3D human poses is the **end-to-end** pipeline.\n",
        "`get_fmpose3d_inference_api` creates an inference object that handles\n",
        "2D detection and 3D lifting in a single `predict` call. Weights are\n",
        "downloaded automatically from HuggingFace on first use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create the human pose API (downloads weights on first call)\n",
        "human_api = get_fmpose3d_inference_api(\n",
        "    model_type=\"fmpose3d_humans\",\n",
        "    device=\"cuda:0\",  # use \"cpu\" if no GPU is available\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Run end-to-end inference on an image\n",
        "image_path = \"path/to/your/image.jpg\"  # replace with your image path\n",
        "result = human_api.predict(source=image_path)\n",
        "\n",
        "print(\"3D poses (root-relative):\", result.poses_3d.shape)        # (num_frames, 17, 3)\n",
        "print(\"3D poses (world coords):\", result.poses_3d_world.shape)   # (num_frames, 17, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accepted input sources\n",
        "\n",
        "`predict` (and `prepare_2d`) accept a variety of input types:\n",
        "\n",
        "- A **file path** (`str` or `Path`) to a single image\n",
        "- A **directory** of images\n",
        "- A **numpy array** — either a single frame `(H, W, C)` or a batch `(N, H, W, C)`\n",
        "- A **list** of any of the above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Animal Pose Estimation (end-to-end)\n",
        "\n",
        "Switching to the **animal** pipeline only requires changing `model_type`.\n",
        "This pipeline uses DeepLabCut SuperAnimal for 2D detection and outputs\n",
        "26-joint Animal3D skeletons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Create the animal pose API\n",
        "animal_api = get_fmpose3d_inference_api(\n",
        "    model_type=\"fmpose3d_animals\",\n",
        "    device=\"cuda:0\",\n",
        ")\n",
        "\n",
        "# Run inference\n",
        "animal_image_path = \"path/to/your/animal_image.jpg\"\n",
        "animal_result = animal_api.predict(source=animal_image_path)\n",
        "\n",
        "print(\"3D poses:\", animal_result.poses_3d.shape)              # (num_frames, 26, 3)\n",
        "print(\"3D poses (regularized):\", animal_result.poses_3d_world.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{note}\n",
        "For animals, `poses_3d_world` contains **limb-regularized** poses (the\n",
        "skeleton is rotated so that the average limb direction is vertical) rather\n",
        "than a camera-to-world transform.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Two-Step Inference (2D then 3D)\n",
        "\n",
        "For more control, you can run the 2D and 3D stages separately. This is\n",
        "useful when you want to inspect or modify 2D keypoints before lifting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "api = get_fmpose3d_inference_api(model_type=\"fmpose3d_animals\", device=\"cuda:0\")\n",
        "\n",
        "# Step 1: detect 2D keypoints\n",
        "result_2d = api.prepare_2d(source=animal_image_path)\n",
        "\n",
        "print(\"2D keypoints:\", result_2d.keypoints.shape)   # (num_persons, num_frames, J, 2)\n",
        "print(\"Confidence scores:\", result_2d.scores.shape)  # (num_persons, num_frames, J)\n",
        "print(\"Image size (H, W):\", result_2d.image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "# Step 2: lift 2D keypoints to 3D\n",
        "result_3d = api.pose_3d(\n",
        "    keypoints_2d=result_2d.keypoints,\n",
        "    image_size=result_2d.image_size,\n",
        ")\n",
        "\n",
        "print(\"Lifted 3D poses:\", result_3d.poses_3d.shape)  # (num_frames, J, 3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Lifting DeepLabCut 2D Predictions to 3D\n",
        "\n",
        "A common workflow is to use a DeepLabCut model you have already trained for\n",
        "2D pose estimation, then lift those predictions to 3D with FMPose3D. The\n",
        "example below runs DLC inference with `deeplabcut.analyze_images` and feeds\n",
        "the resulting keypoints straight into the 3D lifter.\n",
        "\n",
        "```{admonition} Keypoint compatibility\n",
        ":class: warning\n",
        "\n",
        "The FMPose3D lifter was trained on specific skeleton layouts (17 H36M joints\n",
        "for humans, 26 Animal3D joints for animals). Your DLC model's bodyparts must\n",
        "match one of these layouts for the lifted poses to be meaningful. If your\n",
        "skeleton differs, you will need to select or re-order the relevant subset of\n",
        "keypoints before calling `pose_3d`.\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": []
      },
      "outputs": [],
      "source": [
        "import deeplabcut\n",
        "\n",
        "# ── 1. Run DLC 2D inference ───────────────────────────────────────────────\n",
        "# analyze_images returns a dict mapping each image path to its predictions.\n",
        "# Each prediction contains a \"bodyparts\" array of shape\n",
        "# (num_individuals, num_bodyparts, 3) where 3 = (x, y, likelihood).\n",
        "\n",
        "config_path = \"path/to/my_dlc_project/config.yaml\"\n",
        "image_paths = [\"frame_001.png\", \"frame_002.png\", \"frame_003.png\"]\n",
        "\n",
        "predictions = deeplabcut.analyze_images(\n",
        "    config=config_path,\n",
        "    images=image_paths,\n",
        "    shuffle=1,\n",
        "    device=\"cuda:0\",\n",
        ")\n",
        "\n",
        "# ── 2. Extract (x, y) keypoints from each frame ──────────────────────────\n",
        "# Stack all frames into a single array and take only the first individual.\n",
        "all_bodyparts = np.stack([\n",
        "    predictions[img][\"bodyparts\"][0]  # first individual per frame\n",
        "    for img in image_paths\n",
        "])  # shape: (num_frames, num_bodyparts, 3)\n",
        "\n",
        "keypoints_2d = all_bodyparts[:, :, :2]  # drop likelihood → (num_frames, J, 2)\n",
        "print(\"keypoints_2d shape:\", keypoints_2d.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ── 3. Lift DLC 2D keypoints to 3D ────────────────────────────────────────\n",
        "# image_size = (height, width) of the frames the DLC model was run on.\n",
        "import cv2\n",
        "\n",
        "sample_img = cv2.imread(image_paths[0])\n",
        "image_size = sample_img.shape[:2]  # (height, width)\n",
        "\n",
        "api = get_fmpose3d_inference_api(model_type=\"fmpose3d_animals\", device=\"cuda:0\")\n",
        "result_3d = api.pose_3d(\n",
        "    keypoints_2d=keypoints_2d,\n",
        "    image_size=image_size,\n",
        "    seed=42,  # for reproducible sampling\n",
        ")\n",
        "\n",
        "print(\"3D poses (root-relative):\", result_3d.poses_3d.shape)       # (num_frames, J, 3)\n",
        "print(\"3D poses (post-processed):\", result_3d.poses_3d_world.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{tip}\n",
        "If you are working with video frames from `deeplabcut.analyze_videos`\n",
        "instead of individual images, you can read `image_size` from the video:\n",
        "\n",
        "    import cv2\n",
        "    cap = cv2.VideoCapture(\"path/to/video.mp4\")\n",
        "    image_size = (int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),\n",
        "                  int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)))\n",
        "    cap.release()\n",
        "\n",
        "You will also need to load the keypoints from the `.h5` file that\n",
        "`analyze_videos` produces:\n",
        "\n",
        "    import pandas as pd\n",
        "    df = pd.read_hdf(\"path/to/videoDLC_scorer.h5\")\n",
        "    scorer = df.columns.get_level_values(\"scorer\").unique()[0]\n",
        "    bodyparts = df[scorer].columns.get_level_values(\"bodyparts\").unique()\n",
        "    coords = df[scorer].values.reshape(len(df), len(bodyparts), 3)\n",
        "    keypoints_2d = coords[:, :, :2]  # (num_frames, num_bodyparts, 2)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Further Reading\n",
        "\n",
        "- [FMPose3D repository](https://github.com/AdaptiveMotorControlLab/FMPose3D)\n",
        "  — full API documentation and model details.\n",
        "- [DeepLabCut Model Zoo](https://deeplabcut.github.io/DeepLabCut/docs/ModelZoo.html)\n",
        "  — other pre-trained models available in DeepLabCut."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
