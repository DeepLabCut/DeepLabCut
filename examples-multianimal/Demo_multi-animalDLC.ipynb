{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0gDJMy1ywm8"
   },
   "source": [
    "# DeepLabCut Toolbox - DEMO for maDLC\n",
    "https://github.com/AlexEMG/DeepLabCut\n",
    "\n",
    "#### The notebook accompanies the following preprint:\n",
    "\n",
    "*Multi-animal pose estimation and tracking with DeepLabCut.* A. Mathis et al, 2020 in prep.\n",
    "\n",
    "This notebook illustrates how to:\n",
    "\n",
    "- create a multi-animal project\n",
    "- label the data \n",
    "- check the labels \n",
    "- create a multi-animal training dataset\n",
    "- train your new deep neural network \n",
    "- evaluate network\n",
    "- analyze videos\n",
    "- detect tracks \n",
    "- inspect and correct tracks \n",
    "- create videos \n",
    "- create images of scoremaps, pafs, locref output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You must have deeplabcut 2.2 installed (upgrade here if needed):\n",
    "- this will NOT break any of your older 2.x projects!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade deeplabcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/mackenzie/DEEPLABCUT/maDLC/DLCdev\n"
     ]
    }
   ],
   "source": [
    "%cd '/home/mackenzie/DEEPLABCUT/maDLC/DLCdev'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the toolbox:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTtJxcQ7ywnB",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/mackenzie/anaconda3/envs/DLC3_7/lib/python3.7/site-packages/deeplabcut/__init__.py'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!export DLClight=False\n",
    "import deeplabcut\n",
    "deeplabcut.__file__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a new project [if you are using our DEMO data, don't run this]\n",
    "\n",
    "It is always good idea to keep the projects seperate if you want to use different networks to analze your data. You should use one project if you are tracking similar subjects/items even if in different environments. This function creates a new project with sub-directories and a basic configuration file in the user defined directory otherwise the project is created in the current working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task='mysupercoolexpts' # Enter the name of your experiment Task\n",
    "experimenter='teamDLC' # Enter the name of the experimenter\n",
    "video=['/home/yourname/Desktop/yourvideohere.mp4'] # Enter the paths of your videos OR FOLDER you want to grab frames from.\n",
    "#CRUCIAL: you should take as diverse frames as possible from multiple videos and animals!\n",
    "\n",
    "deeplabcut.create_new_project(task,experimenter,video,copy_videos=True, multianimal=True) \n",
    "\n",
    "# NOTE: The function returns the path, where your project is. \n",
    "# You could also enter this manually (e.g. if the project is already created and you want to pick up, where you stopped...)\n",
    "#path_config_file = '/home/Mackenzie/myDLCProjectFolders/config.yaml' # Enter the path of the config file that was just created from the above step (check the folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You also can re-load a project by just updating the path to the config.yaml below:\n",
    "\n",
    "- DEMO users: be sure to have the DEMO Project Folder inside the folder, and please edit the \"Project Path\" inside the config.yaml to be the location of the folder. As a reminder, you can always move the  project folder, just need to edit this 1 line of code. \n",
    "\n",
    "### [If you are using our DEMO data, run this!]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to config.yaml file (printed above!)\n",
    "path_config_file = '/home/ENTERYOURPATH/MutliMouse-Daniel-2019-12-16/config.yaml'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CRUCIAL: go edit the config.yaml file that was created! \n",
    "### If you are using this Notebook to create your own project! \n",
    "\n",
    "You will have a config.yaml file in your project folder. \n",
    "Crucially, please set:\n",
    "\n",
    "- **individuals**\n",
    "        #set the number of animals, i.e. \"indiv1, indiv2, indiv3\" up to the maximum you expect to see in the\n",
    "         training data. if you have video with morelater, that is fine.\n",
    "- **uniquebodyparts** \n",
    "        #this are points that are not tied to an individual, i.e. something like \"cornerofabox\" or \"LED\"\n",
    "- **multianimalbodyparts** \n",
    "        #this is the real bodypart per individual list (as in \"normal\" DLC). Please use more body parts than \n",
    "         you might have been used to previously. Now that a skeleton is used for training you want to be able\n",
    "         to  connect close parts. i.e. in our mouse demo we had snout, leftear, right ear, tailbase; now you\n",
    "         should also include \"neck\" & \"spine\" points i.e. perhaps spine1, spine2, spine3\n",
    "- **skeleton** \n",
    "         #this is now used during training, so please connect the bodyparts in a graph. \n",
    "         CRUCIAL: You also want to over-connect; i.e. let's say you have head, body, left limb, right limb:\n",
    "         - - head\n",
    "           - body\n",
    "         - - body \n",
    "           - leftlimb\n",
    "         - - body \n",
    "           - rightlimb\n",
    "         - - head \n",
    "           - leftlimb\n",
    "         - - head \n",
    "           - rightlimb\n",
    "         - - leftlimb\n",
    "           - rightlimb\n",
    "           \n",
    "**Plus**, edit the number of frames to extract per video, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract frames from videos \n",
    "A key point for a successful feature detector is to select diverse frames, which are typical for the behavior you study that should be labeled.\n",
    "\n",
    "This function selects N frames either uniformly sampled from a particular video (or folder) ('uniform'). Note: this might not yield diverse frames, if the behavior is sparsely distributed (consider using kmeans), and/or select frames manually etc.\n",
    "\n",
    "Also make sure to get select data from different (behavioral) sessions and different animals if those vary substantially (to train an invariant feature detector).\n",
    "\n",
    "Individual images should not be too big (i.e. < 850 x 850 pixel). Although this can be taken care of later as well, it is advisable to crop the frames, to remove unnecessary parts of the frame as much as possible.\n",
    "\n",
    "Always check the output of cropping. If you are happy with the results proceed to labeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOMATIC:\n",
    "deeplabcut.extract_frames(path_config_file) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.label_frames(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Labels, by bodypart:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(path_config_file, visualizeindividuals=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Labels, by individual:\n",
    "\n",
    "You can run this function \"again\" with the flag set to true and it saves both sets of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROlflqQLywnP"
   },
   "outputs": [],
   "source": [
    "deeplabcut.check_labels(path_config_file, visualizeindividuals=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here we have an extra (optional but recommended) step to crop and diversify the images before creating the training set: \n",
    "\n",
    "This is recommended to be able to train in batches, and especially helpful if you have lots of animals. It's worth the few minutes... plus you can then use batch training if your images were not the same size already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.cropimagesandlabels(path_config_file, userfeedback=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create maDLC training dataset:\n",
    "\n",
    "Here you have several options (as always), and some new defaults! Defaults: firstly, this will use ResNet-50 with imgaug and  secondly, new ADAM optimizer with batch training (if your frames are the same size*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.create_multianimaltraining_dataset(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [optional]: Here is how you can edit the pose_config.yaml before training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle=1\n",
    "\n",
    "trainposeconfigfile,testposeconfigfile,snapshotfolder=deeplabcut.return_train_network_path(path_config_file,shuffle=shuffle)\n",
    "edits = {'batch_size': 16,\n",
    "         'optimizer': 'adam',\n",
    "         'grayscale': True,\n",
    "         'rotation': 20,\n",
    "         'covering': True,\n",
    "         'hist_eq': True,\n",
    "         'fliplr': False,\n",
    "         'motion_blur': True,\n",
    "         'multi_step': [[1e-4, 5000], [5*1e-5, 7500], [1e-5, 12000], [1e-6,50000]],\n",
    "         'weigh_only_present_joints': False}\n",
    "cfg_dlc= deeplabcut.auxiliaryfunctions.edit_config(testposeconfigfile, edits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h9H7eqDLywnV"
   },
   "source": [
    "## Start training the multi-task DNN + Feature Detectors:\n",
    "This function trains the network for a specific shuffle of the training dataset. \n",
    "\n",
    "Training can be stopped at any time w/CNTRL+C. Note that the weights are only stored every 'saveiters' steps. Inn reality, you will train until ~50K, so we save every 5-10K). But, you can store more frequently or display more fequently if you are DEMOing the code. Note, this is a *NEW* training regime, so if you are used to DLC 2.1 (and before), this now takes less time to train.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jg96O2acywnW",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#which shuffle do you want to train/evaluate, and analyze with?\n",
    "shuffle = 1 \n",
    "\n",
    "deeplabcut.train_network(path_config_file, shuffle=shuffle, saveiters=1000, displayiters=100, maxiters=30000)\n",
    "#notice the variables \"saveiters\" and \"dsiplayiters\" that can be set in the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CCzxVT_gywnc"
   },
   "source": [
    "## Evaluate the trained network\n",
    "\n",
    "This function evaluates a trained model for a specific shuffle/shuffles at a particular training state (snapshot) or on all the states. The network is evaluated on the data set (images) and stores the results as .csv file in a subdirectory under **evaluation-results**.\n",
    "\n",
    "You can change various parameters in the ```config.yaml``` file of this project. For the evaluation one can change pcutoff. This cutoff also influences how likely estimated postions need to be so that they are shown in the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kuprPKDdywne",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(path_config_file,Shuffles=[shuffle], plotting=True,c_engine=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions visually before moving onwards:\n",
    "\n",
    "`extract_save_all_maps` allows you to plot and save the scoremaps, lofref, and pafs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabcut.extract_save_all_maps(path_config_file, shuffle=shuffle, Indices=[0, 5])\n",
    "# you can drop \"Indices\" to run this on all training/testing images \n",
    "#This creates a new folder called \"maps\" within your evalution-results folder (shuffle specific). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XeqYWGaXywnj"
   },
   "source": [
    "## Analyzing videos\n",
    "\n",
    "- [1] we will analyze a new video \n",
    "- [2] utilize a new function that plots *all* detections, just to check quality (again, i.e. in addition to the network evaluation ). \n",
    "\n",
    "\n",
    "- `deeplabcut.analyze_videos`: This function extracts the pose based on a trained network from videos. The user can choose the trained network - by default the most recent snapshot is used to analyse the videos. However, the user can also specify the snapshot index for the variable **snapshotindex** in the **config.yaml** file).\n",
    "\n",
    "     - In maDLC the under-the-hood code is quite updated, and also now has c-code that would need to be comiled first. There is a non-c-code option though! set `c_engine=False`. It's a bit slower, but the outcome is the same. \n",
    "\n",
    "     - The results are stored in `pickle` file in the same directory, where the video resides. Notice, we will create the standard h5 file later in the process to remind you to check tracking first! \n",
    "\n",
    "\n",
    "- `create_video_with_all_detections` allows you to create a video of all scoremap detections, plotted a points (this is not the final product!) We recommend running this on a short video (perhaps one that you used for labeling). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vv9iHHLlywnl"
   },
   "outputs": [],
   "source": [
    "# Set a video path:\n",
    "#The video can be the one you trained with and new videos that look similar, i.e. same experiments, etc.\n",
    "# TIP: You can add individual videos, OR a path to a folder - it will skip videos that are already analyzed once.\n",
    "\n",
    "videofile_path = '/home/mackenzie/Desktop/video.mp4'      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CFbPPD4hywnq",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#NOTE: c_engine = True is faster, but requires you complie c code first.  *see github wiki for tips!\n",
    "#### This is just for c-engine=True ###\n",
    "import os, sys\n",
    "nmspath = 'deeplabcut/pose_estimation_tensorflow/lib/nms_cython'\n",
    "sys.path.append(os.path.join('/home/mackenzie/anaconda3/envs/DLC3_7/lib/python3.7/site-packages',nmspath))\n",
    "#######################################\n",
    "\n",
    "print(\"Start Analyzing my video(s)!\")\n",
    "scorername = deeplabcut.analyze_videos(path_config_file,[videofile_path], \n",
    "                                       shuffle=shuffle, videotype='.mp4', c_engine=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You input the config path, video path, and then the scorer name:\n",
    "#(note, we extracted the `scorername` now from `analyze_videos` or you can pass it here:)\n",
    "\n",
    "#scorername = 'DLC_resnet50_mwmMar31shuffle3_30000'\n",
    "\n",
    "# There are two pose_config.yaml inputs you might want to change (nmsradius, and minconfidence). \n",
    "# Here is a simple way to do that within the notebook:\n",
    "\n",
    "trainposeconfigfile,testposeconfigfile,snapshotfolder=deeplabcut.return_train_network_path(path_config_file,shuffle=2)\n",
    "cfg_dlc=deeplabcut.auxiliaryfunctions.read_plainconfig(testposeconfigfile)\n",
    "cfg_dlc['nmsradius']=5.\n",
    "cfg_dlc['minconfidence']=.01\n",
    "deeplabcut.auxiliaryfunctions.write_plainconfig(testposeconfigfile,cfg_dlc)\n",
    "\n",
    "deeplabcut.create_video_with_all_detections(path_config_file, [videofile_path], scorername)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the Multi-Animal Tracklets! \n",
    "\n",
    "Please note, this has quite a few parameters, and they need to be cross-validated for best performance. \n",
    "\n",
    "Thus, for eah project we strongly suggest you take the time to cross-validate. \n",
    "\n",
    "## WIP : Here is how... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.convert_detections2tracklets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This function has many parameters, and it's worth it to check out the docstring! ^"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assemble tracklets:\n",
    "deeplabcut.convert_detections2tracklets(path_config_file, [videofile_path], shuffle=shuffle, videotype='mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you should manually verfiy the tracks and correct them if needed! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.refine_tracklets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# YOUL NEED TO SET TEH PICKLE FILE PATH TO THE TRACKS.PICKLE:\n",
    "picklefile ='/home/mackenzie/Desktop/videoDLC_resnet50_MultiMouseMar31shuffle1_30000tracks.pickle'\n",
    "\n",
    "deeplabcut.refine_tracklets(path_config_file, picklefile, videofile_path, min_swap_frac=0,min_tracklet_frac=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ3T3oykywnw"
   },
   "source": [
    "## Create labeled videos\n",
    "\n",
    "This function is for the visualization purpose and can be used to create a video in .mp4 format with the predicted labels. This video is saved in the same directory, where the (unlabeled) video resides. \n",
    "\n",
    "Various parameters can be set with regard to the colormap and the dotsize (matplotlib is used in the backend). See the `config.yaml` file for how to set these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GhI9KLs4ywn0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.create_labeled_video(path_config_file,[videofile_path], shuffle=shuffle, draw_skeleton=True, videotype='mp4', save_frames=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IErvm1K5ywn5"
   },
   "source": [
    "## Plot the trajectories of the analyzed videos\n",
    "This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color. The underlying functions can easily be customized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mP2useJgywn7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "deeplabcut.plot_trajectories(path_config_file,[videofile_path],shuffle=shuffle, showfigures=True, videotype='mp4')\n",
    "#These plots can are interactive and can be customized (see https://matplotlib.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a5nOaWzXywoB"
   },
   "source": [
    "## Extract outlier frames, where the predictions are off.\n",
    "\n",
    "This is optional step allows to add more training data when the evaluation results are poor. In such a case, the user can use the following function to extract frames where the labels are incorrectly predicted. Make sure to provide the correct value of the \"iterations\" as it will be used to create the unique directory where the extracted frames will be saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note, if you have questions on parameters, remember \"?\" gives you answers:\n",
    "deeplabcut.extract_outlier_frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJGiDKuUywoC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "deeplabcut.extract_outlier_frames(path_config_file,[videofile_path], videotype='.mp4', shuffle=shuffle, outlieralgorithm='uncertain',p_bound=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jHjpscPcywoG"
   },
   "source": [
    "The user can run this iteratively, and (even) extract additional frames from the same video."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uaNUm3NSywoH"
   },
   "source": [
    "## Manually correct labels\n",
    "\n",
    "This step allows the user to correct the labels in the extracted frames. Navigate to the folder with the videos and use the GUI as described in the protocol to update the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OJDvJMcrywoI",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#GUI pops up! \n",
    "#sometimes you need to restart the kernel for the GUI to launch.\n",
    "%gui wx\n",
    "deeplabcut.refine_labels(path_config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mcuqoeRbywoL"
   },
   "outputs": [],
   "source": [
    "# Now merge datasets (once you refined all frames)\n",
    "deeplabcut.merge_datasets(path_config_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WRB9KgGsywoP"
   },
   "source": [
    "## Create a new iteration of training dataset, check it and train...\n",
    "\n",
    "Following the refine labels, when you `merge_datasets` this append these frames to the original dataset to create a new iteration of training dataset (and the `iteration` is bumped up in the `config.yaml` file! Now you can start at creating a training set again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Demo-labeledexample-MouseReaching.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:DLC3_7] *",
   "language": "python",
   "name": "conda-env-DLC3_7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
