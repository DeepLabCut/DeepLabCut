{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87e3afb1",
   "metadata": {},
   "source": [
    "# DeepLabCut Toolbox - Demo BUCTD\n",
    "\n",
    "Some resources that can be useful:\n",
    "\n",
    "- [github.com/DeepLabCut/DeepLabCut](https://github.com/DeepLabCut/DeepLabCut)\n",
    "- [DeepLabCut's Documentation: User Guide for Multi-Animal projects](https://deeplabcut.github.io/DeepLabCut/docs/maDLC_UserGuide.html)\n",
    "- [Zhou, Stoffl, Mathis, Mathis. \"Rethinking Pose Estimation in Crowds: Overcoming the Detection Information Bottleneck and Ambiguity.\" Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV). 2023](https://openaccess.thecvf.com/content/ICCV2023/papers/Zhou_Rethinking_Pose_Estimation_in_Crowds_Overcoming_the_Detection_Information_Bottleneck_ICCV_2023_paper.pdf)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is an introduction to training and using CTD models in DeepLabCut, through the [maDLC Tri-Mouse Benchmark Dataset](https://zenodo.org/records/5851157) presented Lauer et al. 2022 (Nature Methods). For more information, you can check out the [DeepLabCut Benchmark Datasets](https://benchmark.deeplabcut.org/datasets.html) for more information.\n",
    "\n",
    "In this notebook, we'll\n",
    "\n",
    "- train an bottom-up model that can provide conditions for the CTD model\n",
    "- evaluate the bottom-up model\n",
    "- (optional/advanced) learn how the CTD model is trained with generative sampling\n",
    "- train the CTD model\n",
    "- evaluate the CTD model\n",
    "- **Nice feature of CTD** use the CTD model to track individuals\n",
    "\n",
    "Note: This notebook **can also be run locally**. However, using a GPU is recommended to train the models and run video inference. Just skip the _Installing DeepLabCut on COLAB_ section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad548ee1",
   "metadata": {},
   "source": [
    "### ⚠️⚠️ Change the Runtime type to use a GPU!⚠️⚠️\n",
    "\n",
    "First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d41bf5e",
   "metadata": {},
   "source": [
    "### Installing DeepLabCut on COLAB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4428d550",
   "metadata": {},
   "source": [
    "Let's install the latest version of DeepLabCut, straight from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9ebeae",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/DeepLabCut/DeepLabCut.git#egg=deeplabcut\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a07bc",
   "metadata": {},
   "source": [
    "**(Be sure to click \"RESTART RUNTIME\" if it is displayed above before moving on !)** You will see this button at the output of the cells above ^."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c35856e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2ca689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import shutil\n",
    "from io import BytesIO\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import deeplabcut\n",
    "import deeplabcut.pose_estimation_pytorch as dlc_torch\n",
    "import deeplabcut.utils.auxiliaryfunctions as auxiliaryfunctions\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a04338",
   "metadata": {},
   "source": [
    "### Downloading the Tri-Mouse Dataset\n",
    "\n",
    "This cell downloads the Tri-Mouse dataset from Zenodo into the current working directory (or `cwd`), which should be the directory you launched the jupyter server from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1c71ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd()\n",
    "config = str(download_path / \"trimice-dlc-2021-06-22\" / \"config.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ed973",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading the tri-mouse dataset into {download_path}\")\n",
    "\n",
    "url_record = \"https://zenodo.org/api/records/5851157\"\n",
    "response = requests.get(url_record)\n",
    "if response.status_code == 200:\n",
    "    file = response.json()[\"files\"][0]\n",
    "    title = file[\"key\"]\n",
    "    print(f\"Downloading {title}...\")\n",
    "    with requests.get(file['links']['self'], stream=True) as r:\n",
    "        with ZipFile(BytesIO(r.content)) as zf:\n",
    "            zf.extractall(path=download_path)\n",
    "else:\n",
    "    raise ValueError(f\"The URL {url_record} could not be reached.\")\n",
    "\n",
    "\n",
    "# Check that the config was downloaded correctly\n",
    "print(f\"Config path: {config}\")\n",
    "if not Path(config).exists():\n",
    "    print(f\"Could not find config at {config}: check that the dataset was downloaded correctly!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414d1700",
   "metadata": {},
   "source": [
    "## Training a CTD Model in DeepLabCut\n",
    "\n",
    "BUCTD (or bottom-up conditioned top-down), as its name suggests, requires a bottom-up model to provide conditions (or **pose proposals**) for the CTD model to fix. So the first step in getting a CTD model that can be used to run inference is to train a bottom-up model to provide conditions!\n",
    "\n",
    "We'll also **ensure that we're training the bottom-up and CTD models on the same train/test splits!** This is important: if you're training the models on different training images and evaluating them on different test images, then their results aren't comparable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e808c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = auxiliaryfunctions.read_config(config)\n",
    "train_frac = cfg[\"TrainingFraction\"][0]\n",
    "print(f\"Using {int(100 * train_frac)}% of the data in the training set.\")\n",
    "\n",
    "num_images = 112\n",
    "train_images = int(train_frac * num_images)\n",
    "\n",
    "seed = 0\n",
    "rng = np.random.default_rng(seed)\n",
    "\n",
    "train_indices = rng.choice(num_images, size=train_images, replace=False, shuffle=False).tolist()\n",
    "test_indices = [idx for idx in range(num_images) if idx not in train_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2165546",
   "metadata": {},
   "source": [
    "### Training a BU Model\n",
    "\n",
    "We'll take the simplest approach possible here and train a ResNet pose estimation model. As the CTD model will be used to improve the predictions made by the BU model, we want something light and fast rather than something heavy and slow!\n",
    "\n",
    "We'll start by **creating the shuffle for the bottom-up model (with index 1) with the selected train/test split**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8329fce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "BU_SHUFFLE = 1\n",
    "\n",
    "deeplabcut.create_training_dataset(\n",
    "    config,\n",
    "    Shuffles=[BU_SHUFFLE],\n",
    "    trainIndices=[train_indices],\n",
    "    testIndices=[test_indices],\n",
    "    net_type=\"resnet_50\",\n",
    "    engine=deeplabcut.Engine.PYTORCH,\n",
    "    userfeedback=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb714ec",
   "metadata": {},
   "source": [
    "We can then train the model defined in the created bottom-up shuffle. To make running this notebook a bit quicker, we'll **only train the BU model for 100 epochs**. The model should still perform well enough, and as we're less interested in the BU model than the CTD model we'll save a bit of time and compute here. Training the model should **take 10 to 20 minutes**, depending on your CPU and GPU performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec8e03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(\n",
    "    config,\n",
    "    shuffle=BU_SHUFFLE,\n",
    "    epochs=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd158943",
   "metadata": {},
   "source": [
    "And finally we evaluate it! If you trained for 100 epochs, you should get an mAP around 90, and RMSE around 4-5 pixels. When calling `evaluate_network`, the PAF graph is pruned (as described in [Lauer et al. 2022 (Nature Methods)](https://www.nature.com/articles/s41592-022-01443-0)) to boost performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a02cf7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config, Shuffles=[BU_SHUFFLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9742e6a8",
   "metadata": {},
   "source": [
    "### Training the CTD Model\n",
    "\n",
    "As for the BU model, we need to start by creating the shuffle for the CTD model. We'll use `create_training_dataset_from_existing_split` to create a shuffle with the same train/test split as the BU shuffle. You could equivalently call `create_training_dataset(..., trainIndices=[train_indices], testIndices=[test_indices], ...)` again, as done above for the BU shuffle.\n",
    "\n",
    "In this notebook, we'll use a preNet CTD architecture. You can check out the paper for more information on how preNet models are designed!\n",
    "\n",
    "We'll also specify which model we want to use to provide conditions with the `ctd_conditions` parameter. As is indicated in the docstring:\n",
    "\n",
    "```\n",
    "ctd_conditions: int | str | Path | tuple[int, str] | tuple[int, int] | None, default = None,\n",
    "  If using a conditional-top-down (CTD) net_type, this argument should be specified. It defines the\n",
    "  conditions that will be used with the CTD model. It can be either:\n",
    "    * A shuffle number (ctd_conditions: int), which must correspond to a bottom-up (BU) network type.\n",
    "    * A predictions file path (ctd_conditions: string | Path), which must correspond to a \n",
    "      .json or .h5 predictions file.\n",
    "    * A shuffle number and a particular snapshot (ctd_conditions: tuple[int, str] | tuple[int, int]), \n",
    "      which respectively correspond to a bottom-up (BU) network type and a particular snapshot name or \n",
    "      index.\n",
    "```\n",
    "\n",
    "We'll use the index of the BU shuffle defined above, and the best snapshot that was saved (indicated through a -1). You can edit which model is used to provide conditions through the `pytorch_config` for the `CTD_SHUFFLE` (in this case shuffle `2`):\n",
    "\n",
    "```yaml\n",
    "# Example: Loading the predictions for snapshot-250.pt of shuffle 1.\n",
    "data:\n",
    "  conditions:\n",
    "    shuffle: 1\n",
    "    snapshot: snapshot-250.pt\n",
    "\n",
    "# Example: Loading the predictions for the last snapshot of shuffle 1.\n",
    "data:\n",
    "  conditions:\n",
    "    shuffle: 1\n",
    "    snapshot_index: -1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f75947d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CTD_SHUFFLE = 2\n",
    "\n",
    "deeplabcut.create_training_dataset_from_existing_split(\n",
    "    config,\n",
    "    from_shuffle=BU_SHUFFLE,\n",
    "    shuffles=[CTD_SHUFFLE],\n",
    "    net_type=\"ctd_prenet_cspnext_m\",\n",
    "    engine=deeplabcut.Engine.PYTORCH,\n",
    "    ctd_conditions=(BU_SHUFFLE, -1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2829415",
   "metadata": {},
   "source": [
    "#### (Optional/Advanced) Learning and visualizing generative sampling during training\n",
    "\n",
    "You can skip this section as it's simply to visualize how CTD models are trained, if you aren't interested in learning about it. This section **uses some internal DeepLabCut functions which may seem a bit complicated if you're not used to using them; you can ignore most of the code and just read the text/comments and look at the outputs if you're more comfortable with that.**\n",
    "\n",
    "Conditional top-down models are trained using _generative sampling_, as introduced in PoseFix \\[1\\]. For every ground truth pose, we'll add some errors. The errors that can be introduced are:\n",
    "\n",
    "- Jitter error is defined as a small displacement from the GT keypoint.\n",
    "- Swap error represents a confusion between the same or similar parts which belong to different persons.\n",
    "- Inversion error occurs when a pose estimation model is confused between semantically similar parts that belong to the same instance.\n",
    "- Miss error represents a large displacement from the GT keypoint position.\n",
    "\n",
    "It's important that \"enough\" generative sampling is applied (so the model can learn how to correct errors), but applying too much can be bad too! You want the model to learn to correct errors that are realistic (w.r.t. the task at hand), not just receive random points and have to learn by itself where the keypoints go. **The default parameters should work well on most datasets.**\n",
    "\n",
    "The way these keypoints are \"sampled\" can be visuallized below. We'll create a `dataset` (which is used by DeepLabCut for training) and sample some data from this dataset. You can see that every time we sample an image, we get different keypoint conditions that will be given to the model. This ensures that the model is well trained to deal with a variety of mistakes that can be made by the bottom up model. On the left side of the plots, you have an image with the ground truth keypoints annotated. On the right side of the plots, you have the pose conditions that the CTD model will receive and will be tasked with fixing.\n",
    "\n",
    "> \\[1\\]: Moon, Gyeongsik, Ju Yong Chang, and Kyoung Mu Lee. \"Posefix: Model-agnostic general human pose refinement network.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a43ec8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_generative_sampling(dataset: dlc_torch.PoseDataset) -> None:\n",
    "    # Sample the same image 3 times and plot the results\n",
    "    for i in range(3):\n",
    "        item = dataset[0]\n",
    "\n",
    "        # Remove ImageNet normalization from the image so it displays well\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        img = item[\"image\"].transpose((1, 2, 0))\n",
    "        img = np.clip(img * std + mean, 0, 1)\n",
    "\n",
    "        # Get the ground trouth and \"conditional pose\"\n",
    "        gt_pose = item[\"annotations\"][\"keypoints\"][0]\n",
    "        gen_samples = item[\"context\"][\"cond_keypoints\"][0]\n",
    "\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        for ax in axs:\n",
    "            ax.imshow(img)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        # plot the ground truth on the left and conditions on the right\n",
    "        for ax, title, keypoints in zip(\n",
    "            axs,\n",
    "            [\"Ground Truth Pose\", \"Pose Conditions\"],\n",
    "            [gt_pose, gen_samples],\n",
    "        ):\n",
    "            ax.set_title(title)\n",
    "            for x, y, vis in keypoints:\n",
    "                if vis > 0:\n",
    "                    ax.scatter([x], [y])\n",
    "\n",
    "\n",
    "ctd_loader = dlc_torch.DLCLoader(config, shuffle=CTD_SHUFFLE)\n",
    "\n",
    "transform = dlc_torch.build_transforms(ctd_loader.model_cfg[\"data\"][\"train\"])\n",
    "dataset = ctd_loader.create_dataset(transform, mode=\"train\", task=ctd_loader.pose_task)\n",
    "\n",
    "# Fix the seeds for reproducibility; you can change the seed from `0` to another value\n",
    "# to change the results\n",
    "dlc_torch.fix_seeds(0)\n",
    "plot_generative_sampling(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d4a150",
   "metadata": {},
   "source": [
    "The generative sampling can be parameterized through the `pytorch_config.yaml` as well. Let's play around with these parameters a bit and see how that changes the conditions that will be given to the model.\n",
    "\n",
    "First, we'll just lower the `keypoint_sigmas`, which impacts how much pose conditions can move during jittering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ee88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctd_loader = dlc_torch.DLCLoader(config, shuffle=CTD_SHUFFLE)\n",
    "\n",
    "# We'll edit the model config here directly; In practice, edit the pytorch_config file instead.\n",
    "# The parameters that can be set here are the parameters of the `dlc_torch.GenSamplingConfig`\n",
    "ctd_loader.model_cfg[\"data\"][\"gen_sampling\"] = {\n",
    "    # lower the keypoint sigma by a factor of 2 (default: 0.1)\n",
    "    #   -> this changes by how much keypoints are jittered; the smaller \n",
    "    #      the value, the smaller the jitter\n",
    "    \"keypoint_sigmas\": 0.05,\n",
    "}\n",
    "\n",
    "transform = dlc_torch.build_transforms(ctd_loader.model_cfg[\"data\"][\"train\"])\n",
    "dataset = ctd_loader.create_dataset(transform, mode=\"train\", task=ctd_loader.pose_task)\n",
    "\n",
    "# Fix the seeds for reproducibility; you can change the seed from `0` to another value\n",
    "# to change the results\n",
    "dlc_torch.fix_seeds(0)\n",
    "plot_generative_sampling(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055dea55",
   "metadata": {},
   "source": [
    "Next, we'll update the probabilities of make errors. You can edit these values yourself to see how it impacts the generative sampling. Note that these probabilities are **not absolute** - as a single type of error is applied to each keypoint, changing the probability of one type of error happening will change the probability that other types of errors occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa509d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctd_loader = dlc_torch.DLCLoader(config, shuffle=CTD_SHUFFLE)\n",
    "\n",
    "# We'll edit the model config here directly; In practice, edit the pytorch_config file instead.\n",
    "# The parameters that can be set here are the parameters of the `dlc_torch.GenSamplingConfig`\n",
    "ctd_loader.model_cfg[\"data\"][\"gen_sampling\"] = {\n",
    "    \"jitter_prob\": 0.5,\n",
    "    \"swap_prob\": 0.1,\n",
    "    \"inv_prob\": 0.1,\n",
    "    \"miss_prob\": 0.25,\n",
    "}\n",
    "\n",
    "transform = dlc_torch.build_transforms(ctd_loader.model_cfg[\"data\"][\"train\"])\n",
    "dataset = ctd_loader.create_dataset(transform, mode=\"train\", task=ctd_loader.pose_task)\n",
    "\n",
    "# Fix the seeds for reproducibility; you can change the seed from `0` to another value\n",
    "# to change the results\n",
    "dlc_torch.fix_seeds(0)\n",
    "plot_generative_sampling(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ecdc8",
   "metadata": {},
   "source": [
    "#### Training and Evaluating the CTD Model\n",
    "\n",
    "Next, we can simply train the CTD model. It should take **15 to 30 minutes** to train the model to 250 epochs on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.train_network(config, shuffle=CTD_SHUFFLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4b810f",
   "metadata": {},
   "source": [
    "If your CTD model is well trained, it should now outperform the performance of the BU model who's predictions it uses as conditions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb3c2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.evaluate_network(config, Shuffles=[CTD_SHUFFLE])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ea24b55",
   "metadata": {},
   "source": [
    "## Tracking with CTD\n",
    "\n",
    "One of the big advantages of having a CTD model is that it can be used to track individuals directly! Let's say you have the pose for your animals at `frame T`. Then you can use those poses as conditions for `frame T+1`, and let your CTD model simply \"update\" the poses depending on how much your mice moved.\n",
    "\n",
    "In the simplest scenario, you only need to run the BU model on the first frame, and then the CTD model takes over for inference and tracking:\n",
    "\n",
    "1. Run the BU model to generate conditions for the 1st frame of the video\n",
    "2. For every frame after that, use the predictions from the previous frame as conditions\n",
    "\n",
    "However, this may not fit your scenario perfectly. Maybe all the mice aren't present in the first frame, and if they aren't detected by the BU model they'll never be tracked. Maybe at some point the CTD model makes an error and you lose track of a mouse. There are some options to deal with this:\n",
    "\n",
    "- Run the BU model every time at least one mouse is not detected (if you expect N mice to be in the video and you only detect N-1 mice, run the BU model):\n",
    "  - In this case, the predictions from the BU model need to be \"merged in\" to the existing N-1 tracks\n",
    "  - We can merge them in by using a similarity score between poses (OKS) which ranges from 0 to 1\n",
    "  - You likely don't want to run the BU model every frame, as this would slow down inference.\n",
    "- Run the BU model every K frames in case new mice appear\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc777c8",
   "metadata": {},
   "source": [
    "### Downloading a Tri-Mouse video\n",
    "\n",
    "First, let's download a video from the 3-mice dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d678a5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_path = Path.cwd()\n",
    "video_name = \"videocompressed1.mp4\"\n",
    "video_path = str(download_path / video_name)\n",
    "print(f\"Video will be saved in {video_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2247556f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Downloading the tri-mouse video into {download_path}\")\n",
    "\n",
    "url_video_record = \"https://zenodo.org/api/records/7883589\"\n",
    "response = requests.get(url_video_record)\n",
    "if response.status_code == 200:\n",
    "    file = response.json()[\"files\"][0]\n",
    "    title = file[\"key\"]\n",
    "    print(f\"Downloading {title}...\")\n",
    "    with requests.get(file['links']['self'], stream=True) as r:\n",
    "        with ZipFile(BytesIO(r.content)) as zf:\n",
    "            zf.extractall(path=download_path)\n",
    "else:\n",
    "    raise ValueError(f\"The URL {url_record} could not be reached.\")\n",
    "\n",
    "# Check that the video was downloaded\n",
    "src_video_path = download_path / \"demo-me-2021-07-14\" / \"videos\" / video_name\n",
    "if not src_video_path.exists():\n",
    "    raise ValueError(\"Failed to download the video\")\n",
    "\n",
    "# Move the video to the final path\n",
    "shutil.move(src_video_path, video_path)\n",
    "if not Path(video_path).exists():\n",
    "    raise ValueError(\"Failed to move the video\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721ce122",
   "metadata": {},
   "source": [
    "### Running Video Analysis\n",
    "\n",
    "You can track using your CTD model by setting `ctd_tracking=True` when calling `analyze_videos`. Of course, you then won't need to convert detections to tracklets or link tracklets, as the CTD model will directly be tracking the animals.\n",
    "\n",
    "You can visualize the outputs by setting `track_method=\"ctd\"` when calling `create_labeled_video`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b3787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "deeplabcut.analyze_videos(\n",
    "    config,\n",
    "    [video_path],\n",
    "    shuffle=CTD_SHUFFLE,\n",
    "    ctd_tracking=True,\n",
    ")\n",
    "deeplabcut.create_labeled_video(\n",
    "    config,\n",
    "    [video_path],\n",
    "    shuffle=CTD_SHUFFLE,\n",
    "    track_method=\"ctd\",\n",
    "    color_by=\"individual\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bd9ab7",
   "metadata": {},
   "source": [
    "It can be beneficial to customize the tracking parameters a bit. The tracking paramaters you can set are:\n",
    "\n",
    "- **`bu_on_lost_idv`**: When True, the BU model is run when there are fewer conditions found than the expected number of individuals in the video.\n",
    "- **`bu_min_frequency`**: The minimum frequency at which the BU model is run to generate conditions. If None, the BU model is only run to initialize the pose in the first frame, and then is not run again. If a positive number N, the BU model is run every N frames. The BU predictions are then combined with the CTD predictions to continue the tracklets.\n",
    "- **`bu_max_frequency`**: The maximum frequency at which the BU model can be run. Must be greater than `bu_min_frequency`. When there are fewer conditions than individuals expected in the video and `bu_on_lost_idv` is True, the BU model may be run on every frame. This can happen if individuals can disappear from the video, and each frame may have a variable number of individuals. If `bu_max_frequency` is set to N, then the BU model will be run at most every N-th frame, which improves the inference speed of the model.\n",
    "- **`threshold_bu_add`**: The OKS threshold below which a BU pose must be (wrt. any existing CTD pose) to be added to the poses.\n",
    "- **`threshold_ctd`**: The score threshold below which detected keypoints are NOT given to the CTD model to predict pose for the next frame.\n",
    "- **`threshold_nms`**: The OKS threshold for non-maximum suppression to remove duplicates poses when two CTD model predictions converge to a single animal.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e667af4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_folder = str(Path(video).parent / \"custom-ctd-tracking\")\n",
    "\n",
    "deeplabcut.analyze_videos(\n",
    "    config,\n",
    "    [video_path],\n",
    "    shuffle=CTD_SHUFFLE,\n",
    "    destfolder=dest_folder,\n",
    "    ctd_tracking=dict(\n",
    "        bu_on_lost_idv=True,\n",
    "        bu_max_frequency=10,\n",
    "        threshold_bu_add=0.5,\n",
    "        threshold_ctd=0.01,\n",
    "        threshold_nms=0.8,\n",
    "    ),\n",
    ")\n",
    "deeplabcut.create_labeled_video(\n",
    "    config,\n",
    "    [video_path],\n",
    "    shuffle=CTD_SHUFFLE,\n",
    "    destfolder=dest_folder,\n",
    "    track_method=\"ctd\",\n",
    "    color_by=\"individual\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
