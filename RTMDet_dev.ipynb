{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6eec817-48c7-40d7-92ab-f20705562a62",
   "metadata": {},
   "source": [
    "# Let's try to figure out how to develop RTMDet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8618fe-8064-4c3c-9840-ff0c19525aa7",
   "metadata": {},
   "source": [
    "## First, let's understand what is the difference between CSPNext and RTMPose"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a7e73-0b2e-44a1-a575-3579f3eb168c",
   "metadata": {},
   "source": [
    "### (RTMPose is CSPNext + SimCC HEAD?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8920742e-91fd-4887-b7b4-fc39f69ac626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'deeplabcut.pose_estimation_pytorch.models.backbones.cspnext.CSPNeXt'>\n",
      "Nb parameters: 12279432\n",
      "Shape of output of dummy tensor: torch.Size([1, 768, 8, 8])\n",
      "--------------------------------------------------------------------------------\n",
      "<class 'deeplabcut.pose_estimation_pytorch.models.model.PoseModel'>\n",
      "Nb parameters: 13172879\n",
      "Shape of output of dummy tensor: torch.Size([1, 5, 512]) , torch.Size([1, 5, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "x = torch.randn(1, 3, 256, 256)\n",
    "\n",
    "# ----\n",
    "\n",
    "# First, let's create a CSPNeXt object\n",
    "from deeplabcut.pose_estimation_pytorch.models.backbones.cspnext import CSPNeXt\n",
    "\n",
    "cspnext = CSPNeXt()\n",
    "print(type(cspnext)) # cspnext is of type deeplabcut.pose_estimation_pytorch.models.backbones.cspnext.CSPNeXt , which is a nn.Module\n",
    "print(f\"Nb parameters: {sum(p.numel() for p in cspnext.parameters())}\")\n",
    "# print(cspnext) # This floods the output\n",
    "y = cspnext(x)\n",
    "print(f\"Shape of output of dummy tensor: {y.shape}\")\n",
    "\n",
    "print(\"-\"* 80)\n",
    "\n",
    "# Next, let's create a RTMPose nn.Module object\n",
    "from deeplabcut.pose_estimation_pytorch.config.utils import get_config_folder_path, replace_default_values\n",
    "from deeplabcut.core.config import read_config_as_dict\n",
    "from deeplabcut.pose_estimation_pytorch.models import PoseModel\n",
    "\n",
    "net_type = \"rtmpose_m\" # for example\n",
    "nb_bodyparts = 5 # for example\n",
    "\n",
    "configs_dir = get_config_folder_path()\n",
    "architecture = net_type.split(\"_\")[0]\n",
    "cfg_path = cfg_path = configs_dir / architecture / f\"{net_type}.yaml\"\n",
    "model_cfg = read_config_as_dict(cfg_path)\n",
    "model_cfg = replace_default_values(\n",
    "    model_cfg,\n",
    "    num_bodyparts=nb_bodyparts,\n",
    ") # Interesting observation: the yaml file defines values that depend on nb_individuals and/or nb_bodyparts, and are updated with real values once known (when creating the actual pytorch_config.yaml)\n",
    "rtmpose = PoseModel.build(model_cfg[\"model\"]) # here, there might be some optional parameters, todo investigate\n",
    "print(type(rtmpose))\n",
    "print(f\"Nb parameters: {sum(p.numel() for p in rtmpose.parameters())}\")\n",
    "# print(rtmpose) # This floods the output\n",
    "z = rtmpose(x)\n",
    "print(f\"Shape of output of dummy tensor: {z['bodypart']['x'].shape} , {z['bodypart']['y'].shape}\")\n",
    "# Okay so basically, the first level keys are defined by the pytorch_config.yaml (in the heads block), and the second level keys are defined in the RTMCCHead forward method.\n",
    "# The first level keys are certainly added when doing PoseModel.build\n",
    "# When printing rtmpose, there are different submodules (backbone, head). They are very certainly created during PoseModel.build()\n",
    "# The differences in sizes of rtmpose are the same ones as the difference in sizes of cspnext. \n",
    "# The different parameters (deepen_factor, widen_factor, backbone_output_channels) are configured in the rtmpose yaml files.\n",
    "\n",
    "# - state_dict() returns a dictionary containing all the modelâ€™s learnable parameters and buffers,\n",
    "# while load_state_dict() restores those values into a model with the same architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c3521a-ab8c-44dc-81dd-975cb8c9d864",
   "metadata": {},
   "source": [
    "## Then, let's dive into how the existing detectors work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f5165-9431-4bb8-abcc-298b9524fed4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ecfc8ef3-a43a-478b-a87b-ecec9655a1b9",
   "metadata": {},
   "source": [
    "## Finally, let's instantiate the official RTMDet network for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1548120a-db8b-471b-bad3-bc9bca9fea7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
