
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Improving network performance on unbalanced data via augmentation ü¶á &#8212; DeepLabCut</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Mission and Values of DeepLabCut" href="../MISSION_AND_VALUES.html" />
    <link rel="prev" title="Intel OpenVINO backend" href="OpenVINO.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">DeepLabCut</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../README.html">
                    Installation: how to install DeepLabCut
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Getting Started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../UseOverviewGuide.html">
   Documentation Overview
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Installation
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../installation.html">
   How To Install DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="installTips.html">
   Installation Tips
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../docker.html">
   DeepLabCut Docker containers
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  User Guides
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../standardDeepLabCut_UserGuide.html">
   DeepLabCut User Guide (for single animal projects)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../maDLC_UserGuide.html">
   DeepLabCut for Multi-Animal Projects
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Overviewof3D.html">
   3D DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../HelperFunctions.html">
   Helper &amp; Advanced Optional Function Documentation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Graphical User Interfaces (GUIs)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../PROJECT_GUI.html">
   Interactive Project Manager GUI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../napariGUI.html">
   napari labeling GUI
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ClusteringNapari.html">
   Clustering in the napari-DeepLabCut GUI
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DeepLabCut-Live!
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../deeplabcutlive.html">
   DeepLabCut-Live!
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DeepLabCut Model Zoo
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ModelZoo.html">
   The DeepLabCut Model Zoo!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="UsingModelZooPupil.html">
   Using ModelZoo models on your own datasets
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MegaDetectorDLCLive.html">
   üíö MegaDetector+DeepLabCut üíú
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  DeepLabCut Benchmark
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../benchmark.html">
   DeepLabCut benchmark
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Hardware
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="TechHardware.html">
   Technical (Hardware) Considerations
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Tutorials &amp; Cookbook
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../tutorial.html">
   Multi-animal pose estimation with DeepLabCut: A 5-minute tutorial
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../convert_maDLC.html">
   How to convert a pre-2.2 project for use with DeepLabCut 2.2
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="io.html">
   Input/output manipulations with DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="nn.html">
   Model training tips &amp; tricks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="post.html">
   Some data processing recipes!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="BatchProcessing.html">
   Automate training and video analysis: Batch Processing
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="DLCMethods.html">
   How to write a DLC Methods Section
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="OpenVINO.html">
   Intel OpenVINO backend
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Improving network performance on unbalanced data via augmentation ü¶á
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Mission &amp; Contribute
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../MISSION_AND_VALUES.html">
   Mission and Values of DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../roadmap.html">
   A development roadmap for DeepLabCut
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Governance.html">
   Governance Model of DeepLabCut
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/DeepLabCut/DeepLabCut/master?urlpath=tree/docs/docs/recipes/flip_and_rotate.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
      <li>
        <a href="https://colab.research.google.com/github/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb/github/DeepLabCut/DeepLabCut/blob/master/docs/docs/recipes/flip_and_rotate.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Colab"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_colab.png">
  </span>
<span class="headerbtn__text-container">Colab</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/DeepLabCut/DeepLabCut"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/DeepLabCut/DeepLabCut/issues/new?title=Issue%20on%20page%20%2Fdocs/recipes/flip_and_rotate.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/docs/recipes/flip_and_rotate.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Improving network performance on unbalanced data via augmentation ü¶á
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#background">
     Background
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-dataset">
     The dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Background
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-deeplabcut-project">
     Creating the DeepLabCut project
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-the-config-file">
     Edit the config file
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convert-training-data-to-dlc-format">
     Convert training data to DLC format
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structuring-the-project-for-testing">
     Structuring the project for testing
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#test-data">
       Test data
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#treatments">
       Treatments
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#refining">
         Refining
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#amount-of-base-training-data">
         Amount of base training data
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-data-indices">
       Training data indices
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#define-shuffles">
       Define shuffles
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-modelprefixes-to-keep-track-of-augmentations">
     Using modelprefixes to keep track of augmentations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regarding-trainingsetindex">
       Regarding trainingsetindex
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-pose-cfg-yaml">
     Edit pose_cfg.yaml
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-networks">
     Train networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#perform-evaluation">
   Perform evaluation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-data-for-plotting-and-statistical-comparison">
     Get data for plotting and statistical comparison
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-plot">
     Interpret plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-per-video">
     Plot per video
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-per-video-plot">
     Interpret per video plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-easy-and-difficult-videos">
     Compare ‚Äúeasy‚Äù and ‚Äúdifficult‚Äù videos
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspect-deeplabcut-predictions-from-evaluations">
     Inspect DeepLabCut predictions from evaluations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#augment-to-reduce-left-right-bias">
     Augment to reduce left-right bias
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-pose-cfg-yaml-for-fliplr-augmentation">
     Edit pose_cfg.yaml for fliplr augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-network-with-fliplr-augmentation">
     Train network with
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-performance-of-fliplr-augmented-network-to-refined-baseline">
     Compare performance of fliplr augmented network to refined baseline
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-performance-of-network-after-fliplr-augmentation">
     Interpret performance of network after ‚Äúfliplr‚Äù augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-network-with-fliplr-and-stronger-rotation-augmentation">
     Train network with
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     and stronger
     <code class="docutils literal notranslate">
      <span class="pre">
       rotation
      </span>
     </code>
     augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-performance-of-network-after-fliplr-and-180-rotation-augmentation">
     Interpret performance of network after
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     and 180¬∞ rotation augmentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-scalability-is-not-important-consider-refining">
     Where scalability is not important, consider refining
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspect-bodypart-predictions-and-augment-with-purpose">
     Inspect bodypart predictions and augment with purpose
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Improving network performance on unbalanced data via augmentation ü¶á</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Improving network performance on unbalanced data via augmentation ü¶á
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#background">
     Background
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-dataset">
     The dataset
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     Background
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-deeplabcut-project">
     Creating the DeepLabCut project
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-the-config-file">
     Edit the config file
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#convert-training-data-to-dlc-format">
     Convert training data to DLC format
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#structuring-the-project-for-testing">
     Structuring the project for testing
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#test-data">
       Test data
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#treatments">
       Treatments
      </a>
      <ul class="nav section-nav flex-column">
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#refining">
         Refining
        </a>
       </li>
       <li class="toc-h4 nav-item toc-entry">
        <a class="reference internal nav-link" href="#amount-of-base-training-data">
         Amount of base training data
        </a>
       </li>
      </ul>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#training-data-indices">
       Training data indices
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#define-shuffles">
       Define shuffles
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#using-modelprefixes-to-keep-track-of-augmentations">
     Using modelprefixes to keep track of augmentations
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#regarding-trainingsetindex">
       Regarding trainingsetindex
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-pose-cfg-yaml">
     Edit pose_cfg.yaml
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-networks">
     Train networks
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#perform-evaluation">
   Perform evaluation
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#get-data-for-plotting-and-statistical-comparison">
     Get data for plotting and statistical comparison
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-plot">
     Interpret plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#plot-per-video">
     Plot per video
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-per-video-plot">
     Interpret per video plot
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-easy-and-difficult-videos">
     Compare ‚Äúeasy‚Äù and ‚Äúdifficult‚Äù videos
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspect-deeplabcut-predictions-from-evaluations">
     Inspect DeepLabCut predictions from evaluations
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#augment-to-reduce-left-right-bias">
     Augment to reduce left-right bias
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#edit-pose-cfg-yaml-for-fliplr-augmentation">
     Edit pose_cfg.yaml for fliplr augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-network-with-fliplr-augmentation">
     Train network with
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#compare-performance-of-fliplr-augmented-network-to-refined-baseline">
     Compare performance of fliplr augmented network to refined baseline
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-performance-of-network-after-fliplr-augmentation">
     Interpret performance of network after ‚Äúfliplr‚Äù augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#train-network-with-fliplr-and-stronger-rotation-augmentation">
     Train network with
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     and stronger
     <code class="docutils literal notranslate">
      <span class="pre">
       rotation
      </span>
     </code>
     augmentation
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#interpret-performance-of-network-after-fliplr-and-180-rotation-augmentation">
     Interpret performance of network after
     <code class="docutils literal notranslate">
      <span class="pre">
       fliplr
      </span>
     </code>
     and 180¬∞ rotation augmentation
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#conclusion">
   Conclusion
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#where-scalability-is-not-important-consider-refining">
     Where scalability is not important, consider refining
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inspect-bodypart-predictions-and-augment-with-purpose">
     Inspect bodypart predictions and augment with purpose
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="improving-network-performance-on-unbalanced-data-via-augmentation">
<h1>Improving network performance on unbalanced data via augmentation ü¶á<a class="headerlink" href="#improving-network-performance-on-unbalanced-data-via-augmentation" title="Permalink to this headline">#</a></h1>
<p>By Jonas Bengt Carina H√•kansson</p>
<p>Part of the 2022 AI Residency Program</p>
<section id="background">
<h2>Background<a class="headerlink" href="#background" title="Permalink to this headline">#</a></h2>
<p>Read the <a class="reference external" href="https://deeplabcut.medium.com/balancing-bats-d3c5bcfb71e1">DeepLabCut Blog post</a> about how one can solve unbalanced data with flipping-keypoint augmentation, then see below for the process and code to do so!</p>
</section>
<section id="the-dataset">
<h2>The dataset<a class="headerlink" href="#the-dataset" title="Permalink to this headline">#</a></h2>
<p>This dataset consists of videos of bats flying freely at the <a class="reference external" href="https://austinbatrefuge.org/">Austin Bat Refuge</a>. The videos were shot by Dr. Aaron Corcoran, Head of the <a class="reference external" href="https://sonarjamming.com/">Sensory &amp; Aerial Ecology lab</a> at the University of Colorado, Colorado Sprungs (UCCS) in 2020. The dataset is comprised of 26 video triplets. Each triplet consists of three videos depicting the same bat flight from different angles.</p>
<p>The videos have been partially digitized in <a class="reference external" href="https://github.com/tlhedrick/dltdv">DLTdv</a> after which custom MATLAB code has been used to extract the digitized frames and coordinates in a format that DeepLabCut can read.</p>
</section>
<section id="id1">
<h2>Background<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>This text documents my experience in using data augmentation to improve the accuracy of DeepLabCut in tracking bodyparts of the bats in the videos during my time as a <a class="reference external" href="https://www.deeplabcutairesidency.org/">DeepLabCut AI Resident</a> during the summer of 2022.</p>
</section>
<section id="creating-the-deeplabcut-project">
<h2>Creating the DeepLabCut project<a class="headerlink" href="#creating-the-deeplabcut-project" title="Permalink to this headline">#</a></h2>
<p>We start by defining the path we will be working in. Then we create the project.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut

project_folder = &quot;/home/user/projects/&quot; #the folder in which the DLC project will be created
deeplabcut.create_new_project(project=&#39;bat_augmentation_austin_2020_bat_data&#39;,experimenter=&#39;DLC&#39;,videos=[&#39;/home/user/dummyVideos/&#39;],working_directory=project_folder)
</pre></div>
</div>
</div>
</div>
</section>
<section id="edit-the-config-file">
<h2>Edit the config file<a class="headerlink" href="#edit-the-config-file" title="Permalink to this headline">#</a></h2>
<p>Now that the project is created we need to modify our project config file. Open config.yaml in the project folder.</p>
<p>The config file can be modified with any text editor, but I will do it via Python commands to highlight the possibility of automating this process.</p>
<p><img alt="what to change in the config file" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950854851-BEJVFJYTWOZDXFFF1091/config001_videos_and_bodyparts.png?format=750w" /></p>
<p><em>The list of videos and the list of bodyparts need to change to reflect the actual videos and bodyparts</em></p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#define config file
config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;

#import tools for modifying our config file
from deeplabcut.utils.auxiliaryfunctions import read_config, edit_config
</pre></div>
</div>
</div>
</div>
<p><img alt="Bat bodyparts" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950854711-GE81YU6I16GB4EXYLGWG/bat_markers.png?format=750w" />
<em>The locations of the labels/bodyparts on the bats.</em></p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># replace the default list of bodyparts with a list of the parts that we have actually digitized
edit_config(config_path,{&quot;bodyparts&quot;:[&#39;t3L&#39;, &#39;wstL&#39;, &#39;t5L&#39;, &#39;elbL&#39;, &#39;shdL&#39;, &#39;ankL&#39;, &#39;nl&#39;, &#39;str&#39;, &#39;lmb&#39;, &#39;shdR&#39;, &#39;ankR&#39;, &#39;elbR&#39;, &#39;wstR&#39;, &#39;t5R&#39;, &#39;t3R&#39;, &#39;tail&#39;]})
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#fetch the list of videos from an older project using the same videos
videolist = read_config(&quot;/home/user/projects/old_project-DLC-2022-08-03/config.yaml&quot;)[&quot;video_sets&quot;]
edit_config(config_path,{&#39;video_sets&#39;:videolist})
</pre></div>
</div>
</div>
</div>
</section>
<section id="convert-training-data-to-dlc-format">
<h2>Convert training data to DLC format<a class="headerlink" href="#convert-training-data-to-dlc-format" title="Permalink to this headline">#</a></h2>
<p>Since this data was labeled outside of DeepLabCut, we first used our own code to convert it into csv formatted csv files, we then use the function <code class="docutils literal notranslate"><span class="pre">convertcsv2h5</span></code> to convert these csv:s into h5 files that DeepLabCut can read.</p>
<p>After doing this, I like to use the function <code class="docutils literal notranslate"><span class="pre">check_labels</span></code> to make sure that the conversion was successful.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Convert training data into the DeepLabCut format
import deeplabcut
deeplabcut.convertcsv2h5(config_path, userfeedback=False)

# Check labels (sanity check)
deeplabcut.check_labels(config_path)
</pre></div>
</div>
</div>
</div>
<p><img alt="Labeled frame" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950852823-0AF8A8L896O58A94UXIV/094_093_089_val_AS_c2_frame325_bodypart.png?format=500w" /></p>
<p><em>The labels appear to be in the right place.</em></p>
</section>
<section id="structuring-the-project-for-testing">
<h2>Structuring the project for testing<a class="headerlink" href="#structuring-the-project-for-testing" title="Permalink to this headline">#</a></h2>
<p>Since bats are such a challenging animal to automatcially digitize, in my lab, we‚Äôve been relying on what I call ‚Äúrefining‚Äù to improve DLC accuracy. This just means that for each video that we want to analyze, we first digitize a few frames from it and include those frames as training data for the DeepLabCut network.</p>
<p>We typically digitize approximately one frame per wingbeat. For a wingbeat frequency of 15 and a framerate of 800, this means approximately every 50th frame. For a lower framerate, say 400, we instead need to digitize every 50th frame. This is obviously less labour than digitizing every frame, but the workflow still scales poorly with increased acquisition. Therefore, I want to reduce the required amount of manual digitization. The challenge I‚Äôve set myself is therefore this - <strong>to use augmentation to match or beat the accuracy of refining</strong>.</p>
<section id="test-data">
<h3>Test data<a class="headerlink" href="#test-data" title="Permalink to this headline">#</a></h3>
<p>I randomly chose seven digitized triplets of videos for testing. Every 50th frame of these will be used for testing.</p>
</section>
<section id="treatments">
<h3>Treatments<a class="headerlink" href="#treatments" title="Permalink to this headline">#</a></h3>
<p>I will use two ‚Äútreatments‚Äù for testing. Firstly, I simply test how increasing the amount of training data affects accuracy. Secondly, I test how adding refining data affects accuracy. The result is a four permutations of treatments in total, sorted into four shuffles as shown in <em>table 1</em>.</p>
<section id="refining">
<h4>Refining<a class="headerlink" href="#refining" title="Permalink to this headline">#</a></h4>
<p>The seven testing video triplets have had every 25th frame digitized. Frames 50, 100, 150, ‚Ä¶ is used for testing. In this treatment, frames 25, 75, 175, ‚Ä¶ will be used for used as training data, i.e. refining. I call the networks not trained on refining data OOD for out of domain and the ones traioned on refining data Ref for refining.</p>
</section>
<section id="amount-of-base-training-data">
<h4>Amount of base training data<a class="headerlink" href="#amount-of-base-training-data" title="Permalink to this headline">#</a></h4>
<p>Subtracting the seven testing video triplets, I‚Äôm left with 19 (57 videos in total). I then randomly split these into two groups of 9 and 10. I will either train on only 10 of these video triplets, or on all 19. I call these treatments ‚Äúhalf‚Äù and ‚Äúfull‚Äù.</p>
<p>This treatment might not seem relevant for figuring out how augmentation affects performance, and in truth, I chose to perform this test to answer another question, but you will see that the results of this test are very relevant to the challenge written in bold above.</p>
<p>I will refer to training data from the 19 videos not tested on as ‚Äúbase training data‚Äù.</p>
<p><em>Table 1: The two treatments result in four permutations, organized in four shuffles.</em></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">|</span>       <span class="o">|**</span><span class="n">Half</span><span class="o">**|**</span><span class="n">Full</span><span class="o">**|</span>
<span class="o">|-------|--------|--------|</span>
<span class="o">|**</span><span class="n">OOD</span><span class="o">**|</span><span class="n">shuffle1</span><span class="o">|</span><span class="n">shuffle2</span><span class="o">|</span>
<span class="o">|**</span><span class="n">Ref</span><span class="o">**|</span><span class="n">shuffle3</span><span class="o">|</span><span class="n">shuffle4</span><span class="o">|</span>
</pre></div>
</div>
</section>
</section>
<section id="training-data-indices">
<h3>Training data indices<a class="headerlink" href="#training-data-indices" title="Permalink to this headline">#</a></h3>
<p>By default, DeepLabCut assumes that we want to randomly divide the data into training and testing data. But for testing, you might want more control of this. Therefore, we will manually define which frames are for training and which are for testing. More on this later, but for now, we need to create a dummy training dataset as this will make DeepLabCut create a nice list of all the frames in the dataset.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;

# Dummy training dataset to get indexes and so on from later
deeplabcut.create_training_dataset(config_path, Shuffles=[99])
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-shuffles">
<h3>Define shuffles<a class="headerlink" href="#define-shuffles" title="Permalink to this headline">#</a></h3>
<p>Now that we have created a dummy training dataset, we can parse its list of files and add the indices of the ones we want to train on to the list of training data indices, and the ones we want to test on to the test data indices.</p>
<p>This is easy to do since when I converted our training data from DLTdv, I made sure to name the folders with endings indicating where they belong with regards to our treatments.</p>
<p>The naming convention looks like this:</p>
<ul class="simple">
<li><p>The folders that end with ‚Äú_A‚Äù belong in the first half of the training data, that means that when we train on ‚Äúhalf‚Äù the data, we only train on these.</p></li>
<li><p>The folders that end with ‚Äú_B‚Äù belong in the second half of the training data, that means that when we train on the ‚Äúfull‚Äù data, we train on these as well as the ones from the first half.</p></li>
<li><p>The folders that end with ‚Äú_25_ref‚Äù contain the refining frames from the 7 test videos, so when we train with the ‚ÄúRef‚Äù data, we include these as training data.</p></li>
<li><p>The folders that end with ‚Äú_50_test‚Äù are from the 7 test videos and they are always used as testing data.</p></li>
</ul>
<p>So by parsing the list of frames from the dataset, we can create our four shuffles as specified in <em>table 1</em> and in the list above.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;

# We need pandas for creatig a nice list to parse
import pandas as pd

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# create empty lists for putting testing and training indices in
test_inds = []
train_inds = []

# train on half, test data is OOD, shuffle 1
test_inds = []
train_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_A&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

deeplabcut.create_training_dataset(
    config_path,
    Shuffles=[1],
    trainIndices=[train_inds],
    testIndices=[test_inds],
    net_type=&quot;resnet_50&quot;,
    augmenter_type=&quot;../imagesaug&quot;
)

# train on half+ref, shuffle 2
test_inds = []
train_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_A&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_25_ref&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

deeplabcut.create_training_dataset(
    config_path,
    Shuffles=[2],
    trainIndices=[train_inds],
    testIndices=[test_inds],
    net_type=&quot;resnet_50&quot;,
    augmenter_type=&quot;../imagesaug&quot;
)

# train on full, test data is OOD, shuffle 3
test_inds = []
train_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_A&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_B&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

deeplabcut.create_training_dataset(
    config_path,
    Shuffles=[3],
    trainIndices=[train_inds],
    testIndices=[test_inds],
    net_type=&quot;resnet_50&quot;,
    augmenter_type=&quot;../imagesaug&quot;
)

# train on full+ref, shuffle 4
test_inds = []
train_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_A&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_B&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_25_ref&quot;):
        train_inds.append(i)
    elif str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

deeplabcut.create_training_dataset(
    config_path,
    Shuffles=[4],
    trainIndices=[train_inds],
    testIndices=[test_inds],
    net_type=&quot;resnet_50&quot;,
    augmenter_type=&quot;../imagesaug&quot;
)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="using-modelprefixes-to-keep-track-of-augmentations">
<h2>Using modelprefixes to keep track of augmentations<a class="headerlink" href="#using-modelprefixes-to-keep-track-of-augmentations" title="Permalink to this headline">#</a></h2>
<p>Now we are using the shuffles to keep track of the training data used for our different networks, but we will also be trying different augmentatns. So how do we keep track of that? We could just keep making new shuffles and keep a list of what shuffle is what, but that can quickly become unwieldy.</p>
<p>Instead, we can use modelprefixes. They provide a way to structure a DLC project into something like subprojects. Basically, if you copy a dlc-model folder (in my case, found in <code class="docutils literal notranslate"><span class="pre">/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/dlc-models/iteration-0</span></code>) into a subfolder in the project, you can then interact with the copied dlc model by using the argument <code class="docutils literal notranslate"><span class="pre">modelprefix</span></code>.</p>
<p>Say, for example, that I want to try a more aggressive motion blur augmentation, then I could create a folder called <code class="docutils literal notranslate"><span class="pre">extra_blurry</span></code> in my project, copy the dlc-models into it, and then change the pose_cfg.yaml files that contain the augmentation parameters to increase the motion blur augmentation.</p>
<p>I would then train the model with this command:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">deeplabcut</span><span class="o">.</span><span class="n">train_network</span><span class="p">(</span>
    <span class="n">config_path</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="n">shuffle_number</span><span class="p">,</span>
    <span class="n">modelprefix</span><span class="o">=</span><span class="s1">&#39;extra_blurry&#39;</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>Our first networks will be trained with the default augmentation, but it‚Äôs still useful to create a folder for them as we will want to copy the dlc-models in the root project folder for later augmentations, so it makes sense to keep those clean and do all training with model prefixes.</p>
<section id="regarding-trainingsetindex">
<h3>Regarding trainingsetindex<a class="headerlink" href="#regarding-trainingsetindex" title="Permalink to this headline">#</a></h3>
<p>Before we can create our subfolders, we need to spend a minute on the trainingset indices.</p>
<p>We often use different splits between training and testing data. This is a number &lt; 1 that sets the proportion of the training dataset that should be used for training and testing. The default is 0.95, meaning 95% is used for training, and 5% for testing. This can be either be specified in the config.yaml file or deduced automatically by DeepLabCut if we manually specify the training and testing indices (like we did). In the case of the latter, the split is indicated by the filenames of the the files in the training-dataset folder in the project folder. In my case this would be:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="n">user</span><span class="o">/</span><span class="n">projects</span><span class="o">/</span><span class="n">bat_augmentation_austin_2020_bat_data</span><span class="o">-</span><span class="n">DLC</span><span class="o">-</span><span class="mi">2022</span><span class="o">-</span><span class="mi">08</span><span class="o">-</span><span class="mi">18</span><span class="o">/</span><span class="n">training</span><span class="o">-</span><span class="n">datasets</span><span class="o">/</span><span class="n">iteration</span><span class="o">-</span><span class="mi">0</span><span class="o">/</span><span class="n">UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18</span>
</pre></div>
</div>
<p>Let‚Äôs look at the contents of that folder to deduce the training splits.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import os
files = os.listdir(&quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18&quot;)
print(*files,sep=&#39;\n&#39;)
</pre></div>
</div>
</div>
</div>
<p>I‚Äôve worked a bit on other stuff in this project, so there is some stuff here we need not consider (51 and 52)- But we see files related to shuffles 1 to 4. Let‚Äôs focus on the .mat files:</p>
<ul class="simple">
<li><p>bat_augmentation_austin_2020_bat_data_DLC80shuffle1.mat</p></li>
<li><p>bat_augmentation_austin_2020_bat_data_DLC84shuffle2.mat</p></li>
<li><p>bat_augmentation_austin_2020_bat_data_DLC88shuffle3.mat</p></li>
<li><p>bat_augmentation_austin_2020_bat_data_DLC89shuffle4.mat</p></li>
</ul>
<p>Not that ‚ÄúDLC‚Äù here refers to the scorer for the training data, so if you used the scorer name ‚ÄúStephinMerritt‚Äù, your shuffle1 file name could be ‚Äúcrocodiles_galloping_2023_StephinMerritt95shuffle1.mat‚Äù.</p>
<p>Anyway, we see that there are numbers between the scorer name and the shuffle name, these represent the fraction of the training data compared to the training data plus the testing data.</p>
<p>So for shuffle 1, of the data used, 80% is used as training data, for shuffle 2, that number is instead 84%, and so on. These numbers are called training fractions.</p>
<p>In config.yaml, there is a property called <code class="docutils literal notranslate"><span class="pre">TrainingFraction</span></code>. This is a list of the training fractions used in the project. In this project, that list needs to be changed to look like this:</p>
<p><img alt="TrainingFraction" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950855466-W5BOH6B4JJCO4M6RNVFG/config002_TrainingFraction.png?format=1000w" /></p>
<p>When we then reference a shuffle, we also need to reference the corresponding training fraction, this is used by specifying the parameter <code class="docutils literal notranslate"><span class="pre">trainingsetindex</span></code>, and note that we start counting from zero. So for each of our shuffles, the index of the training fraction is one smaller than the shuffle number.</p>
<p>With all that clarified, let‚Äôs now create our first project subfolder/modelprefix.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;

# we also need the package os for folder manipulation
import os
# and shutil for copying files
import shutil

#import tools for reading our config file
from deeplabcut.utils.auxiliaryfunctions import read_config

# Number and name for our model folder
model_number = 0
modelprefix_pre = &#39;data_augm&#39;
daug_str = &#39;base&#39;

# Get config as dict and associated paths
cfg = read_config(config_path)
project_path = cfg[&quot;project_path&quot;] # or: os.path.dirname(config_path) #dlc_models_path = os.path.join(project_path, &quot;dlc-models&quot;)
training_datasets_path = os.path.join(project_path, &quot;training-datasets&quot;)

# Define shuffles
shuffles = [1,2,3,4]
trainingsetindices = [0, 1, 2, 3]

# Get train and test pose config file paths from base project, for each shuffle
list_base_train_pose_config_file_paths = []
list_base_test_pose_config_file_paths = []
for shuffle_number, trainingsetindex in zip(shuffles, trainingsetindices):
    base_train_pose_config_file_path_TEMP,\
    base_test_pose_config_file_path_TEMP,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle_number,
                                            trainingsetindex=trainingsetindex)  # base_train_pose_config_file
    list_base_train_pose_config_file_paths.append(base_train_pose_config_file_path_TEMP)
    list_base_test_pose_config_file_paths.append(base_test_pose_config_file_path_TEMP)

# Create subdirs for this augmentation method
model_prefix = &#39;_&#39;.join([modelprefix_pre, &quot;{0:0=2d}&quot;.format(model_number), daug_str]) # modelprefix_pre = aug_
aug_project_path = os.path.join(project_path, model_prefix)
aug_dlc_models = os.path.join(aug_project_path, &quot;dlc-models&quot;, )

# make the folder for this modelprefix
try:
    os.mkdir(aug_project_path)
except OSError as error:
    print(error)
    print(&quot;Skipping this one as it already exists&quot;)

# Copy base train pose config file to the directory of this augmentation method
for j, (shuffle, trainingsetindex) in enumerate(zip(shuffles,trainingsetindices)):
    one_train_pose_config_file_path,\
    one_test_pose_config_file_path,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)
    
    # make train and test directories for this subdir
    os.makedirs(str(os.path.dirname(one_train_pose_config_file_path))) # create parentdir &#39;train&#39;
    os.makedirs(str(os.path.dirname(one_test_pose_config_file_path))) # create parentdir &#39;test
    
    # copy test and train config from base project to this subdir
    # copy base train config file
    shutil.copyfile(list_base_train_pose_config_file_paths[j],
                        one_train_pose_config_file_path) 
    # copy base test config file
    shutil.copyfile(list_base_test_pose_config_file_paths[j],
                        one_test_pose_config_file_path)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="edit-pose-cfg-yaml">
<h2>Edit pose_cfg.yaml<a class="headerlink" href="#edit-pose-cfg-yaml" title="Permalink to this headline">#</a></h2>
<p>We will also edit the settings for this network to use the adam optimizer.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
model_prefix = &#39;data_augm_00_base&#39;

## Initialise dict with additional edits to train config: optimizer
train_edits_dict = {}
dict_optimizer = {&#39;optimizer&#39;:&#39;adam&#39;,
    &#39;batch_size&#39;: 8, # the gpu I&#39;m using has plenty of memory so batch size 8 makes sense
    &#39;multi_step&#39;: [[1e-4, 7500], [5 * 1e-5, 12000], [1e-5, 150000]]} # if no yaml file passed, initialise as an empty dict
train_edits_dict.update({&#39;optimizer&#39;: dict_optimizer[&#39;optimizer&#39;], #&#39;adam&#39;,
    &#39;batch_size&#39;: dict_optimizer[&#39;batch_size&#39;],
    &#39;multi_step&#39;: dict_optimizer[&#39;multi_step&#39;]})

for shuffle, trainingsetindex in zip(shuffles,trainingsetindices):
    one_train_pose_config_file_path,\
    _,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)

    edit_config(str(one_train_pose_config_file_path), train_edits_dict)
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-networks">
<h2>Train networks<a class="headerlink" href="#train-networks" title="Permalink to this headline">#</a></h2>
<p>Now we need to train our four shuffles. This will take several hours.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut
# define config path and model prefix
config_path=&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;
model_prefix = &#39;data_augm_00_base&#39;

# the computer I&#39;m working on has several gpus, here I used the third one.
gputouse=3

# define shuffles and trainingsetindices
shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

# loop over shuffles and train each
for shuffle, trainingsetindex in zip(shuffles, trainingsetindices):
    deeplabcut.train_network(
        config_path,
        shuffle=shuffle,
        modelprefix=model_prefix,
        gputouse=gputouse,
        trainingsetindex=trainingsetindex,
        max_snapshots_to_keep=3, # training for 150000 iterations so let&#39;s save 50, 100, and 150.
        saveiters=50000
    )
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="perform-evaluation">
<h1>Perform evaluation<a class="headerlink" href="#perform-evaluation" title="Permalink to this headline">#</a></h1>
<p>After training the networks, we need to test how they perform. To do that, we first need to evaluate them using the buil-in DeepLabCut method <code class="docutils literal notranslate"><span class="pre">evaluate_network</span></code>. By default this will evaluate the last snapshot, in our case, this means that the network will be evaluated after 150k iterations, but we want to know how well it does at 50k and 100k too, so let‚Äôs edit our config file to test all saved snapshot.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &#39;/home/juser/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;

from deeplabcut.utils.auxiliaryfunctions import read_config, edit_config

edit_config(config_path,{&#39;snapshotindex&#39;:&#39;all&#39;})
</pre></div>
</div>
</div>
</div>
<p>Now we can evaluate the all three snapshots of all three shuffles. Remember that each shuffle has a different <code class="docutils literal notranslate"><span class="pre">trainingsetindex</span></code>, so we need to perform the evaluations in a loop.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut
config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix = &#39;data_augm_00_base&#39;
Shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

for shuffle, trainingsetindex in zip(Shuffles,trainingsetindices):
    deeplabcut.evaluate_network(config_path, modelprefix = model_prefix, Shuffles = [shuffle], trainingsetindex=trainingsetindex)
</pre></div>
</div>
</div>
</div>
<section id="get-data-for-plotting-and-statistical-comparison">
<h2>Get data for plotting and statistical comparison<a class="headerlink" href="#get-data-for-plotting-and-statistical-comparison" title="Permalink to this headline">#</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">evaluate_network</span></code> function calculates the euclidean distances between predicted points and groundtruth (i.e. human labels), but it only shows the average for that snapshot and shuffle over all the test data. What we want are long lists of the distances between ground truth points and predicted points. That way, on top of calculating the average, we can also perform stastistical testing.</p>
<p>We will use a function I wrote called <code class="docutils literal notranslate"><span class="pre">getErrorDistribution</span></code> to get the list mentioned above. We will do this for each shuffle.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix = &#39;data_augm_00_base&#39;
Shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

# We need pandas for creatig a nice list to parse
import pandas as pd

import sys
sys.path.append(&#39;..&#39;) #my python file for this function is stored in the parent folder as I&#39;m running this
from getErrorDistribution import getErrorDistribution #import the getErrorDistribution function
import numpy as np

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# get test indices
test_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

error_distributions = []
error_distributions_pcut = []

for shuffle, trainFractionIndex in zip(Shuffles,trainingsetindices):
   error_distributions_temp = []
   error_distributions_pcut_temp = []
   for snapshot in [0,1,2]: #we saved three snapshots, one at 50k iteratinos, one at 100k, and one at 150k
      (
            ErrorDistribution_all,
            ErrorDistribution_test,
            ErrorDistribution_train,
            ErrorDistributionPCutOff_all,
            _,
            _
      )  = getErrorDistribution(
            config_path,
            shuffle=shuffle,
            snapindex=snapshot,
            trainFractionIndex = trainFractionIndex,
            modelprefix = model_prefix
      )
      error_distributions_temp.append(ErrorDistribution_all.iloc[test_inds].values.flatten())
      error_distributions_pcut_temp.append(ErrorDistributionPCutOff_all.iloc[test_inds].values.flatten())
   error_distributions.append(error_distributions_temp)
   error_distributions_pcut.append(error_distributions_pcut_temp)
error_distributionsb = np.array(error_distributions) # array with dimensions [shuffle, snapshot, frames]
error_distributions_pcut = np.array(error_distributions_pcut) # array with dimensions [shuffle, snapshot, frames]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt

fig, (ax1,ax2) = plt.subplots(1,2)
fig.set_figheight(8)
fig.set_figwidth(10)

for shuffle in [0,1,2,3]: #we start counting at 0, so for now, let&#39;s consider each index one less
    ax1.errorbar(np.array([50,100,150])-1.5+shuffle,np.nanmean(error_distributions[shuffle,:],axis=1), np.nanstd(error_distributions[shuffle,:],axis=1)/len(test_inds)**.5)
    ax2.errorbar(np.array([50,100,150])-1.5+shuffle,np.nanmean(error_distributions_pcut[shuffle,:],axis=1), np.nanstd(error_distributions_pcut[shuffle,:],axis=1)/len(test_inds)**.5)

ax1.set_xticks([50, 100, 150])
ax2.set_xticks([50, 100, 150])
ax1.set_xlim([25,175])
ax2.set_xlim([25,175])
ax1.set_ylim([0,23])
ax2.set_ylim([0,23])
ax1.set_title(&quot;Without P-cut&quot;)
ax2.set_title(&quot;With P-cut 0.6&quot;)
ax2.set_yticklabels([])
ax2.legend([&quot;half, OOD&quot;,&quot;half, Ref&quot;,&quot;full, OOD&quot;, &quot;full, Ref&quot;])

# add a big axis, hide frame
fig.add_subplot(111, frameon=False)
## hide tick and tick label of the big axis
plt.tick_params(labelcolor=&#39;none&#39;, which=&#39;both&#39;, top=False, bottom=False, left=False, right=False)
plt.xlabel(&quot;Iterations (thousands)&quot;)
plt.ylabel(&quot;Error (px)&quot;)

</pre></div>
</div>
</div>
</div>
<p><img alt="download.png" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950856669-RVS6GMR4411BUVXY74FN/pre_aug_performance.png?format=750w" /></p>
</section>
<section id="interpret-plot">
<h2>Interpret plot<a class="headerlink" href="#interpret-plot" title="Permalink to this headline">#</a></h2>
<p>Just looking at the plot, a few things seem clear, let‚Äôs start with the obvious and least surprising. By training a network on a subset of frames of the videos to be analyzed, we really improve accuracy. When doing this, i.e. when refining, increasing the amount of training data, i.e. frames not belonging to the videos to be analyzed, has little effect on accuracy.</p>
<p>But surprisingly, when not refining, it appears that increasing the amount of training data can worsen performance. What‚Äôs going on? We will get back to this later, but for now, let‚Äôs just confirm my points here with some statistical testing.</p>
<p>For the statistical comparison, we‚Äôll use a paired Wilcoxon ranked sum test, since we can‚Äôt assume the data to be normally distributed. We‚Äôll do this for the set of errors with P-cut filter applied since doing it on both would be tedious and I assume most users will use some form of P-cut on their data. Furthermore, I will do the statistical comparison only on  the last snapshot since, again, doing it on all three would be tedious.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import numpy as np
from scipy.stats import wilcoxon

p_value = np.empty((4,4))
p_value[:]=np.NaN

for i in [0,1,2,3]:
    for j in [0,1,2,3]:
        if j&lt;=i: continue
        _, p_value[i,j] = wilcoxon(x = error_distributions_pcut[i,-1,:], y = error_distributions_pcut[j,-1,:],nan_policy=&#39;omit&#39;)

print(p_value)
</pre></div>
</div>
</div>
</div>
</section>
<section id="plot-per-video">
<h2>Plot per video<a class="headerlink" href="#plot-per-video" title="Permalink to this headline">#</a></h2>
<p>The statistics are in line with expectations, except for regarding the difference between the two refined shuffles (shuffle2: ‚Äúhalf, ref‚Äù and shuffle4: ‚Äúfull, ref‚Äù). The two of them being significantly different surprises me and makes me want to investigate the error distributions more in detail.</p>
<p>To that end, let‚Äôs plot the error distributions of the four shuffles per camera and per video number. Remember, out seven test video triplets each consist of three videos depicting the same bat flight.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix = &#39;data_augm_00_base&#39;
Shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

# We need pandas for creatig a nice list to parse
import pandas as pd

import sys
sys.path.append(&#39;..&#39;) #my python file for this function is stored in the parent folder as I&#39;m running this
from getErrorDistribution import getErrorDistribution #import the getErrorDistribution function
import numpy as np

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# get test indices
test_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

# this gives us the paths of our 27 test videos
test_paths = list(set([image_paths[i][1] for i in test_inds]))

#%% sorted so that the corresponding videos have the same index in three lists (one per camera)
test_paths_cam1 =   [&#39;TS5-544-Cam1_2020-06-25_000099Track8_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000103Track3_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000104Track3_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000108Track6_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000123Track6_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000128Track2_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000134Track5_50_test&#39;
                    ]
test_paths_cam2 =   [&#39;IL5-519-Cam2_2020-06-25_000099Track6_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000103Track3_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000104Track2_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000109Track1_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000124Track9_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000130Track2_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000136Track10_50_test&#39;
                    ]
test_paths_cam3 =   [&#39;IL5-534-Cam3_2020-06-25_000095Track14_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000100Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000101Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000106Track3_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000122Track7_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000127Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000133Track9_50_test&#39;
                    ]

nvideos = 7 # number of videos

# get test frame indexes per camera
test_inds_cam1 = [[],[],[],[],[],[],[]]
test_inds_cam2 = [[],[],[],[],[],[],[]]
test_inds_cam3 = [[],[],[],[],[],[],[]]

for i, path in enumerate(image_paths):
    for j in range(nvideos):
        if str(path[1]).__eq__(test_paths_cam1[j]):
            test_inds_cam1[j].append(i)
        elif str(path[1]).__eq__(test_paths_cam2[j]):
            test_inds_cam2[j].append(i)
        elif str(path[1]).__eq__(test_paths_cam3[j]):
            test_inds_cam3[j].append(i)

nshuffles = len(Shuffles)

#pre-allocate matrixes for mean values and standard errors
mean_cam1 = np.zeros([nshuffles,nvideos]) # shuffle x movie
mean_cam2 = np.zeros([nshuffles,nvideos])
mean_cam3 = np.zeros([nshuffles,nvideos])

ste_cam1 = np.zeros([nshuffles,nvideos])
ste_cam2 = np.zeros([nshuffles,nvideos])
ste_cam3 = np.zeros([nshuffles,nvideos])

meanPcut_cam1 = np.zeros([nshuffles,nvideos]) # shuffle x movie
meanPcut_cam2 = np.zeros([nshuffles,nvideos])
meanPcut_cam3 = np.zeros([nshuffles,nvideos])

stePcut_cam1 = np.zeros([nshuffles,nvideos])
stePcut_cam2 = np.zeros([nshuffles,nvideos])
stePcut_cam3 = np.zeros([nshuffles,nvideos])

# %%

for i, shuffle in enumerate(Shuffles):    
    trainFractionIndex = i
    snapshot=-1
    (
        ErrorDistribution_all,
        _,
        _,
        ErrorDistributionPCutOff_all,
        _,
        _
    )  = getErrorDistribution(
        config_path,
        shuffle=shuffle,
        snapindex=snapshot,
        trainFractionIndex = trainFractionIndex,
        modelprefix = model_prefix
    )
    for movie_number in range(7):

        meanPcut_cam1[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])
        stePcut_cam1[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam1[movie_number]][:].size**.5)

        meanPcut_cam2[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam2[movie_number]][:])
        stePcut_cam2[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam2[movie_number]][:].size**.5)

        meanPcut_cam3[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam3[movie_number]][:])
        stePcut_cam3[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam3[movie_number]][:].size**.5)

fig, (ax1,ax2,ax3) = plt.subplots(3,1)
fig.set_figheight(15)
fig.set_figwidth(10)
for i, shuffle in enumerate(Shuffles):
    
    # to jitter the error bars to keep  them from overlapping
    movie_number = list(range(1,8))
    movie_number = [x - 2/50 + shuffle/50 for x in movie_number]
    
    ax1.errorbar(movie_number,meanPcut_cam1[i,:], stePcut_cam1[i,:,])

    ax2.errorbar(movie_number,meanPcut_cam2[i,:], stePcut_cam2[i,:,])

    ax3.errorbar(movie_number,meanPcut_cam3[i,:], stePcut_cam3[i,:,])

ax1.set_ylim([0, 50])
ax2.set_ylim([0, 50])
ax3.set_ylim([0, 50])
ax1.set_title(&quot;Cam 1&quot;)
ax2.set_title(&quot;Cam 2&quot;)
ax3.set_title(&quot;Cam 3&quot;)
ax1.set_ylabel(&quot;Error (px&quot;)
ax2.set_ylabel(&quot;Error (px&quot;)
ax3.set_ylabel(&quot;Error (px&quot;)
ax1.legend([&quot;half, OOD&quot;,&quot;half, Ref&quot;,&quot;full, OOD&quot;, &quot;full, Ref&quot;])
</pre></div>
</div>
</div>
</div>
<p><img alt="download2.png" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950857302-7FYQ87AQ5OKGV6D8D9DF/pre_aug_per_video.png?format=500w" /></p>
</section>
<section id="interpret-per-video-plot">
<h2>Interpret per video plot<a class="headerlink" href="#interpret-per-video-plot" title="Permalink to this headline">#</a></h2>
<p>The first thing that becomes clear to me from inspecting this plot is that there are some videos on which all shuffles perform well no matter refining. These would be video 3, 6 and 7. The second thing is that there are instances where ‚Äúhalf, ref‚Äù does better than ‚Äúfull, ref‚Äù and vice versa, which could explain why they are significantly different according to the statistical test performed above, despite their overall means being relatively similar according to the plot of the mean errors on the full error distributions.</p>
</section>
<section id="compare-easy-and-difficult-videos">
<h2>Compare ‚Äúeasy‚Äù and ‚Äúdifficult‚Äù videos<a class="headerlink" href="#compare-easy-and-difficult-videos" title="Permalink to this headline">#</a></h2>
<p>For shuffle3, ‚Äúfull, OOD‚Äù, video 1, 2, 4, and 5 does poorly whereas video 3, 6, and 7 do rather well. Why is this? To get an idea of if the videos on which the network does well differs from the one on which it does poorly, we can create labeled frames with the predicted positions of the bodyparts and the correct positions (human labels). This is done with the <code class="docutils literal notranslate"><span class="pre">evaluate_network</span></code> function using the <code class="docutils literal notranslate"><span class="pre">plotting</span></code> input parameter.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &#39;/home/jusers/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;
model_prefix = &#39;data_augm_00_base&#39;

# we only want to plot the last snapshot (150k iterations)
from deeplabcut.utils.auxiliaryfunctions import read_config, edit_config

edit_config(config_path,{&#39;snapshotindex&#39;:-1})

shuffle = 3
trainingsetindex = 2

deeplabcut.evaluate_network(config_path, modelprefix = model_prefix, Shuffles = [shuffle], trainingsetindex=trainingsetindex, plotting=True)
</pre></div>
</div>
</div>
</div>
</section>
<section id="inspect-deeplabcut-predictions-from-evaluations">
<h2>Inspect DeepLabCut predictions from evaluations<a class="headerlink" href="#inspect-deeplabcut-predictions-from-evaluations" title="Permalink to this headline">#</a></h2>
<p>Inspecting and comparing frames reveal an interesting relationship. It appears the network does well when the bat is flying left-right, and bad when the bat is flying right to left.</p>
<p>The two following example images reveal what I mean.</p>
<p><img alt="&quot;bad&quot;" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950853195-EBZD23OVFO0NUSW94CUT/Test-IL5-519-Cam2_2020-06-25_000103Track3_50_test-103_103_100_AS_c2_frame200.png?format=500w" /></p>
<p><em>The bat is flying from the right to the left, and the network has mistaken the left side of the bat for the right side.</em></p>
<p><img alt="&quot;good&quot;" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950854190-2VRZO69FEZXJ7GZGFV6J/Test-IL5-519-Cam2_2020-06-25_000130Track2_50_test-128_130_127_AS_c2_frame300.png?format=500w" /></p>
<p>*The bat is flying from the left to the right, and the network is accurately predicting all points.</p>
<p>The inspection of the predicted bodypart locations reveal that the low accuracy for some videos is caused by the network mixing up left and right. This could also explain why the performance goes down when we add more data. It appears that the added data results in the network becoming biased towards one flight direction.</p>
</section>
<section id="augment-to-reduce-left-right-bias">
<h2>Augment to reduce left-right bias<a class="headerlink" href="#augment-to-reduce-left-right-bias" title="Permalink to this headline">#</a></h2>
<p>Our evaluation showed that our network has a bias towards bats flying in one direction. In an attempt to reduce that bias and make the network better at telling left from right independent of flight direction, we will use a left-right flipping augmentaton.</p>
<p>First up, let‚Äôs make another modelprefix folder in our project. It basically the same code as before but with some minor modifications, I‚Äôll highlight the changes.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;

# we also need the package os for folder manipulation
import os
# and shutil for copying files
import shutil

#import tools for reading our config file
from deeplabcut.utils.auxiliaryfunctions import read_config

# Number and name for our model folder
model_number = 1 # CHANGE
modelprefix_pre = &#39;data_augm&#39;
daug_str = &#39;fliplr&#39; # CHANGE

# Get config as dict and associated paths
cfg = read_config(config_path)
project_path = cfg[&quot;project_path&quot;] # or: os.path.dirname(config_path) #dlc_models_path = os.path.join(project_path, &quot;dlc-models&quot;)
training_datasets_path = os.path.join(project_path, &quot;training-datasets&quot;)

# Define shuffles
shuffles = [1,2,3,4]
trainingsetindices = [0, 1, 2, 3]

# Get train and test pose config file paths from base project, for each shuffle
list_base_train_pose_config_file_paths = []
list_base_test_pose_config_file_paths = []
for shuffle_number, trainingsetindex in zip(shuffles, trainingsetindices):
    base_train_pose_config_file_path_TEMP,\
    base_test_pose_config_file_path_TEMP,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle_number,
                                            trainingsetindex=trainingsetindex)  # base_train_pose_config_file
    list_base_train_pose_config_file_paths.append(base_train_pose_config_file_path_TEMP)
    list_base_test_pose_config_file_paths.append(base_test_pose_config_file_path_TEMP)

# Create subdirs for this augmentation method
model_prefix = &#39;_&#39;.join([modelprefix_pre, &quot;{0:0=2d}&quot;.format(model_number), daug_str]) # modelprefix_pre = aug_
aug_project_path = os.path.join(project_path, model_prefix)
aug_dlc_models = os.path.join(aug_project_path, &quot;dlc-models&quot;, )

# make the folder for this modelprefix
try:
    os.mkdir(aug_project_path)
except OSError as error:
    print(error)
    print(&quot;Skipping this one as it already exists&quot;)

# Copy base train pose config file to the directory of this augmentation method
for j, (shuffle, trainingsetindex) in enumerate(zip(shuffles,trainingsetindices)):
    one_train_pose_config_file_path,\
    one_test_pose_config_file_path,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)
    
    # make train and test directories for this subdir
    os.makedirs(str(os.path.dirname(one_train_pose_config_file_path))) # create parentdir &#39;train&#39;
    os.makedirs(str(os.path.dirname(one_test_pose_config_file_path))) # create parentdir &#39;test
    
    # copy test and train config from base project to this subdir
    # copy base train config file
    shutil.copyfile(list_base_train_pose_config_file_paths[j],
                        one_train_pose_config_file_path) 
    # copy base test config file
    shutil.copyfile(list_base_test_pose_config_file_paths[j],
                        one_test_pose_config_file_path)
</pre></div>
</div>
</div>
</div>
</section>
<section id="edit-pose-cfg-yaml-for-fliplr-augmentation">
<h2>Edit pose_cfg.yaml for fliplr augmentation<a class="headerlink" href="#edit-pose-cfg-yaml-for-fliplr-augmentation" title="Permalink to this headline">#</a></h2>
<p>As before, we will edit the settings for this network to use the adam optimizer. More importantly, though, we will apply the <code class="docutils literal notranslate"><span class="pre">fliplr</span></code> augmentation.</p>
<p>This randomly flips some training frames left and right during training. Now, this alone would lead to worse performance, since we would be training the network to think that the left wing is the right wing, and vice versa. But this augmentation method takes that into account, and in addition to mirroring the image, it also swaps the labels from left to right and vice versa. To accomlish this, we need to supply the pose_cfg.yaml file with a list of which points should be swapped.</p>
<p>The image of a bat outline above shows he locations of the labels/bodyparts on the bats, and from it, it‚Äôs clear which points should be swapped, all points that aren‚Äôt on the bilateral have a name and end in eithr R or L. So t3R should be swapped with t3L when flipping, and so on.</p>
<p>The labels are these, and in this order:
0. ‚Äòt3L‚Äô</p>
<ol class="simple">
<li><p>‚ÄòwstL‚Äô</p></li>
<li><p>‚Äòt5L‚Äô</p></li>
<li><p>‚ÄòelbL‚Äô</p></li>
<li><p>‚ÄòshdL‚Äô</p></li>
<li><p>‚ÄòankL‚Äô</p></li>
<li><p>‚Äònl‚Äô</p></li>
<li><p>‚Äòstr‚Äô</p></li>
<li><p>‚Äòlmb‚Äô</p></li>
<li><p>‚ÄòshdR‚Äô</p></li>
<li><p>‚ÄòankR‚Äô</p></li>
<li><p>‚ÄòelbR‚Äô</p></li>
<li><p>‚ÄòwstR‚Äô</p></li>
<li><p>‚Äòt5R‚Äô</p></li>
<li><p>‚Äòt3R‚Äô</p></li>
<li><p>‚Äòtail‚Äô</p></li>
</ol>
<p>So the symmetric pairs are the following:</p>
<ul class="simple">
<li><p>t3:   (0, 14)</p></li>
<li><p>wst:  (1, 12)</p></li>
<li><p>t5:   (2, 13)</p></li>
<li><p>elb:  (3, 11)</p></li>
<li><p>shd:  (4, 9)</p></li>
<li><p>ank:  (5, 10)</p></li>
</ul>
<p>Now, let‚Äôs edit the <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> accordingly.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#import tools for changing our config file
from deeplabcut.utils.auxiliaryfunctions import edit_config

model_prefix = &#39;data_augm_01_fliplr&#39;

## Initialise dict with additional edits to train config: optimizer
train_edits_dict = {}
dict_optimizer = {&#39;optimizer&#39;:&#39;adam&#39;,
    &#39;batch_size&#39;: 8, # the gpu I&#39;m using has plenty of memory so batch size 8 makes sense
    &#39;multi_step&#39;: [[1e-4, 7500], [5 * 1e-5, 12000], [1e-5, 150000]]} # if no yaml file passed, initialise as an empty dict
train_edits_dict.update({&#39;optimizer&#39;: dict_optimizer[&#39;optimizer&#39;], #&#39;adam&#39;,
    &#39;batch_size&#39;: dict_optimizer[&#39;batch_size&#39;],
    &#39;multi_step&#39;: dict_optimizer[&#39;multi_step&#39;]})

# Augmentation edits
edits_dict = dict()
edits_dict[&quot;symmetric_pairs&quot;] = (0, 14), (1, 12), (2, 13), (3, 11), (4, 9), (5, 10)
edits_dict[&quot;fliplr&quot;] = True

for shuffle, trainingsetindex in zip(shuffles,trainingsetindices):
    one_train_pose_config_file_path,\
    _,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)

    edit_config(str(one_train_pose_config_file_path), edits_dict)
    edit_config(str(one_train_pose_config_file_path), train_edits_dict)
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-network-with-fliplr-augmentation">
<h2>Train network with <code class="docutils literal notranslate"><span class="pre">fliplr</span></code> augmentation<a class="headerlink" href="#train-network-with-fliplr-augmentation" title="Permalink to this headline">#</a></h2>
<p>Now let‚Äôs train the network with the new augmentation. Again, this will take several hours.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut
# define config path and model prefix
config_path=&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;
model_prefix = &#39;data_augm_01_flipr&#39;

# the computer I&#39;m working on has several gpus, here I used the third one.
gputouse=3

# define shuffles and trainingsetindices
shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

# loop over shuffles and train each
for shuffle, trainingsetindex in zip(shuffles, trainingsetindices):
    deeplabcut.train_network(
        config_path,
        shuffle=shuffle,
        modelprefix=model_prefix,
        gputouse=gputouse,
        trainingsetindex=trainingsetindex,
        max_snapshots_to_keep=3, # training for 150000 iterations so let&#39;s save 50, 100, and 150.
        saveiters=50000
    )
</pre></div>
</div>
</div>
</div>
</section>
<section id="compare-performance-of-fliplr-augmented-network-to-refined-baseline">
<h2>Compare performance of fliplr augmented network to refined baseline<a class="headerlink" href="#compare-performance-of-fliplr-augmented-network-to-refined-baseline" title="Permalink to this headline">#</a></h2>
<p>Since the challenge I set is <strong>to use augmentation to match or beat the accuracy of refining</strong>, let‚Äôs stick to comparing baseline ‚ÄúFull, ref‚Äù to fliplr augmented ‚ÄúFull, OOD‚Äù.</p>
<p>The process is similar to what we‚Äôve already done, but now we are only comparing two networks.</p>
<p>We will use <code class="docutils literal notranslate"><span class="pre">evaluate_network</span></code> to get create accuracy data, then we will use <code class="docutils literal notranslate"><span class="pre">getErrorDistribution</span></code> to access that data and plot the average performance with error bars. Same as before.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix = &#39;data_augm_01_fliplr&#39;
Shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

#import tools for modifying our config file
from deeplabcut.utils.auxiliaryfunctions import edit_config

# make sure we are testing all snapshots
edit_config(config_path,{&#39;snapshotindex&#39;:&#39;all&#39;})

for shuffle, trainingsetindex in zip(Shuffles,trainingsetindices):
    deeplabcut.evaluate_network(config_path, modelprefix = model_prefix, Shuffles = [shuffle], trainingsetindex=trainingsetindex)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix_base = &#39;data_augm_00_base&#39;
model_prefix_augm = &#39;data_augm_01_fliplr&#39;
Shuffles = [4,3] # let&#39;s start with the refined un-augmented, i.e. shuffle 4
trainingsetindices = [3,2]

# We need pandas for creatig a nice list to parse
import pandas as pd

import sys
sys.path.append(&#39;..&#39;) #my python file for this function is stored in the parent folder as I&#39;m running this
from getErrorDistribution import getErrorDistribution #import the getErrorDistribution function
import numpy as np

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# get test indices
test_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

error_distributions_pcut = []

for shuffle, trainFractionIndex in zip(Shuffles,trainingsetindices):
   error_distributions_pcut_temp = []
   if shuffle == 4: model_prefix = model_prefix_base
   elif shuffle == 3: model_prefix = model_prefix_augm
   
   for snapshot in [0,1,2]: #we saved three snapshots, one at 50k iteratinos, one at 100k, and one at 150k
      (
            _,
            _,
            _,
            ErrorDistributionPCutOff_all,
            _,
            _
      )  = getErrorDistribution(
            config_path,
            shuffle=shuffle,
            snapindex=snapshot,
            trainFractionIndex = trainFractionIndex,
            modelprefix = model_prefix
      )
      error_distributions_pcut_temp.append(ErrorDistributionPCutOff_all.iloc[test_inds].values.flatten())
   error_distributions_pcut.append(error_distributions_pcut_temp)

error_distributions_pcut = np.array(error_distributions_pcut) # array with dimensions [shuffle, snapshot, frames]

import matplotlib.pyplot as plt
plt.figure(figsize=(10, 5))
for shuffle in [0,1]: #we start counting at 0, so for now, let&#39;s consider each index one less
    plt.errorbar(np.array([50,100,150])-1.5+shuffle,np.nanmean(error_distributions_pcut[shuffle,:],axis=1), np.nanstd(error_distributions_pcut[shuffle,:],axis=1)/len(test_inds)**.5)

plt.xticks([50, 100, 150])
plt.xlim([25,175])
plt.ylim([0,10])
plt.title(&quot;Error with P-cut 0.6, comparing baseline to fliplr and 180 degrees rotation augmented&quot;)

plt.legend([&quot;Full, ref, baseline&quot;, &quot;Full, OOD, fliplr&quot;])

plt.xlabel(&quot;Iterations (thousands)&quot;)
plt.ylabel(&quot;Error (px)&quot;)
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpret-performance-of-network-after-fliplr-augmentation">
<h2>Interpret performance of network after ‚Äúfliplr‚Äù augmentation<a class="headerlink" href="#interpret-performance-of-network-after-fliplr-augmentation" title="Permalink to this headline">#</a></h2>
<p>That is a significant improvement in accuracy. The <code class="docutils literal notranslate"><span class="pre">fliplr</span></code> augmentation seems to prevent the network from becoming biased to one flight direction. It is however, not quite beating the performance of the baseline refined case (shuffle4: full, Ref).</p>
<p>There is another augmentation that, in our case, is related to the flight direction, namely the rotation augmentation. The idea here is that in order to prevent the network from associating absolute in-image spatial orientation with positions for certain labels, we want to randomly rotate some of the training data. The degree to which this makes sense is context dependent. If you are analyzing a horse walking in a horizontal direction from the camera‚Äôs point of view, large degrees of rotation make little sense as the network will never encounter a horse walking upside down. But if you are filming an animal from above, or below if the animal is flying or swimming, such that the animal is free to move in any direction, then this type of augmentation makes sense.</p>
<p>By default, DeepLabCut applies a 25¬∞ rotation augmentation, I tried increasing that, first to 90¬∞, then to 180¬∞. I will only cover the 180¬∞ case here as the 90¬∞ case is exactly the same, only with slightly worse performance.</p>
<p>All the steps taken are the same as before.</p>
<ul class="simple">
<li><p>Create the model prefix folder</p></li>
<li><p>Change the <code class="docutils literal notranslate"><span class="pre">pose_cfg.yaml</span></code> file to apply the augmentation</p></li>
<li><p>Train the network</p></li>
<li><p>Evaluate the network performance</p></li>
<li><p>Plot results</p></li>
</ul>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
# in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;

# we also need the package os for folder manipulation
import os
# and shutil for copying files
import shutil

#import tools for reading our config file
from deeplabcut.utils.auxiliaryfunctions import read_config

# Number and name for our model folder
model_number = 3 # CHANGE
modelprefix_pre = &#39;data_augm&#39;
daug_str = &#39;max_rotate&#39; # CHANGE

# Get config as dict and associated paths
cfg = read_config(config_path)
project_path = cfg[&quot;project_path&quot;] # or: os.path.dirname(config_path) #dlc_models_path = os.path.join(project_path, &quot;dlc-models&quot;)
training_datasets_path = os.path.join(project_path, &quot;training-datasets&quot;)

# Define shuffles
shuffles = [1,2,3,4]
trainingsetindices = [0, 1, 2, 3]

# Get train and test pose config file paths from base project, for each shuffle
list_base_train_pose_config_file_paths = []
list_base_test_pose_config_file_paths = []
for shuffle_number, trainingsetindex in zip(shuffles, trainingsetindices):
    base_train_pose_config_file_path_TEMP,\
    base_test_pose_config_file_path_TEMP,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle_number,
                                            trainingsetindex=trainingsetindex)  # base_train_pose_config_file
    list_base_train_pose_config_file_paths.append(base_train_pose_config_file_path_TEMP)
    list_base_test_pose_config_file_paths.append(base_test_pose_config_file_path_TEMP)

# Create subdirs for this augmentation method
model_prefix = &#39;_&#39;.join([modelprefix_pre, &quot;{0:0=2d}&quot;.format(model_number), daug_str]) # modelprefix_pre = aug_
aug_project_path = os.path.join(project_path, model_prefix)
aug_dlc_models = os.path.join(aug_project_path, &quot;dlc-models&quot;, )

# make the folder for this modelprefix
try:
    os.mkdir(aug_project_path)
except OSError as error:
    print(error)
    print(&quot;Skipping this one as it already exists&quot;)

# Copy base train pose config file to the directory of this augmentation method
for j, (shuffle, trainingsetindex) in enumerate(zip(shuffles,trainingsetindices)):
    one_train_pose_config_file_path,\
    one_test_pose_config_file_path,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)
    
    # make train and test directories for this subdir
    os.makedirs(str(os.path.dirname(one_train_pose_config_file_path))) # create parentdir &#39;train&#39;
    os.makedirs(str(os.path.dirname(one_test_pose_config_file_path))) # create parentdir &#39;test
    
    # copy test and train config from base project to this subdir
    # copy base train config file
    shutil.copyfile(list_base_train_pose_config_file_paths[j],
                        one_train_pose_config_file_path) 
    # copy base test config file
    shutil.copyfile(list_base_test_pose_config_file_paths[j],
                        one_test_pose_config_file_path)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>#import tools for changing our config file
from deeplabcut.utils.auxiliaryfunctions import edit_config

model_prefix = &#39;data_augm_03_max_rotate&#39;

## Initialise dict with additional edits to train config: optimizer
train_edits_dict = {}
dict_optimizer = {&#39;optimizer&#39;:&#39;adam&#39;,
    &#39;batch_size&#39;: 8, # the gpu I&#39;m using has plenty of memory so batch size 8 makes sense
    &#39;multi_step&#39;: [[1e-4, 7500], [5 * 1e-5, 12000], [1e-5, 150000]]} # if no yaml file passed, initialise as an empty dict
train_edits_dict.update({&#39;optimizer&#39;: dict_optimizer[&#39;optimizer&#39;], #&#39;adam&#39;,
    &#39;batch_size&#39;: dict_optimizer[&#39;batch_size&#39;],
    &#39;multi_step&#39;: dict_optimizer[&#39;multi_step&#39;]})

# Augmentation edits
edits_dict = dict()
edits_dict[&quot;symmetric_pairs&quot;] = (0, 14), (1, 12), (2, 13), (3, 11), (4, 9), (5, 10)
edits_dict[&quot;fliplr&quot;] = True
edits_dict[&quot;rotation&quot;] = 180

for shuffle, trainingsetindex in zip(shuffles,trainingsetindices):
    one_train_pose_config_file_path,\
    _,\
    _ = deeplabcut.return_train_network_path(config_path,
                                            shuffle=shuffle,
                                            trainingsetindex=trainingsetindex,
                                            modelprefix=model_prefix)

    edit_config(str(one_train_pose_config_file_path), edits_dict)
    edit_config(str(one_train_pose_config_file_path), train_edits_dict)
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-network-with-fliplr-and-stronger-rotation-augmentation">
<h2>Train network with <code class="docutils literal notranslate"><span class="pre">fliplr</span></code> and stronger <code class="docutils literal notranslate"><span class="pre">rotation</span></code> augmentation<a class="headerlink" href="#train-network-with-fliplr-and-stronger-rotation-augmentation" title="Permalink to this headline">#</a></h2>
<p>Now let‚Äôs train the network with the new augmentation. Again, this will take several hours.</p>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import deeplabcut
# define config path and model prefix
config_path=&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&#39;
model_prefix = &#39;data_augm_03_max_rotate&#39;

# the computer I&#39;m working on has several gpus, here I used the third one.
gputouse=3

# define shuffles and trainingsetindices
shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

# loop over shuffles and train each
for shuffle, trainingsetindex in zip(shuffles, trainingsetindices):
    deeplabcut.train_network(
        config_path,
        shuffle=shuffle,
        modelprefix=model_prefix,
        gputouse=gputouse,
        trainingsetindex=trainingsetindex,
        max_snapshots_to_keep=3, # training for 150000 iterations so let&#39;s save 50, 100, and 150.
        saveiters=50000
    )
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Evaluate the network
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix = &#39;data_augm_03_max_rotate&#39;
Shuffles = [1,2,3,4]
trainingsetindices = [0,1,2,3]

#import tools for modifying our config file
from deeplabcut.utils.auxiliaryfunctions import edit_config

# make sure we are testing all snapshots
edit_config(config_path,{&#39;snapshotindex&#39;:&#39;all&#39;})

for shuffle, trainingsetindex in zip(Shuffles,trainingsetindices):
    deeplabcut.evaluate_network(config_path, modelprefix = model_prefix, Shuffles = [shuffle], trainingsetindex=trainingsetindex, gputouse=3)
</pre></div>
</div>
</div>
</div>
<div class="cell tag_hide-output docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># Get detailed error data for plotting

# in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix_base = &#39;data_augm_00_base&#39;
model_prefix_augm = &#39;data_augm_03_max_rotate&#39;
Shuffles = [4,3] # let&#39;s start with the refined un-augmented, i.e. shuffle 4
trainingsetindices = [3,2]

# We need pandas for creatig a nice list to parse
import pandas as pd

import sys
sys.path.append(&#39;..&#39;) #my python file for this function is stored in the parent folder as I&#39;m running this
from getErrorDistribution import getErrorDistribution #import the getErrorDistribution function
import numpy as np

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# get test indices
test_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

error_distributions_pcut = []

for shuffle, trainFractionIndex in zip(Shuffles,trainingsetindices):
   error_distributions_pcut_temp = []
   if shuffle == 4: model_prefix = model_prefix_base
   elif shuffle == 3: model_prefix = model_prefix_augm
   
   for snapshot in [0,1,2]: #we saved three snapshots, one at 50k iteratinos, one at 100k, and one at 150k
      (
            _,
            _,
            _,
            ErrorDistributionPCutOff_all,
            _,
            _
      )  = getErrorDistribution(
            config_path,
            shuffle=shuffle,
            snapindex=snapshot,
            trainFractionIndex = trainFractionIndex,
            modelprefix = model_prefix
      )
      error_distributions_pcut_temp.append(ErrorDistributionPCutOff_all.iloc[test_inds].values.flatten())
   error_distributions_pcut.append(error_distributions_pcut_temp)

error_distributions_pcut = np.array(error_distributions_pcut) # array with dimensions [shuffle, snapshot, frames]
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>import matplotlib.pyplot as plt

plt.figure(figsize=(10, 5))

for shuffle in [0,1]: #we start counting at 0, so for now, let&#39;s consider each index one less
    plt.errorbar(np.array([50,100,150])-1.5+shuffle,np.nanmean(error_distributions_pcut[shuffle,:],axis=1), np.nanstd(error_distributions_pcut[shuffle,:],axis=1)/len(test_inds)**.5)

plt.xticks([50, 100, 150])
plt.xlim([25,175])
plt.ylim([0,10])
plt.title(&quot;Error with P-cut 0.6, comparing baseline to fliplr and 180 degrees rotation augmented&quot;)

plt.legend([&quot;Full, ref, baseline&quot;, &quot;Full, OOD, fliplr_180_rotate&quot;])

plt.xlabel(&quot;Iterations (thousands)&quot;)
plt.ylabel(&quot;Error (px)&quot;)
</pre></div>
</div>
</div>
</div>
<p><img alt="download3.png" src="https://images.squarespace-cdn.com/content/v1/57f6d51c9f74566f55ecf271/1661950856089-7I0IXOS1G0D2YUJ9GNSB/flip_rot180.png?format=1500w" /></p>
<p>With this, it appears that we have reached our goal, which was <strong>to use augmentation to match or beat the accuracy of refining</strong>. But let‚Äôs also take a look at the accuracy per video. I will also plot the baseline, un-refined full network accuracy here to highlight how much the augmentation has improved the performance of the network.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># in case we restarted the kernel or something, let&#39;s make
# sure deeplabcut is imported and the config_path defined
import deeplabcut

config_path = &quot;/home/user/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/config.yaml&quot;
model_prefix_base = &#39;data_augm_00_base&#39;
model_prefix_augm = &#39;data_augm_03_max_rotate&#39;
Shuffles = [4,3,3]
trainingsetindices = [3,2,2]

# We need pandas for creatig a nice list to parse
import pandas as pd

import sys
sys.path.append(&#39;..&#39;) #my python file for this function is stored in the parent folder as I&#39;m running this
from getErrorDistribution import getErrorDistribution #import the getErrorDistribution function
import numpy as np

# Read the h5 file containing all the frames, (project_folder/training-datasets/iteration-0/UnaufmentedDataSet_project_folder/CollectedData_LabelerName.h5)
df = pd.read_hdf(&#39;/home/juser/projects/bat_augmentation_austin_2020_bat_data-DLC-2022-08-18/training-datasets/iteration-0/UnaugmentedDataSet_bat_augmentation_austin_2020_bat_dataAug18/CollectedData_DLC.h5&#39;)

image_paths = df.index.to_list() # turn dataframe into list

# get test indices
test_inds = []
for i, path in enumerate(image_paths):
    if str(path[1]).endswith(&quot;_50_test&quot;):
        test_inds.append(i)

# this gives us the paths of our 27 test videos
test_paths = list(set([image_paths[i][1] for i in test_inds]))

#%% sorted so that the corresponding videos have the same index in three lists (one per camera)
test_paths_cam1 =   [&#39;TS5-544-Cam1_2020-06-25_000099Track8_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000103Track3_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000104Track3_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000108Track6_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000123Track6_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000128Track2_50_test&#39;,
                    &#39;TS5-544-Cam1_2020-06-25_000134Track5_50_test&#39;
                    ]
test_paths_cam2 =   [&#39;IL5-519-Cam2_2020-06-25_000099Track6_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000103Track3_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000104Track2_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000109Track1_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000124Track9_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000130Track2_50_test&#39;,
                    &#39;IL5-519-Cam2_2020-06-25_000136Track10_50_test&#39;
                    ]
test_paths_cam3 =   [&#39;IL5-534-Cam3_2020-06-25_000095Track14_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000100Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000101Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000106Track3_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000122Track7_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000127Track4_50_test&#39;,
                    &#39;IL5-534-Cam3_2020-06-25_000133Track9_50_test&#39;
                    ]

nvideos = 7 # number of videos

# get test frame indexes per camera
test_inds_cam1 = [[],[],[],[],[],[],[]]
test_inds_cam2 = [[],[],[],[],[],[],[]]
test_inds_cam3 = [[],[],[],[],[],[],[]]

for i, path in enumerate(image_paths):
    for j in range(nvideos):
        if str(path[1]).__eq__(test_paths_cam1[j]):
            test_inds_cam1[j].append(i)
        elif str(path[1]).__eq__(test_paths_cam2[j]):
            test_inds_cam2[j].append(i)
        elif str(path[1]).__eq__(test_paths_cam3[j]):
            test_inds_cam3[j].append(i)

nshuffles = len(Shuffles)

#pre-allocate matrixes for mean values and standard errors
mean_cam1 = np.zeros([nshuffles,nvideos]) # shuffle x movie
mean_cam2 = np.zeros([nshuffles,nvideos])
mean_cam3 = np.zeros([nshuffles,nvideos])

ste_cam1 = np.zeros([nshuffles,nvideos])
ste_cam2 = np.zeros([nshuffles,nvideos])
ste_cam3 = np.zeros([nshuffles,nvideos])

meanPcut_cam1 = np.zeros([nshuffles,nvideos]) # shuffle x movie
meanPcut_cam2 = np.zeros([nshuffles,nvideos])
meanPcut_cam3 = np.zeros([nshuffles,nvideos])

stePcut_cam1 = np.zeros([nshuffles,nvideos])
stePcut_cam2 = np.zeros([nshuffles,nvideos])
stePcut_cam3 = np.zeros([nshuffles,nvideos])

# %%

for i, shuffle in enumerate(Shuffles):
    if shuffle == 4 or i == 2: model_prefix = model_prefix_base
    elif shuffle == 3: model_prefix = model_prefix_augm 

    trainFractionIndex = shuffle-1
    snapshot=-1
    (
        ErrorDistribution_all,
        _,
        _,
        ErrorDistributionPCutOff_all,
        _,
        _
    )  = getErrorDistribution(
        config_path,
        shuffle=shuffle,
        snapindex=snapshot,
        trainFractionIndex = trainFractionIndex,
        modelprefix = model_prefix
    )
    for movie_number in range(7):

        meanPcut_cam1[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])
        stePcut_cam1[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam1[movie_number]][:].size**.5)

        meanPcut_cam2[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam2[movie_number]][:])
        stePcut_cam2[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam2[movie_number]][:].size**.5)

        meanPcut_cam3[i,movie_number] = np.nanmean(ErrorDistributionPCutOff_all.values[test_inds_cam3[movie_number]][:])
        stePcut_cam3[i,movie_number] = np.nanstd(ErrorDistributionPCutOff_all.values[test_inds_cam1[movie_number]][:])/(ErrorDistribution_all.values[test_inds_cam3[movie_number]][:].size**.5)

fig, (ax1,ax2,ax3) = plt.subplots(3,1)
fig.set_figheight(15)
fig.set_figwidth(10)
for i, shuffle in enumerate(Shuffles):
    
    # to jitter the error bars to keep  them from overlapping
    movie_number = list(range(1,8))
    movie_number = [x - 2/50 + shuffle/50 for x in movie_number]
    
    ax1.errorbar(movie_number,meanPcut_cam1[i,:], stePcut_cam1[i,:,])

    ax2.errorbar(movie_number,meanPcut_cam2[i,:], stePcut_cam2[i,:,])

    ax3.errorbar(movie_number,meanPcut_cam3[i,:], stePcut_cam3[i,:,])

ax1.set_ylim([0, 50])
ax2.set_ylim([0, 50])
ax3.set_ylim([0, 50])
ax1.set_title(&quot;Cam 1&quot;)
ax2.set_title(&quot;Cam 2&quot;)
ax3.set_title(&quot;Cam 3&quot;)
ax1.set_ylabel(&quot;Error (px&quot;)
ax2.set_ylabel(&quot;Error (px&quot;)
ax3.set_ylabel(&quot;Error (px&quot;)
ax1.legend([&quot;Full, ref, baseline&quot;, &quot;Full, OOD, fliplr_180_rotate&quot;, &quot;Full, OOD, baseline&quot;])
</pre></div>
</div>
</div>
</div>
</section>
<section id="interpret-performance-of-network-after-fliplr-and-180-rotation-augmentation">
<h2>Interpret performance of network after <code class="docutils literal notranslate"><span class="pre">fliplr</span></code> and 180¬∞ rotation augmentation<a class="headerlink" href="#interpret-performance-of-network-after-fliplr-and-180-rotation-augmentation" title="Permalink to this headline">#</a></h2>
<p>The results look very promising. Video 1 keeps being difficult. This is likely due to this video depicting a bat flying in a manner that is atypical for the dataset used here. Most videos in the dataset depict rather straight-forward flybys, but video 1 shows a bat slowly ascending while turning. Even so, the augmentation has lowered the pixel error for video 1 compared to the baseline, and for all other videos, the augmentation has led to performance close to, or better than, the refined case.</p>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="conclusion">
<h1>Conclusion<a class="headerlink" href="#conclusion" title="Permalink to this headline">#</a></h1>
<p>What follows are some of my conclusions from using augmentation to improve performance when tracking wing movement of bats using DeepLabCut.</p>
<section id="where-scalability-is-not-important-consider-refining">
<h2>Where scalability is not important, consider refining<a class="headerlink" href="#where-scalability-is-not-important-consider-refining" title="Permalink to this headline">#</a></h2>
<p>We saw from our baseline accuracy tests that when refining, there was no gain from increasing the amount of base training data. In practical terms this means that if you know which videos you want to analyze, and when you do not plan on using the network for analyzing other videos in the future, then it makes sense to prioritize digitizing frames from the videos to be analyzed.</p>
</section>
<section id="inspect-bodypart-predictions-and-augment-with-purpose">
<h2>Inspect bodypart predictions and augment with purpose<a class="headerlink" href="#inspect-bodypart-predictions-and-augment-with-purpose" title="Permalink to this headline">#</a></h2>
<p>At first, I tried a couple of permutations of different image augmentations and saw no or little performance improvement. I tried augmentations related to scale, brightness, blurriness, and probably some other that slip my mind at the moment. I more or less concluded that augmentations have little effect and that for a given set of training and test data, DeepLabCut‚Äôs default training parameters are close to optimal and little can be gained by augmenting the training data. But then I looked closer at the labeled frames from <code class="docutils literal notranslate"><span class="pre">evaluate_network</span></code> and realized that the high pixel errors were mainly caused by the network getting left and right mixed up and augmented accordingly. Doing that, I saw an average accuracy improvement of more than 10 pixels. For this dataset, 10 pixels is quite big, for comparison, on the largest image in the test dataset, the forearm of the bat is less than 70 pixels long.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "conda-env-DEEPLABCUT_newGUI-py"
        },
        kernelOptions: {
            kernelName: "conda-env-DEEPLABCUT_newGUI-py",
            path: "./docs/recipes"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'conda-env-DEEPLABCUT_newGUI-py'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="OpenVINO.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Intel OpenVINO backend</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../MISSION_AND_VALUES.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Mission and Values of DeepLabCut</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By The DeepLabCut Team<br/>
  
      &copy; Copyright 2022.<br/>
    <div class="extra_footer">
      <div>Powered by <a href="https://jupyterbook.org/">Jupyter Book</a>.</div>

    </div>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>