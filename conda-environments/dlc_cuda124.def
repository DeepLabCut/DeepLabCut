Bootstrap: docker
From: mambaorg/micromamba:jammy-cuda-12.4.1

%files
    environment.yaml /tmp/environment.yaml

%environment
    export PATH=/opt/envs/DLC-DEV/bin:$PATH
    export PYTHONUNBUFFERED=1
    export MPLBACKEND=Agg

%post -c /bin/bash
    set -euo pipefail
    export MAMBA_ROOT_PREFIX=/opt/micromamba
    micromamba config set channels conda-forge pytorch nvidia
    micromamba config set channel_priority flexible
    micromamba create -y -p /opt/envs/DLC-DEV -f /tmp/environment.yaml
    micromamba clean -a -y
    /opt/envs/DLC-DEV/bin/python -V
    /opt/envs/DLC-DEV/bin/python - <<PY
import torch
print("torch", getattr(torch,"__version__",None), "cuda?", getattr(torch,"cuda",None) and torch.cuda.is_available())
print("cuda runtime:", getattr(getattr(torch,"version",None),"cuda",None))
PY

%runscript
    exec "$@"